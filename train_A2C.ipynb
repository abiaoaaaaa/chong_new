{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3 import PPO,DDPG,DQN\n",
    "from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import env\n",
    "env = env.ElectricVehicleEnv()\n",
    "#vec_env = make_vec_env(env, n_envs=4)\n",
    "check_env(env)\n",
    "env = DummyVecEnv([lambda: env])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-01T08:40:31.881418700Z",
     "start_time": "2024-09-01T08:40:25.681700600Z"
    }
   },
   "id": "376f27caa974f1e8"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=-6573.89 +/- 3255.40\n",
      "Episode length: 1.80 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1284.74 +/- 2906.20\n",
      "Episode length: 1.15 +/- 0.36\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-2505.83 +/- 3729.59\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=4000, episode_reward=-3256.10 +/- 4352.04\n",
      "Episode length: 4.50 +/- 4.77\n",
      "Eval num_timesteps=5000, episode_reward=-2097.21 +/- 3525.10\n",
      "Episode length: 1.25 +/- 0.43\n",
      "Eval num_timesteps=6000, episode_reward=-1886.73 +/- 3650.10\n",
      "Episode length: 3.00 +/- 4.00\n",
      "Eval num_timesteps=7000, episode_reward=-2096.38 +/- 3525.59\n",
      "Episode length: 1.25 +/- 0.43\n",
      "Eval num_timesteps=8000, episode_reward=-98.68 +/- 48.19\n",
      "Episode length: 1.35 +/- 0.48\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=-82.39 +/- 41.04\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-62.34 +/- 3.09\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11000, episode_reward=-61.44 +/- 3.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=-62.43 +/- 4.03\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-60.77 +/- 4.27\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=-61.16 +/- 4.35\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-63.07 +/- 3.94\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-62.71 +/- 4.22\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-60.73 +/- 3.58\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=-61.90 +/- 3.09\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-62.32 +/- 3.38\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-61.60 +/- 3.06\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-60.33 +/- 3.11\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22000, episode_reward=-62.24 +/- 3.61\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-61.93 +/- 4.05\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-62.79 +/- 3.30\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-62.64 +/- 3.79\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-30.13 +/- 39.19\n",
      "Episode length: 1.45 +/- 0.50\n",
      "New best mean reward!\n",
      "Eval num_timesteps=27000, episode_reward=-22.46 +/- 63.91\n",
      "Episode length: 1.30 +/- 0.46\n",
      "New best mean reward!\n",
      "Eval num_timesteps=28000, episode_reward=-42.28 +/- 39.43\n",
      "Episode length: 1.25 +/- 0.43\n",
      "Eval num_timesteps=29000, episode_reward=1.89 +/- 68.44\n",
      "Episode length: 1.55 +/- 0.50\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-16.43 +/- 68.97\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=31000, episode_reward=4.73 +/- 73.11\n",
      "Episode length: 1.50 +/- 0.50\n",
      "New best mean reward!\n",
      "Eval num_timesteps=32000, episode_reward=-31.07 +/- 57.93\n",
      "Episode length: 1.25 +/- 0.43\n",
      "Eval num_timesteps=33000, episode_reward=-22.70 +/- 64.04\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=34000, episode_reward=-10.18 +/- 60.90\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=35000, episode_reward=-38.35 +/- 39.26\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=36000, episode_reward=20.36 +/- 75.36\n",
      "Episode length: 1.60 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=37000, episode_reward=19.62 +/- 76.15\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=38000, episode_reward=-3.64 +/- 71.12\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=39000, episode_reward=-8.46 +/- 59.47\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=40000, episode_reward=1.90 +/- 74.59\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=41000, episode_reward=-33.30 +/- 49.57\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=42000, episode_reward=-5.00 +/- 65.78\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=43000, episode_reward=-19.27 +/- 63.03\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=44000, episode_reward=-19.80 +/- 47.44\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=45000, episode_reward=-22.97 +/- 56.91\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=46000, episode_reward=-23.19 +/- 64.34\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=47000, episode_reward=-0.33 +/- 69.37\n",
      "Episode length: 1.50 +/- 0.50\n",
      "Eval num_timesteps=48000, episode_reward=-22.46 +/- 63.90\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=49000, episode_reward=-16.46 +/- 54.73\n",
      "Episode length: 1.50 +/- 0.50\n",
      "Eval num_timesteps=50000, episode_reward=-19.76 +/- 63.35\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=51000, episode_reward=-36.71 +/- 58.06\n",
      "Episode length: 1.15 +/- 0.36\n",
      "Eval num_timesteps=52000, episode_reward=-8.96 +/- 59.88\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=53000, episode_reward=-22.72 +/- 56.75\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=54000, episode_reward=7.04 +/- 65.57\n",
      "Episode length: 1.65 +/- 0.48\n",
      "Eval num_timesteps=55000, episode_reward=-22.84 +/- 56.84\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=56000, episode_reward=-22.60 +/- 56.67\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=57000, episode_reward=20.83 +/- 68.73\n",
      "Episode length: 1.70 +/- 0.46\n",
      "New best mean reward!\n",
      "Eval num_timesteps=58000, episode_reward=-17.18 +/- 62.75\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=59000, episode_reward=-20.02 +/- 56.12\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=60000, episode_reward=-41.66 +/- 39.16\n",
      "Episode length: 1.25 +/- 0.43\n",
      "Eval num_timesteps=61000, episode_reward=-30.58 +/- 57.67\n",
      "Episode length: 1.25 +/- 0.43\n",
      "Eval num_timesteps=62000, episode_reward=9.64 +/- 70.47\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=63000, episode_reward=-20.25 +/- 63.66\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=64000, episode_reward=-22.46 +/- 63.90\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=65000, episode_reward=-1.43 +/- 70.31\n",
      "Episode length: 1.50 +/- 0.50\n",
      "Eval num_timesteps=66000, episode_reward=-28.00 +/- 57.51\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=67000, episode_reward=-17.42 +/- 62.93\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=68000, episode_reward=-14.60 +/- 62.06\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=69000, episode_reward=-9.69 +/- 60.50\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=70000, episode_reward=-3.27 +/- 70.83\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=71000, episode_reward=-5.24 +/- 71.40\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=72000, episode_reward=-30.97 +/- 49.67\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=73000, episode_reward=-825.24 +/- 2440.81\n",
      "Episode length: 1.50 +/- 0.50\n",
      "Eval num_timesteps=74000, episode_reward=-13.36 +/- 67.97\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=75000, episode_reward=-19.43 +/- 47.14\n",
      "Episode length: 1.55 +/- 0.50\n",
      "Eval num_timesteps=76000, episode_reward=-11.53 +/- 60.84\n",
      "Episode length: 1.50 +/- 0.50\n",
      "Eval num_timesteps=77000, episode_reward=-31.21 +/- 49.80\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=78000, episode_reward=-25.53 +/- 64.52\n",
      "Episode length: 1.25 +/- 0.43\n",
      "Eval num_timesteps=79000, episode_reward=-2030.75 +/- 3530.82\n",
      "Episode length: 1.70 +/- 0.46\n",
      "Eval num_timesteps=80000, episode_reward=-39.82 +/- 40.01\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=81000, episode_reward=-23.09 +/- 56.99\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=82000, episode_reward=-31.07 +/- 57.92\n",
      "Episode length: 1.25 +/- 0.43\n",
      "Eval num_timesteps=83000, episode_reward=-1.16 +/- 76.12\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=84000, episode_reward=-2140.91 +/- 3704.20\n",
      "Episode length: 2.40 +/- 1.56\n",
      "Eval num_timesteps=85000, episode_reward=12.34 +/- 68.71\n",
      "Episode length: 1.65 +/- 0.48\n",
      "Eval num_timesteps=86000, episode_reward=-27.51 +/- 57.24\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=87000, episode_reward=4.24 +/- 73.55\n",
      "Episode length: 1.50 +/- 0.50\n",
      "Eval num_timesteps=88000, episode_reward=-29.48 +/- 57.11\n",
      "Episode length: 1.25 +/- 0.43\n",
      "Eval num_timesteps=89000, episode_reward=4.46 +/- 67.06\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=90000, episode_reward=-19.88 +/- 63.43\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=91000, episode_reward=-20.74 +/- 63.94\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=92000, episode_reward=-30.23 +/- 49.22\n",
      "Episode length: 1.35 +/- 0.48\n",
      "Eval num_timesteps=93000, episode_reward=-28.75 +/- 49.72\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=94000, episode_reward=-22.46 +/- 63.90\n",
      "Episode length: 1.30 +/- 0.46\n",
      "Eval num_timesteps=95000, episode_reward=-20.60 +/- 70.44\n",
      "Episode length: 1.25 +/- 0.43\n",
      "Eval num_timesteps=96000, episode_reward=13.68 +/- 70.09\n",
      "Episode length: 1.85 +/- 0.79\n",
      "Eval num_timesteps=97000, episode_reward=-8.68 +/- 69.72\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=98000, episode_reward=7.32 +/- 69.30\n",
      "Episode length: 2.10 +/- 0.99\n",
      "Eval num_timesteps=99000, episode_reward=-18.06 +/- 66.21\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=100000, episode_reward=-8.44 +/- 69.53\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=101000, episode_reward=-7.31 +/- 56.95\n",
      "Episode length: 2.10 +/- 0.99\n",
      "Eval num_timesteps=102000, episode_reward=-6.77 +/- 63.61\n",
      "Episode length: 2.00 +/- 1.00\n",
      "Eval num_timesteps=103000, episode_reward=-18.11 +/- 59.41\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=104000, episode_reward=-0.29 +/- 72.64\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=105000, episode_reward=-14.67 +/- 58.95\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=106000, episode_reward=-29.11 +/- 54.47\n",
      "Episode length: 2.00 +/- 1.61\n",
      "Eval num_timesteps=107000, episode_reward=-1.24 +/- 50.85\n",
      "Episode length: 3.40 +/- 1.85\n",
      "Eval num_timesteps=108000, episode_reward=1.98 +/- 71.94\n",
      "Episode length: 2.20 +/- 1.47\n",
      "Eval num_timesteps=109000, episode_reward=-19.79 +/- 60.90\n",
      "Episode length: 1.75 +/- 1.18\n",
      "Eval num_timesteps=110000, episode_reward=-2.08 +/- 67.64\n",
      "Episode length: 1.85 +/- 1.15\n",
      "Eval num_timesteps=111000, episode_reward=2.06 +/- 66.58\n",
      "Episode length: 2.00 +/- 1.22\n",
      "Eval num_timesteps=112000, episode_reward=-10.48 +/- 69.08\n",
      "Episode length: 1.55 +/- 0.92\n",
      "Eval num_timesteps=113000, episode_reward=-23.93 +/- 60.60\n",
      "Episode length: 1.60 +/- 1.07\n",
      "Eval num_timesteps=114000, episode_reward=24.03 +/- 76.37\n",
      "Episode length: 2.20 +/- 0.98\n",
      "New best mean reward!\n",
      "Eval num_timesteps=115000, episode_reward=-7.19 +/- 56.85\n",
      "Episode length: 2.10 +/- 0.99\n",
      "Eval num_timesteps=116000, episode_reward=-12.54 +/- 64.32\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=117000, episode_reward=-14.67 +/- 58.95\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=118000, episode_reward=-14.91 +/- 60.40\n",
      "Episode length: 1.90 +/- 1.26\n",
      "Eval num_timesteps=119000, episode_reward=-19.21 +/- 65.24\n",
      "Episode length: 1.50 +/- 0.92\n",
      "Eval num_timesteps=120000, episode_reward=4.83 +/- 76.54\n",
      "Episode length: 2.10 +/- 1.34\n",
      "Eval num_timesteps=121000, episode_reward=8.64 +/- 64.53\n",
      "Episode length: 2.90 +/- 1.73\n",
      "Eval num_timesteps=122000, episode_reward=-17.99 +/- 60.91\n",
      "Episode length: 1.90 +/- 1.26\n",
      "Eval num_timesteps=123000, episode_reward=-23.11 +/- 61.30\n",
      "Episode length: 1.75 +/- 1.18\n",
      "Eval num_timesteps=124000, episode_reward=10.72 +/- 65.05\n",
      "Episode length: 2.55 +/- 1.32\n",
      "Eval num_timesteps=125000, episode_reward=-10.32 +/- 64.52\n",
      "Episode length: 1.80 +/- 1.17\n",
      "Eval num_timesteps=126000, episode_reward=-9.84 +/- 60.53\n",
      "Episode length: 2.20 +/- 1.36\n",
      "Eval num_timesteps=127000, episode_reward=12.23 +/- 68.61\n",
      "Episode length: 2.80 +/- 1.66\n",
      "Eval num_timesteps=128000, episode_reward=-10.95 +/- 66.45\n",
      "Episode length: 2.00 +/- 1.26\n",
      "Eval num_timesteps=129000, episode_reward=-4.56 +/- 64.42\n",
      "Episode length: 2.60 +/- 1.74\n",
      "Eval num_timesteps=130000, episode_reward=-23.48 +/- 61.53\n",
      "Episode length: 1.75 +/- 1.18\n",
      "Eval num_timesteps=131000, episode_reward=-30.89 +/- 47.74\n",
      "Episode length: 1.85 +/- 1.31\n",
      "Eval num_timesteps=132000, episode_reward=-18.72 +/- 64.95\n",
      "Episode length: 1.50 +/- 0.92\n",
      "Eval num_timesteps=133000, episode_reward=0.75 +/- 67.69\n",
      "Episode length: 2.50 +/- 1.66\n",
      "Eval num_timesteps=134000, episode_reward=-27.05 +/- 54.23\n",
      "Episode length: 1.70 +/- 1.19\n",
      "Eval num_timesteps=135000, episode_reward=-25.79 +/- 47.57\n",
      "Episode length: 1.95 +/- 1.36\n",
      "Eval num_timesteps=136000, episode_reward=-34.41 +/- 46.04\n",
      "Episode length: 1.70 +/- 1.23\n",
      "Eval num_timesteps=137000, episode_reward=1.82 +/- 73.13\n",
      "Episode length: 2.05 +/- 1.20\n",
      "Eval num_timesteps=138000, episode_reward=-2.69 +/- 68.15\n",
      "Episode length: 1.85 +/- 1.15\n",
      "Eval num_timesteps=139000, episode_reward=-14.14 +/- 54.64\n",
      "Episode length: 2.15 +/- 1.39\n",
      "Eval num_timesteps=140000, episode_reward=-19.18 +/- 60.52\n",
      "Episode length: 1.75 +/- 1.18\n",
      "Eval num_timesteps=141000, episode_reward=-27.91 +/- 54.74\n",
      "Episode length: 1.70 +/- 1.19\n",
      "Eval num_timesteps=142000, episode_reward=-23.56 +/- 60.38\n",
      "Episode length: 1.60 +/- 1.07\n",
      "Eval num_timesteps=143000, episode_reward=-10.32 +/- 64.50\n",
      "Episode length: 1.80 +/- 1.17\n",
      "Eval num_timesteps=144000, episode_reward=-14.42 +/- 60.04\n",
      "Episode length: 1.90 +/- 1.26\n",
      "Eval num_timesteps=145000, episode_reward=-22.18 +/- 54.32\n",
      "Episode length: 1.85 +/- 1.28\n",
      "Eval num_timesteps=146000, episode_reward=-3.42 +/- 68.77\n",
      "Episode length: 1.85 +/- 1.15\n",
      "Eval num_timesteps=147000, episode_reward=4.58 +/- 71.75\n",
      "Episode length: 2.40 +/- 1.56\n",
      "Eval num_timesteps=148000, episode_reward=-23.08 +/- 60.04\n",
      "Episode length: 1.90 +/- 1.48\n",
      "Eval num_timesteps=149000, episode_reward=10.22 +/- 70.32\n",
      "Episode length: 2.35 +/- 1.28\n",
      "Eval num_timesteps=150000, episode_reward=-19.09 +/- 65.16\n",
      "Episode length: 1.50 +/- 0.92\n",
      "Eval num_timesteps=151000, episode_reward=-4.43 +/- 64.32\n",
      "Episode length: 2.60 +/- 1.74\n",
      "Eval num_timesteps=152000, episode_reward=21.24 +/- 73.61\n",
      "Episode length: 2.60 +/- 1.50\n",
      "Eval num_timesteps=153000, episode_reward=-11.42 +/- 65.34\n",
      "Episode length: 1.80 +/- 1.17\n",
      "Eval num_timesteps=154000, episode_reward=-35.17 +/- 45.92\n",
      "Episode length: 1.65 +/- 1.19\n",
      "Eval num_timesteps=155000, episode_reward=-11.19 +/- 66.64\n",
      "Episode length: 2.00 +/- 1.26\n",
      "Eval num_timesteps=156000, episode_reward=-34.90 +/- 46.33\n",
      "Episode length: 1.70 +/- 1.23\n",
      "Eval num_timesteps=157000, episode_reward=-39.49 +/- 50.91\n",
      "Episode length: 1.40 +/- 1.02\n",
      "Eval num_timesteps=158000, episode_reward=-19.61 +/- 66.20\n",
      "Episode length: 1.80 +/- 1.33\n",
      "Eval num_timesteps=159000, episode_reward=-19.99 +/- 60.57\n",
      "Episode length: 2.10 +/- 1.61\n",
      "Eval num_timesteps=160000, episode_reward=26.77 +/- 66.37\n",
      "Episode length: 2.75 +/- 1.22\n",
      "New best mean reward!\n",
      "Eval num_timesteps=161000, episode_reward=-26.93 +/- 54.15\n",
      "Episode length: 1.70 +/- 1.19\n",
      "Eval num_timesteps=162000, episode_reward=-18.23 +/- 55.56\n",
      "Episode length: 2.10 +/- 1.37\n",
      "Eval num_timesteps=163000, episode_reward=-27.24 +/- 55.24\n",
      "Episode length: 1.80 +/- 1.25\n",
      "Eval num_timesteps=164000, episode_reward=4.96 +/- 76.45\n",
      "Episode length: 2.10 +/- 1.34\n",
      "Eval num_timesteps=165000, episode_reward=-5.83 +/- 65.13\n",
      "Episode length: 2.15 +/- 1.31\n",
      "Eval num_timesteps=166000, episode_reward=-2.30 +/- 64.81\n",
      "Episode length: 2.30 +/- 1.35\n",
      "Eval num_timesteps=167000, episode_reward=-1.44 +/- 64.05\n",
      "Episode length: 2.30 +/- 1.35\n",
      "Eval num_timesteps=168000, episode_reward=1.23 +/- 64.32\n",
      "Episode length: 2.45 +/- 1.36\n",
      "Eval num_timesteps=169000, episode_reward=-27.25 +/- 60.78\n",
      "Episode length: 1.60 +/- 1.07\n",
      "Eval num_timesteps=170000, episode_reward=-39.91 +/- 51.57\n",
      "Episode length: 1.35 +/- 0.85\n",
      "Eval num_timesteps=171000, episode_reward=-10.70 +/- 66.28\n",
      "Episode length: 2.00 +/- 1.26\n",
      "Eval num_timesteps=172000, episode_reward=-1.69 +/- 64.27\n",
      "Episode length: 2.30 +/- 1.35\n",
      "Eval num_timesteps=173000, episode_reward=4.57 +/- 71.13\n",
      "Episode length: 1.90 +/- 1.14\n",
      "Eval num_timesteps=174000, episode_reward=-13.90 +/- 54.44\n",
      "Episode length: 2.15 +/- 1.39\n",
      "Eval num_timesteps=175000, episode_reward=-19.67 +/- 60.83\n",
      "Episode length: 1.75 +/- 1.18\n",
      "Eval num_timesteps=176000, episode_reward=5.22 +/- 66.20\n",
      "Episode length: 2.15 +/- 1.28\n",
      "Eval num_timesteps=177000, episode_reward=-13.73 +/- 60.72\n",
      "Episode length: 2.05 +/- 1.32\n",
      "Eval num_timesteps=178000, episode_reward=1.83 +/- 68.59\n",
      "Episode length: 2.25 +/- 1.30\n",
      "Eval num_timesteps=179000, episode_reward=-9.98 +/- 70.45\n",
      "Episode length: 1.80 +/- 1.12\n",
      "Eval num_timesteps=180000, episode_reward=-9.72 +/- 65.54\n",
      "Episode length: 2.00 +/- 1.26\n",
      "Eval num_timesteps=181000, episode_reward=1.59 +/- 68.80\n",
      "Episode length: 2.25 +/- 1.30\n",
      "Eval num_timesteps=182000, episode_reward=-12.81 +/- 71.67\n",
      "Episode length: 1.90 +/- 1.34\n",
      "Eval num_timesteps=183000, episode_reward=-6.45 +/- 70.38\n",
      "Episode length: 1.95 +/- 1.20\n",
      "Eval num_timesteps=184000, episode_reward=-32.00 +/- 54.63\n",
      "Episode length: 1.65 +/- 1.15\n",
      "Eval num_timesteps=185000, episode_reward=-3.02 +/- 63.96\n",
      "Episode length: 2.10 +/- 1.30\n",
      "Eval num_timesteps=186000, episode_reward=-22.99 +/- 61.25\n",
      "Episode length: 1.75 +/- 1.18\n",
      "Eval num_timesteps=187000, episode_reward=-1273.76 +/- 3045.84\n",
      "Episode length: 2.20 +/- 1.47\n",
      "Eval num_timesteps=188000, episode_reward=-13.73 +/- 60.72\n",
      "Episode length: 2.05 +/- 1.32\n",
      "Eval num_timesteps=189000, episode_reward=6.71 +/- 66.58\n",
      "Episode length: 2.40 +/- 1.32\n",
      "Eval num_timesteps=190000, episode_reward=9.86 +/- 70.69\n",
      "Episode length: 2.35 +/- 1.28\n",
      "Eval num_timesteps=191000, episode_reward=-15.31 +/- 54.72\n",
      "Episode length: 2.65 +/- 1.85\n",
      "Eval num_timesteps=192000, episode_reward=-27.37 +/- 55.30\n",
      "Episode length: 1.80 +/- 1.25\n",
      "Eval num_timesteps=193000, episode_reward=-5.94 +/- 63.71\n",
      "Episode length: 1.95 +/- 1.24\n",
      "Eval num_timesteps=194000, episode_reward=-6.82 +/- 75.06\n",
      "Episode length: 1.75 +/- 1.04\n",
      "Eval num_timesteps=195000, episode_reward=-23.72 +/- 61.68\n",
      "Episode length: 1.75 +/- 1.18\n",
      "Eval num_timesteps=196000, episode_reward=-23.23 +/- 61.39\n",
      "Episode length: 1.75 +/- 1.18\n",
      "Eval num_timesteps=197000, episode_reward=4.95 +/- 71.42\n",
      "Episode length: 2.40 +/- 1.56\n",
      "Eval num_timesteps=198000, episode_reward=-15.57 +/- 62.07\n",
      "Episode length: 2.05 +/- 1.32\n",
      "Eval num_timesteps=199000, episode_reward=-3.05 +/- 74.64\n",
      "Episode length: 1.90 +/- 1.14\n",
      "Eval num_timesteps=200000, episode_reward=-18.86 +/- 66.50\n",
      "Episode length: 1.70 +/- 1.10\n",
      "Eval num_timesteps=201000, episode_reward=-3.56 +/- 56.24\n",
      "Episode length: 3.00 +/- 1.84\n",
      "Eval num_timesteps=202000, episode_reward=-24.06 +/- 60.62\n",
      "Episode length: 1.90 +/- 1.48\n",
      "Eval num_timesteps=203000, episode_reward=13.24 +/- 78.42\n",
      "Episode length: 2.10 +/- 1.14\n",
      "Eval num_timesteps=204000, episode_reward=-23.60 +/- 61.61\n",
      "Episode length: 1.75 +/- 1.18\n",
      "Eval num_timesteps=205000, episode_reward=-8.25 +/- 70.62\n",
      "Episode length: 2.10 +/- 1.48\n",
      "Eval num_timesteps=206000, episode_reward=-2.07 +/- 69.39\n",
      "Episode length: 2.10 +/- 1.26\n",
      "Eval num_timesteps=207000, episode_reward=1.22 +/- 69.13\n",
      "Episode length: 2.25 +/- 1.30\n",
      "Eval num_timesteps=208000, episode_reward=-27.33 +/- 59.61\n",
      "Episode length: 1.45 +/- 0.92\n",
      "Eval num_timesteps=209000, episode_reward=0.40 +/- 76.30\n",
      "Episode length: 1.50 +/- 0.74\n",
      "Eval num_timesteps=210000, episode_reward=-23.68 +/- 60.44\n",
      "Episode length: 1.60 +/- 1.07\n",
      "Eval num_timesteps=211000, episode_reward=-1.92 +/- 62.95\n",
      "Episode length: 2.10 +/- 1.30\n",
      "Eval num_timesteps=212000, episode_reward=-2123.39 +/- 3629.85\n",
      "Episode length: 2.15 +/- 1.39\n",
      "Eval num_timesteps=213000, episode_reward=-10.90 +/- 60.20\n",
      "Episode length: 2.05 +/- 1.32\n",
      "Eval num_timesteps=214000, episode_reward=-14.67 +/- 60.21\n",
      "Episode length: 1.90 +/- 1.26\n",
      "Eval num_timesteps=215000, episode_reward=-15.09 +/- 62.40\n",
      "Episode length: 1.45 +/- 0.50\n",
      "Eval num_timesteps=216000, episode_reward=-7.58 +/- 66.83\n",
      "Episode length: 1.50 +/- 0.50\n",
      "Eval num_timesteps=217000, episode_reward=-36.10 +/- 57.80\n",
      "Episode length: 1.15 +/- 0.36\n",
      "Eval num_timesteps=218000, episode_reward=14.42 +/- 69.32\n",
      "Episode length: 1.85 +/- 0.79\n",
      "Eval num_timesteps=219000, episode_reward=-12.13 +/- 68.66\n",
      "Episode length: 1.45 +/- 0.67\n",
      "Eval num_timesteps=220000, episode_reward=-17.55 +/- 64.40\n",
      "Episode length: 1.50 +/- 0.74\n",
      "Eval num_timesteps=221000, episode_reward=-2.45 +/- 67.95\n",
      "Episode length: 1.85 +/- 1.15\n",
      "Eval num_timesteps=222000, episode_reward=-2.32 +/- 67.85\n",
      "Episode length: 1.85 +/- 1.15\n",
      "Eval num_timesteps=223000, episode_reward=-15.77 +/- 61.02\n",
      "Episode length: 1.90 +/- 1.26\n",
      "Eval num_timesteps=224000, episode_reward=-3.22 +/- 72.79\n",
      "Episode length: 1.60 +/- 0.92\n",
      "Eval num_timesteps=225000, episode_reward=-28.38 +/- 54.04\n",
      "Episode length: 2.00 +/- 1.61\n",
      "Eval num_timesteps=226000, episode_reward=-40.34 +/- 51.25\n",
      "Episode length: 1.40 +/- 1.02\n",
      "Eval num_timesteps=227000, episode_reward=-7.15 +/- 69.80\n",
      "Episode length: 2.10 +/- 1.48\n",
      "Eval num_timesteps=228000, episode_reward=-5.43 +/- 63.71\n",
      "Episode length: 2.30 +/- 1.35\n",
      "Eval num_timesteps=229000, episode_reward=-7.33 +/- 75.01\n",
      "Episode length: 1.80 +/- 1.17\n",
      "Eval num_timesteps=230000, episode_reward=-1.18 +/- 63.23\n",
      "Episode length: 2.80 +/- 1.78\n",
      "Eval num_timesteps=231000, episode_reward=-28.35 +/- 52.86\n",
      "Episode length: 1.80 +/- 1.25\n",
      "Eval num_timesteps=232000, episode_reward=-838.34 +/- 2544.16\n",
      "Episode length: 2.95 +/- 1.32\n",
      "Eval num_timesteps=233000, episode_reward=12.11 +/- 68.72\n",
      "Episode length: 2.80 +/- 1.66\n",
      "Eval num_timesteps=234000, episode_reward=-7.16 +/- 64.27\n",
      "Episode length: 2.40 +/- 1.69\n",
      "Eval num_timesteps=235000, episode_reward=-11.47 +/- 65.29\n",
      "Episode length: 2.20 +/- 1.60\n",
      "Eval num_timesteps=236000, episode_reward=-26.88 +/- 55.02\n",
      "Episode length: 1.80 +/- 1.25\n",
      "Eval num_timesteps=237000, episode_reward=-11.94 +/- 71.80\n",
      "Episode length: 1.80 +/- 1.12\n",
      "Eval num_timesteps=238000, episode_reward=-1.70 +/- 69.09\n",
      "Episode length: 2.10 +/- 1.26\n",
      "Eval num_timesteps=239000, episode_reward=-31.75 +/- 54.51\n",
      "Episode length: 1.65 +/- 1.15\n",
      "Eval num_timesteps=240000, episode_reward=-34.54 +/- 46.09\n",
      "Episode length: 1.70 +/- 1.23\n",
      "Eval num_timesteps=241000, episode_reward=-10.47 +/- 70.79\n",
      "Episode length: 1.80 +/- 1.12\n",
      "Eval num_timesteps=242000, episode_reward=-26.51 +/- 54.80\n",
      "Episode length: 1.80 +/- 1.25\n",
      "Eval num_timesteps=243000, episode_reward=-435.27 +/- 1845.52\n",
      "Episode length: 2.10 +/- 1.41\n",
      "Eval num_timesteps=244000, episode_reward=-1271.49 +/- 3021.40\n",
      "Episode length: 2.00 +/- 1.22\n",
      "Eval num_timesteps=245000, episode_reward=-18.37 +/- 49.07\n",
      "Episode length: 2.25 +/- 1.44\n",
      "Eval num_timesteps=246000, episode_reward=-19.42 +/- 60.65\n",
      "Episode length: 1.75 +/- 1.18\n",
      "Eval num_timesteps=247000, episode_reward=5.92 +/- 69.90\n",
      "Episode length: 1.90 +/- 1.14\n",
      "Eval num_timesteps=248000, episode_reward=-14.67 +/- 60.22\n",
      "Episode length: 1.90 +/- 1.26\n",
      "Eval num_timesteps=249000, episode_reward=-10.86 +/- 55.01\n",
      "Episode length: 2.30 +/- 1.42\n",
      "Eval num_timesteps=250000, episode_reward=17.81 +/- 69.50\n",
      "Episode length: 2.10 +/- 1.18\n",
      "Eval num_timesteps=251000, episode_reward=5.19 +/- 70.58\n",
      "Episode length: 1.90 +/- 1.14\n",
      "Eval num_timesteps=252000, episode_reward=12.45 +/- 72.68\n",
      "Episode length: 1.95 +/- 1.12\n",
      "Eval num_timesteps=253000, episode_reward=4.57 +/- 71.14\n",
      "Episode length: 1.90 +/- 1.14\n",
      "Eval num_timesteps=254000, episode_reward=-14.30 +/- 59.95\n",
      "Episode length: 1.90 +/- 1.26\n",
      "Eval num_timesteps=255000, episode_reward=3.92 +/- 75.74\n",
      "Episode length: 1.65 +/- 0.91\n",
      "Eval num_timesteps=256000, episode_reward=17.36 +/- 65.63\n",
      "Episode length: 2.35 +/- 1.28\n",
      "Eval num_timesteps=257000, episode_reward=-21.85 +/- 59.32\n",
      "Episode length: 1.60 +/- 1.07\n",
      "Eval num_timesteps=258000, episode_reward=-3.46 +/- 72.99\n",
      "Episode length: 1.60 +/- 0.92\n",
      "Eval num_timesteps=259000, episode_reward=-22.79 +/- 54.72\n",
      "Episode length: 1.85 +/- 1.28\n",
      "Eval num_timesteps=260000, episode_reward=-27.98 +/- 64.72\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=261000, episode_reward=-8.75 +/- 107.11\n",
      "Episode length: 1.65 +/- 0.48\n",
      "Eval num_timesteps=262000, episode_reward=-60.40 +/- 93.73\n",
      "Episode length: 1.50 +/- 0.50\n",
      "Eval num_timesteps=263000, episode_reward=-27.42 +/- 54.45\n",
      "Episode length: 1.70 +/- 1.19\n",
      "Eval num_timesteps=264000, episode_reward=-10.41 +/- 57.91\n",
      "Episode length: 2.20 +/- 1.36\n",
      "Eval num_timesteps=265000, episode_reward=-13.46 +/- 60.40\n",
      "Episode length: 2.50 +/- 1.77\n",
      "Eval num_timesteps=266000, episode_reward=13.10 +/- 77.78\n",
      "Episode length: 2.20 +/- 1.33\n",
      "Eval num_timesteps=267000, episode_reward=-8.57 +/- 70.65\n",
      "Episode length: 2.10 +/- 1.48\n",
      "Eval num_timesteps=268000, episode_reward=3.31 +/- 61.18\n",
      "Episode length: 3.00 +/- 1.79\n",
      "Eval num_timesteps=269000, episode_reward=-3.34 +/- 68.68\n",
      "Episode length: 2.30 +/- 1.58\n",
      "Eval num_timesteps=270000, episode_reward=-35.44 +/- 43.91\n",
      "Episode length: 1.90 +/- 1.61\n",
      "Eval num_timesteps=271000, episode_reward=-16.24 +/- 60.06\n",
      "Episode length: 2.30 +/- 1.71\n",
      "Eval num_timesteps=272000, episode_reward=-19.86 +/- 66.22\n",
      "Episode length: 1.80 +/- 1.33\n",
      "Eval num_timesteps=273000, episode_reward=-19.10 +/- 66.67\n",
      "Episode length: 1.70 +/- 1.10\n",
      "Eval num_timesteps=274000, episode_reward=-18.74 +/- 66.43\n",
      "Episode length: 1.70 +/- 1.10\n",
      "Eval num_timesteps=275000, episode_reward=-17.98 +/- 55.38\n",
      "Episode length: 2.10 +/- 1.37\n",
      "Eval num_timesteps=276000, episode_reward=-31.51 +/- 54.39\n",
      "Episode length: 1.65 +/- 1.15\n",
      "Eval num_timesteps=277000, episode_reward=-13.64 +/- 78.34\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=278000, episode_reward=-22.25 +/- 60.80\n",
      "Episode length: 1.75 +/- 1.18\n",
      "Eval num_timesteps=279000, episode_reward=-14.48 +/- 71.00\n",
      "Episode length: 1.65 +/- 1.01\n",
      "Eval num_timesteps=280000, episode_reward=13.98 +/- 77.73\n",
      "Episode length: 2.10 +/- 1.14\n",
      "Eval num_timesteps=281000, episode_reward=-2139.28 +/- 3716.30\n",
      "Episode length: 3.45 +/- 1.88\n",
      "Eval num_timesteps=282000, episode_reward=-22.13 +/- 60.72\n",
      "Episode length: 1.75 +/- 1.18\n",
      "Eval num_timesteps=283000, episode_reward=2.57 +/- 63.00\n",
      "Episode length: 2.45 +/- 1.36\n",
      "Eval num_timesteps=284000, episode_reward=-10.82 +/- 66.38\n",
      "Episode length: 2.00 +/- 1.26\n",
      "Eval num_timesteps=285000, episode_reward=-27.26 +/- 65.86\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=286000, episode_reward=-22.50 +/- 60.94\n",
      "Episode length: 1.75 +/- 1.18\n",
      "Eval num_timesteps=287000, episode_reward=-14.10 +/- 61.00\n",
      "Episode length: 2.05 +/- 1.32\n",
      "Eval num_timesteps=288000, episode_reward=-6.94 +/- 70.73\n",
      "Episode length: 1.95 +/- 1.20\n",
      "Eval num_timesteps=289000, episode_reward=18.62 +/- 66.35\n",
      "Episode length: 2.65 +/- 1.28\n",
      "Eval num_timesteps=290000, episode_reward=-31.87 +/- 54.58\n",
      "Episode length: 1.65 +/- 1.15\n",
      "Eval num_timesteps=291000, episode_reward=-3.32 +/- 68.97\n",
      "Episode length: 2.30 +/- 1.58\n",
      "Eval num_timesteps=292000, episode_reward=-35.91 +/- 44.62\n",
      "Episode length: 1.90 +/- 1.61\n",
      "Eval num_timesteps=293000, episode_reward=1.59 +/- 68.80\n",
      "Episode length: 2.25 +/- 1.30\n",
      "Eval num_timesteps=294000, episode_reward=-22.13 +/- 60.70\n",
      "Episode length: 1.75 +/- 1.18\n",
      "Eval num_timesteps=295000, episode_reward=-10.82 +/- 61.33\n",
      "Episode length: 2.20 +/- 1.36\n",
      "Eval num_timesteps=296000, episode_reward=-31.58 +/- 59.61\n",
      "Episode length: 1.50 +/- 1.07\n",
      "Eval num_timesteps=297000, episode_reward=-23.61 +/- 66.61\n",
      "Episode length: 1.55 +/- 0.97\n",
      "Eval num_timesteps=298000, episode_reward=-16.40 +/- 60.61\n",
      "Episode length: 2.30 +/- 1.71\n",
      "Eval num_timesteps=299000, episode_reward=-2.93 +/- 74.55\n",
      "Episode length: 1.90 +/- 1.14\n",
      "Eval num_timesteps=300000, episode_reward=-1.22 +/- 73.22\n",
      "Episode length: 1.90 +/- 1.14\n",
      "Eval num_timesteps=301000, episode_reward=-132.33 +/- 171.87\n",
      "Episode length: 1.95 +/- 1.32\n",
      "Eval num_timesteps=302000, episode_reward=-10.21 +/- 65.92\n",
      "Episode length: 2.00 +/- 1.26\n",
      "Eval num_timesteps=303000, episode_reward=-14.33 +/- 55.68\n",
      "Episode length: 2.25 +/- 1.41\n",
      "Eval num_timesteps=304000, episode_reward=-14.23 +/- 66.15\n",
      "Episode length: 1.85 +/- 1.19\n",
      "Eval num_timesteps=305000, episode_reward=-34.66 +/- 46.17\n",
      "Episode length: 1.70 +/- 1.23\n",
      "Eval num_timesteps=306000, episode_reward=0.99 +/- 67.47\n",
      "Episode length: 2.50 +/- 1.66\n",
      "Eval num_timesteps=307000, episode_reward=-12.68 +/- 70.59\n",
      "Episode length: 1.55 +/- 0.92\n",
      "Eval num_timesteps=308000, episode_reward=-31.56 +/- 53.50\n",
      "Episode length: 1.55 +/- 1.07\n",
      "Eval num_timesteps=309000, episode_reward=9.64 +/- 74.28\n",
      "Episode length: 2.30 +/- 1.45\n",
      "Eval num_timesteps=310000, episode_reward=-2.19 +/- 69.51\n",
      "Episode length: 2.10 +/- 1.26\n",
      "Eval num_timesteps=311000, episode_reward=13.61 +/- 78.08\n",
      "Episode length: 2.10 +/- 1.14\n",
      "Eval num_timesteps=312000, episode_reward=-10.09 +/- 65.82\n",
      "Episode length: 2.00 +/- 1.26\n",
      "Eval num_timesteps=313000, episode_reward=-1.33 +/- 68.78\n",
      "Episode length: 2.10 +/- 1.26\n",
      "Eval num_timesteps=314000, episode_reward=-24.92 +/- 54.71\n",
      "Episode length: 2.20 +/- 1.72\n",
      "Eval num_timesteps=315000, episode_reward=-23.44 +/- 60.30\n",
      "Episode length: 1.60 +/- 1.07\n",
      "Eval num_timesteps=316000, episode_reward=-19.46 +/- 65.39\n",
      "Episode length: 1.50 +/- 0.92\n",
      "Eval num_timesteps=317000, episode_reward=-14.30 +/- 59.95\n",
      "Episode length: 1.90 +/- 1.26\n",
      "Eval num_timesteps=318000, episode_reward=-39.59 +/- 50.48\n",
      "Episode length: 1.25 +/- 0.70\n",
      "Eval num_timesteps=319000, episode_reward=-11.83 +/- 70.00\n",
      "Episode length: 1.55 +/- 0.92\n",
      "Eval num_timesteps=320000, episode_reward=-14.58 +/- 64.90\n",
      "Episode length: 1.65 +/- 1.06\n",
      "Eval num_timesteps=321000, episode_reward=-27.46 +/- 59.67\n",
      "Episode length: 1.45 +/- 0.92\n",
      "Eval num_timesteps=322000, episode_reward=-35.65 +/- 53.12\n",
      "Episode length: 1.50 +/- 1.02\n",
      "Eval num_timesteps=323000, episode_reward=-2.80 +/- 70.01\n",
      "Episode length: 2.10 +/- 1.26\n",
      "Eval num_timesteps=324000, episode_reward=-14.72 +/- 66.48\n",
      "Episode length: 1.85 +/- 1.19\n",
      "Eval num_timesteps=325000, episode_reward=-1231.05 +/- 2994.78\n",
      "Episode length: 2.25 +/- 1.09\n",
      "Eval num_timesteps=326000, episode_reward=-23.00 +/- 66.28\n",
      "Episode length: 1.55 +/- 0.97\n",
      "Eval num_timesteps=327000, episode_reward=-18.60 +/- 61.33\n",
      "Episode length: 1.90 +/- 1.26\n",
      "Eval num_timesteps=328000, episode_reward=-210.35 +/- 136.02\n",
      "Episode length: 2.10 +/- 0.99\n",
      "Eval num_timesteps=329000, episode_reward=-22.98 +/- 55.75\n",
      "Episode length: 1.95 +/- 1.32\n",
      "Eval num_timesteps=330000, episode_reward=-10.69 +/- 64.80\n",
      "Episode length: 1.80 +/- 1.17\n",
      "Eval num_timesteps=331000, episode_reward=5.47 +/- 72.36\n",
      "Episode length: 2.20 +/- 1.25\n",
      "Eval num_timesteps=332000, episode_reward=-9.60 +/- 65.46\n",
      "Episode length: 2.00 +/- 1.26\n",
      "Eval num_timesteps=333000, episode_reward=-23.37 +/- 66.48\n",
      "Episode length: 1.55 +/- 0.97\n",
      "Eval num_timesteps=334000, episode_reward=2.33 +/- 63.25\n",
      "Episode length: 2.45 +/- 1.36\n",
      "Eval num_timesteps=335000, episode_reward=-2.07 +/- 69.39\n",
      "Episode length: 2.10 +/- 1.26\n",
      "Eval num_timesteps=336000, episode_reward=5.22 +/- 66.20\n",
      "Episode length: 2.15 +/- 1.28\n",
      "Eval num_timesteps=337000, episode_reward=-14.34 +/- 61.19\n",
      "Episode length: 2.05 +/- 1.32\n",
      "Eval num_timesteps=338000, episode_reward=-43.55 +/- 42.38\n",
      "Episode length: 1.40 +/- 0.97\n",
      "Eval num_timesteps=339000, episode_reward=-10.82 +/- 61.32\n",
      "Episode length: 2.20 +/- 1.36\n",
      "Eval num_timesteps=340000, episode_reward=-2.57 +/- 68.06\n",
      "Episode length: 1.85 +/- 1.15\n",
      "Eval num_timesteps=341000, episode_reward=30.90 +/- 72.87\n",
      "Episode length: 2.50 +/- 1.16\n",
      "New best mean reward!\n",
      "Eval num_timesteps=342000, episode_reward=2.68 +/- 72.40\n",
      "Episode length: 2.05 +/- 1.20\n",
      "Eval num_timesteps=343000, episode_reward=14.72 +/- 72.73\n",
      "Episode length: 2.30 +/- 1.23\n",
      "Eval num_timesteps=344000, episode_reward=0.87 +/- 67.57\n",
      "Episode length: 2.50 +/- 1.66\n",
      "Eval num_timesteps=345000, episode_reward=0.62 +/- 62.08\n",
      "Episode length: 2.80 +/- 1.78\n",
      "Eval num_timesteps=346000, episode_reward=-19.46 +/- 61.88\n",
      "Episode length: 1.90 +/- 1.26\n",
      "Eval num_timesteps=347000, episode_reward=-6.90 +/- 69.62\n",
      "Episode length: 2.10 +/- 1.48\n",
      "Eval num_timesteps=348000, episode_reward=2.09 +/- 58.18\n",
      "Episode length: 2.65 +/- 1.39\n",
      "Eval num_timesteps=349000, episode_reward=-22.63 +/- 66.07\n",
      "Episode length: 1.55 +/- 0.97\n",
      "Eval num_timesteps=350000, episode_reward=-22.74 +/- 55.57\n",
      "Episode length: 1.95 +/- 1.32\n",
      "Eval num_timesteps=351000, episode_reward=-26.64 +/- 60.45\n",
      "Episode length: 1.60 +/- 1.07\n",
      "Eval num_timesteps=352000, episode_reward=-23.16 +/- 54.98\n",
      "Episode length: 1.85 +/- 1.28\n",
      "Eval num_timesteps=353000, episode_reward=-13.97 +/- 55.37\n",
      "Episode length: 2.25 +/- 1.41\n",
      "Eval num_timesteps=354000, episode_reward=5.35 +/- 72.47\n",
      "Episode length: 2.20 +/- 1.25\n",
      "Eval num_timesteps=355000, episode_reward=-2.80 +/- 70.01\n",
      "Episode length: 2.10 +/- 1.26\n",
      "Eval num_timesteps=356000, episode_reward=-31.02 +/- 59.77\n",
      "Episode length: 1.45 +/- 0.92\n",
      "Eval num_timesteps=357000, episode_reward=-14.34 +/- 61.17\n",
      "Episode length: 2.05 +/- 1.32\n",
      "Eval num_timesteps=358000, episode_reward=-2.29 +/- 59.65\n",
      "Episode length: 2.50 +/- 1.40\n",
      "Eval num_timesteps=359000, episode_reward=-14.47 +/- 66.31\n",
      "Episode length: 1.85 +/- 1.19\n",
      "Eval num_timesteps=360000, episode_reward=-22.13 +/- 60.70\n",
      "Episode length: 1.75 +/- 1.18\n",
      "Eval num_timesteps=361000, episode_reward=-47.80 +/- 30.38\n",
      "Episode length: 1.45 +/- 1.07\n",
      "Eval num_timesteps=362000, episode_reward=14.12 +/- 68.83\n",
      "Episode length: 2.50 +/- 1.28\n",
      "Eval num_timesteps=363000, episode_reward=9.96 +/- 79.15\n",
      "Episode length: 1.95 +/- 1.07\n",
      "Eval num_timesteps=364000, episode_reward=8.03 +/- 65.19\n",
      "Episode length: 2.90 +/- 1.73\n",
      "Eval num_timesteps=365000, episode_reward=-7.19 +/- 75.32\n",
      "Episode length: 1.75 +/- 1.04\n",
      "Eval num_timesteps=366000, episode_reward=-39.04 +/- 44.51\n",
      "Episode length: 1.55 +/- 1.12\n",
      "Eval num_timesteps=367000, episode_reward=-18.85 +/- 61.49\n",
      "Episode length: 1.90 +/- 1.26\n",
      "Eval num_timesteps=368000, episode_reward=-7.89 +/- 64.85\n",
      "Episode length: 2.40 +/- 1.69\n",
      "Eval num_timesteps=369000, episode_reward=-23.12 +/- 66.35\n",
      "Episode length: 1.55 +/- 0.97\n",
      "Eval num_timesteps=370000, episode_reward=1.83 +/- 68.57\n",
      "Episode length: 2.25 +/- 1.30\n",
      "Eval num_timesteps=371000, episode_reward=-21.52 +/- 60.34\n",
      "Episode length: 1.75 +/- 1.18\n",
      "Eval num_timesteps=372000, episode_reward=-11.19 +/- 66.64\n",
      "Episode length: 2.00 +/- 1.26\n",
      "Eval num_timesteps=373000, episode_reward=-23.22 +/- 49.82\n",
      "Episode length: 2.15 +/- 1.42\n",
      "Eval num_timesteps=374000, episode_reward=-6.43 +/- 60.54\n",
      "Episode length: 2.35 +/- 1.39\n",
      "Eval num_timesteps=375000, episode_reward=-44.17 +/- 49.56\n",
      "Episode length: 1.20 +/- 0.60\n",
      "Eval num_timesteps=376000, episode_reward=1.59 +/- 68.80\n",
      "Episode length: 2.25 +/- 1.30\n",
      "Eval num_timesteps=377000, episode_reward=-26.88 +/- 60.58\n",
      "Episode length: 1.60 +/- 1.07\n",
      "Eval num_timesteps=378000, episode_reward=-18.48 +/- 61.24\n",
      "Episode length: 1.90 +/- 1.26\n",
      "Eval num_timesteps=379000, episode_reward=-28.35 +/- 61.35\n",
      "Episode length: 1.60 +/- 1.07\n",
      "Eval num_timesteps=380000, episode_reward=10.23 +/- 65.59\n",
      "Episode length: 2.55 +/- 1.32\n",
      "Eval num_timesteps=381000, episode_reward=-23.29 +/- 59.07\n",
      "Episode length: 2.00 +/- 1.55\n",
      "Eval num_timesteps=382000, episode_reward=-22.74 +/- 55.57\n",
      "Episode length: 1.95 +/- 1.32\n",
      "Eval num_timesteps=383000, episode_reward=-18.85 +/- 61.47\n",
      "Episode length: 1.90 +/- 1.26\n",
      "Eval num_timesteps=384000, episode_reward=-24.45 +/- 65.60\n",
      "Episode length: 1.35 +/- 0.73\n",
      "Eval num_timesteps=385000, episode_reward=1.78 +/- 71.13\n",
      "Episode length: 1.75 +/- 1.04\n",
      "Eval num_timesteps=386000, episode_reward=-15.19 +/- 65.33\n",
      "Episode length: 1.65 +/- 1.06\n",
      "Eval num_timesteps=387000, episode_reward=-23.36 +/- 61.45\n",
      "Episode length: 1.75 +/- 1.18\n",
      "Eval num_timesteps=388000, episode_reward=-6.33 +/- 70.28\n",
      "Episode length: 1.95 +/- 1.20\n",
      "Eval num_timesteps=389000, episode_reward=6.08 +/- 71.81\n",
      "Episode length: 2.20 +/- 1.25\n",
      "Eval num_timesteps=390000, episode_reward=-14.11 +/- 66.07\n",
      "Episode length: 1.85 +/- 1.19\n",
      "Eval num_timesteps=391000, episode_reward=-21.75 +/- 48.69\n",
      "Episode length: 2.15 +/- 1.42\n",
      "Eval num_timesteps=392000, episode_reward=-15.95 +/- 71.92\n",
      "Episode length: 1.65 +/- 1.01\n",
      "Eval num_timesteps=393000, episode_reward=-15.59 +/- 71.68\n",
      "Episode length: 1.65 +/- 1.01\n",
      "Eval num_timesteps=394000, episode_reward=-13.49 +/- 60.52\n",
      "Episode length: 2.05 +/- 1.32\n",
      "Eval num_timesteps=395000, episode_reward=-13.85 +/- 60.82\n",
      "Episode length: 2.05 +/- 1.32\n",
      "Eval num_timesteps=396000, episode_reward=-11.32 +/- 71.38\n",
      "Episode length: 1.80 +/- 1.12\n",
      "Eval num_timesteps=397000, episode_reward=-23.11 +/- 61.31\n",
      "Episode length: 1.75 +/- 1.18\n",
      "Eval num_timesteps=398000, episode_reward=-2.17 +/- 59.52\n",
      "Episode length: 2.50 +/- 1.40\n",
      "Eval num_timesteps=399000, episode_reward=-22.62 +/- 61.02\n",
      "Episode length: 1.75 +/- 1.18\n",
      "Eval num_timesteps=400000, episode_reward=-30.28 +/- 53.74\n",
      "Episode length: 1.65 +/- 1.15\n",
      "Eval num_timesteps=401000, episode_reward=-27.37 +/- 55.30\n",
      "Episode length: 1.80 +/- 1.25\n",
      "Eval num_timesteps=402000, episode_reward=10.71 +/- 74.31\n",
      "Episode length: 2.15 +/- 1.19\n",
      "Eval num_timesteps=403000, episode_reward=-22.83 +/- 59.93\n",
      "Episode length: 1.60 +/- 1.07\n",
      "Eval num_timesteps=404000, episode_reward=34.90 +/- 78.78\n",
      "Episode length: 2.25 +/- 1.04\n",
      "New best mean reward!\n",
      "Eval num_timesteps=405000, episode_reward=-14.21 +/- 55.58\n",
      "Episode length: 2.25 +/- 1.41\n",
      "Eval num_timesteps=406000, episode_reward=-19.22 +/- 61.72\n",
      "Episode length: 1.90 +/- 1.26\n",
      "Eval num_timesteps=407000, episode_reward=19.00 +/- 60.83\n",
      "Episode length: 2.85 +/- 1.28\n",
      "Eval num_timesteps=408000, episode_reward=2.33 +/- 63.25\n",
      "Episode length: 2.45 +/- 1.36\n",
      "Eval num_timesteps=409000, episode_reward=-10.21 +/- 65.92\n",
      "Episode length: 2.00 +/- 1.26\n",
      "Eval num_timesteps=410000, episode_reward=-18.83 +/- 49.92\n",
      "Episode length: 2.30 +/- 1.45\n",
      "Eval num_timesteps=411000, episode_reward=-26.51 +/- 54.78\n",
      "Episode length: 1.80 +/- 1.25\n",
      "Eval num_timesteps=412000, episode_reward=-26.89 +/- 60.55\n",
      "Episode length: 1.60 +/- 1.07\n",
      "Eval num_timesteps=413000, episode_reward=-7.88 +/- 70.36\n",
      "Episode length: 2.10 +/- 1.48\n",
      "Eval num_timesteps=414000, episode_reward=5.97 +/- 67.28\n",
      "Episode length: 2.40 +/- 1.32\n",
      "Eval num_timesteps=415000, episode_reward=-13.85 +/- 60.82\n",
      "Episode length: 2.05 +/- 1.32\n",
      "Eval num_timesteps=416000, episode_reward=-13.36 +/- 60.44\n",
      "Episode length: 2.05 +/- 1.32\n",
      "Eval num_timesteps=417000, episode_reward=2.20 +/- 68.25\n",
      "Episode length: 2.25 +/- 1.30\n",
      "Eval num_timesteps=418000, episode_reward=5.84 +/- 72.03\n",
      "Episode length: 2.20 +/- 1.25\n",
      "Eval num_timesteps=419000, episode_reward=-5.71 +/- 65.03\n",
      "Episode length: 2.15 +/- 1.31\n",
      "Eval num_timesteps=420000, episode_reward=-1.20 +/- 63.83\n",
      "Episode length: 2.30 +/- 1.35\n",
      "Eval num_timesteps=421000, episode_reward=-1.82 +/- 69.20\n",
      "Episode length: 2.10 +/- 1.26\n",
      "Eval num_timesteps=422000, episode_reward=-3.82 +/- 63.78\n",
      "Episode length: 2.60 +/- 1.74\n",
      "Eval num_timesteps=423000, episode_reward=-27.62 +/- 60.96\n",
      "Episode length: 1.60 +/- 1.07\n",
      "Eval num_timesteps=424000, episode_reward=2.06 +/- 66.58\n",
      "Episode length: 2.00 +/- 1.22\n",
      "Eval num_timesteps=425000, episode_reward=-14.91 +/- 60.40\n",
      "Episode length: 1.90 +/- 1.26\n",
      "Eval num_timesteps=426000, episode_reward=-18.61 +/- 61.31\n",
      "Episode length: 1.90 +/- 1.26\n",
      "Eval num_timesteps=427000, episode_reward=26.52 +/- 71.34\n",
      "Episode length: 2.55 +/- 1.20\n",
      "Eval num_timesteps=428000, episode_reward=-14.96 +/- 66.65\n",
      "Episode length: 1.85 +/- 1.19\n",
      "Eval num_timesteps=429000, episode_reward=-5.70 +/- 65.05\n",
      "Episode length: 2.15 +/- 1.31\n",
      "Eval num_timesteps=430000, episode_reward=1.70 +/- 73.24\n",
      "Episode length: 2.05 +/- 1.20\n",
      "Eval num_timesteps=431000, episode_reward=-19.23 +/- 66.74\n",
      "Episode length: 1.70 +/- 1.10\n",
      "Eval num_timesteps=432000, episode_reward=-6.82 +/- 70.63\n",
      "Episode length: 1.95 +/- 1.20\n",
      "Eval num_timesteps=433000, episode_reward=-1.94 +/- 69.31\n",
      "Episode length: 2.10 +/- 1.26\n",
      "Eval num_timesteps=434000, episode_reward=1.71 +/- 68.70\n",
      "Episode length: 2.25 +/- 1.30\n",
      "Eval num_timesteps=435000, episode_reward=-2.68 +/- 69.90\n",
      "Episode length: 2.10 +/- 1.26\n",
      "Eval num_timesteps=436000, episode_reward=13.86 +/- 73.60\n",
      "Episode length: 2.30 +/- 1.23\n",
      "Eval num_timesteps=437000, episode_reward=1.46 +/- 68.90\n",
      "Episode length: 2.25 +/- 1.30\n",
      "Eval num_timesteps=438000, episode_reward=-5.94 +/- 60.11\n",
      "Episode length: 2.35 +/- 1.39\n",
      "Eval num_timesteps=439000, episode_reward=-14.67 +/- 60.21\n",
      "Episode length: 1.90 +/- 1.26\n",
      "Eval num_timesteps=440000, episode_reward=-22.06 +/- 54.23\n",
      "Episode length: 1.85 +/- 1.28\n",
      "Eval num_timesteps=441000, episode_reward=-28.19 +/- 60.05\n",
      "Episode length: 1.45 +/- 0.92\n",
      "Eval num_timesteps=442000, episode_reward=1.20 +/- 67.36\n",
      "Episode length: 2.00 +/- 1.22\n",
      "Eval num_timesteps=443000, episode_reward=-6.82 +/- 70.65\n",
      "Episode length: 1.95 +/- 1.20\n",
      "Eval num_timesteps=444000, episode_reward=-14.11 +/- 66.06\n",
      "Episode length: 1.85 +/- 1.19\n",
      "Eval num_timesteps=445000, episode_reward=-5.71 +/- 65.03\n",
      "Episode length: 2.15 +/- 1.31\n",
      "Eval num_timesteps=446000, episode_reward=-19.54 +/- 60.75\n",
      "Episode length: 1.75 +/- 1.18\n",
      "Eval num_timesteps=447000, episode_reward=-8.01 +/- 64.94\n",
      "Episode length: 2.40 +/- 1.69\n",
      "Eval num_timesteps=448000, episode_reward=-10.70 +/- 66.29\n",
      "Episode length: 2.00 +/- 1.26\n",
      "Eval num_timesteps=449000, episode_reward=-23.22 +/- 49.80\n",
      "Episode length: 2.15 +/- 1.42\n",
      "Eval num_timesteps=450000, episode_reward=-3.57 +/- 69.17\n",
      "Episode length: 2.30 +/- 1.58\n",
      "Eval num_timesteps=451000, episode_reward=14.84 +/- 72.61\n",
      "Episode length: 2.30 +/- 1.23\n",
      "Eval num_timesteps=452000, episode_reward=-6.45 +/- 70.37\n",
      "Episode length: 1.95 +/- 1.20\n",
      "Eval num_timesteps=453000, episode_reward=-2.06 +/- 64.60\n",
      "Episode length: 2.30 +/- 1.35\n",
      "Eval num_timesteps=454000, episode_reward=-10.37 +/- 64.48\n",
      "Episode length: 2.20 +/- 1.60\n",
      "Eval num_timesteps=455000, episode_reward=-5.94 +/- 60.10\n",
      "Episode length: 2.35 +/- 1.39\n",
      "Eval num_timesteps=456000, episode_reward=1.32 +/- 77.79\n",
      "Episode length: 1.85 +/- 1.06\n",
      "Eval num_timesteps=457000, episode_reward=-14.24 +/- 70.83\n",
      "Episode length: 1.65 +/- 1.01\n",
      "Eval num_timesteps=458000, episode_reward=-15.22 +/- 71.46\n",
      "Episode length: 1.65 +/- 1.01\n",
      "Eval num_timesteps=459000, episode_reward=-10.46 +/- 66.10\n",
      "Episode length: 2.00 +/- 1.26\n",
      "Eval num_timesteps=460000, episode_reward=-18.48 +/- 61.24\n",
      "Episode length: 1.90 +/- 1.26\n",
      "Eval num_timesteps=461000, episode_reward=14.60 +/- 72.86\n",
      "Episode length: 2.30 +/- 1.23\n",
      "Eval num_timesteps=462000, episode_reward=-3.94 +/- 66.92\n",
      "Episode length: 2.15 +/- 1.31\n",
      "Eval num_timesteps=463000, episode_reward=5.83 +/- 64.62\n",
      "Episode length: 2.45 +/- 1.36\n",
      "Eval num_timesteps=464000, episode_reward=-18.82 +/- 65.77\n",
      "Episode length: 1.75 +/- 1.18\n",
      "Eval num_timesteps=465000, episode_reward=-9.86 +/- 70.63\n",
      "Episode length: 1.85 +/- 1.19\n",
      "Eval num_timesteps=466000, episode_reward=5.49 +/- 74.36\n",
      "Episode length: 2.15 +/- 1.31\n",
      "Eval num_timesteps=467000, episode_reward=-0.60 +/- 73.54\n",
      "Episode length: 2.00 +/- 1.26\n",
      "Eval num_timesteps=468000, episode_reward=8.17 +/- 75.11\n",
      "Episode length: 2.10 +/- 1.26\n",
      "Eval num_timesteps=469000, episode_reward=2.29 +/- 70.05\n",
      "Episode length: 2.15 +/- 1.39\n",
      "Eval num_timesteps=470000, episode_reward=7.68 +/- 75.54\n",
      "Episode length: 2.10 +/- 1.26\n",
      "Eval num_timesteps=471000, episode_reward=14.99 +/- 74.48\n",
      "Episode length: 2.25 +/- 1.30\n",
      "Eval num_timesteps=472000, episode_reward=-2.17 +/- 71.59\n",
      "Episode length: 2.05 +/- 1.32\n",
      "Eval num_timesteps=473000, episode_reward=11.23 +/- 71.84\n",
      "Episode length: 2.35 +/- 1.39\n",
      "Eval num_timesteps=474000, episode_reward=0.02 +/- 73.04\n",
      "Episode length: 2.00 +/- 1.26\n",
      "Eval num_timesteps=475000, episode_reward=-17.26 +/- 66.42\n",
      "Episode length: 1.80 +/- 1.25\n",
      "Eval num_timesteps=476000, episode_reward=17.55 +/- 71.42\n",
      "Episode length: 2.50 +/- 1.40\n",
      "Eval num_timesteps=477000, episode_reward=-3.27 +/- 72.46\n",
      "Episode length: 2.05 +/- 1.32\n",
      "Eval num_timesteps=478000, episode_reward=7.43 +/- 75.77\n",
      "Episode length: 2.10 +/- 1.26\n",
      "Eval num_timesteps=479000, episode_reward=-9.97 +/- 71.30\n",
      "Episode length: 1.90 +/- 1.26\n",
      "Eval num_timesteps=480000, episode_reward=-7.89 +/- 69.80\n",
      "Episode length: 1.90 +/- 1.26\n",
      "Eval num_timesteps=481000, episode_reward=-1282.96 +/- 3037.54\n",
      "Episode length: 2.35 +/- 1.59\n",
      "Eval num_timesteps=482000, episode_reward=13.29 +/- 73.02\n",
      "Episode length: 2.30 +/- 1.35\n",
      "Eval num_timesteps=483000, episode_reward=-29.92 +/- 59.74\n",
      "Episode length: 1.50 +/- 1.02\n",
      "Eval num_timesteps=484000, episode_reward=-8.13 +/- 69.99\n",
      "Episode length: 1.90 +/- 1.26\n",
      "Eval num_timesteps=485000, episode_reward=-1.68 +/- 71.20\n",
      "Episode length: 2.05 +/- 1.32\n",
      "Eval num_timesteps=486000, episode_reward=-1.68 +/- 71.20\n",
      "Episode length: 2.05 +/- 1.32\n",
      "Eval num_timesteps=487000, episode_reward=-18.22 +/- 63.44\n",
      "Episode length: 1.85 +/- 1.31\n",
      "Eval num_timesteps=488000, episode_reward=-3.99 +/- 69.75\n",
      "Episode length: 2.10 +/- 1.37\n",
      "Eval num_timesteps=489000, episode_reward=-16.77 +/- 66.10\n",
      "Episode length: 1.80 +/- 1.25\n",
      "Eval num_timesteps=490000, episode_reward=19.70 +/- 78.86\n",
      "Episode length: 2.10 +/- 1.14\n",
      "Eval num_timesteps=491000, episode_reward=4.77 +/- 71.81\n",
      "Episode length: 2.20 +/- 1.36\n",
      "Eval num_timesteps=492000, episode_reward=12.92 +/- 73.37\n",
      "Episode length: 2.30 +/- 1.35\n",
      "Eval num_timesteps=493000, episode_reward=-17.26 +/- 66.41\n",
      "Episode length: 1.80 +/- 1.25\n",
      "Eval num_timesteps=494000, episode_reward=22.40 +/- 75.69\n",
      "Episode length: 2.35 +/- 1.28\n",
      "Eval num_timesteps=495000, episode_reward=-23.10 +/- 63.20\n",
      "Episode length: 1.65 +/- 1.15\n",
      "Eval num_timesteps=496000, episode_reward=-9.60 +/- 71.03\n",
      "Episode length: 1.90 +/- 1.26\n",
      "Eval num_timesteps=497000, episode_reward=9.00 +/- 77.41\n",
      "Episode length: 2.05 +/- 1.20\n",
      "Eval num_timesteps=498000, episode_reward=7.43 +/- 75.76\n",
      "Episode length: 2.10 +/- 1.26\n",
      "Eval num_timesteps=499000, episode_reward=24.69 +/- 79.26\n",
      "Episode length: 2.25 +/- 1.18\n",
      "Eval num_timesteps=500000, episode_reward=-2.54 +/- 71.88\n",
      "Episode length: 2.05 +/- 1.32\n",
      "Eval num_timesteps=501000, episode_reward=-27.02 +/- 67.86\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=502000, episode_reward=-21.74 +/- 67.61\n",
      "Episode length: 1.60 +/- 1.07\n",
      "Eval num_timesteps=503000, episode_reward=9.71 +/- 71.06\n",
      "Episode length: 2.40 +/- 1.43\n",
      "Eval num_timesteps=504000, episode_reward=-10.62 +/- 69.46\n",
      "Episode length: 1.95 +/- 1.32\n",
      "Eval num_timesteps=505000, episode_reward=-6.31 +/- 73.88\n",
      "Episode length: 1.85 +/- 1.19\n",
      "Eval num_timesteps=506000, episode_reward=-19.95 +/- 70.51\n",
      "Episode length: 1.55 +/- 0.97\n",
      "Eval num_timesteps=507000, episode_reward=-1.55 +/- 71.10\n",
      "Episode length: 2.05 +/- 1.32\n",
      "Eval num_timesteps=508000, episode_reward=-8.74 +/- 70.43\n",
      "Episode length: 1.90 +/- 1.26\n",
      "Eval num_timesteps=509000, episode_reward=-13.25 +/- 70.63\n",
      "Episode length: 1.70 +/- 1.10\n",
      "Eval num_timesteps=510000, episode_reward=7.80 +/- 75.43\n",
      "Episode length: 2.10 +/- 1.26\n",
      "Eval num_timesteps=511000, episode_reward=27.77 +/- 72.32\n",
      "Episode length: 2.55 +/- 1.32\n",
      "Eval num_timesteps=512000, episode_reward=37.73 +/- 72.10\n",
      "Episode length: 2.60 +/- 1.24\n",
      "New best mean reward!\n",
      "Eval num_timesteps=513000, episode_reward=7.55 +/- 75.66\n",
      "Episode length: 2.10 +/- 1.26\n",
      "Eval num_timesteps=514000, episode_reward=12.06 +/- 74.22\n",
      "Episode length: 2.30 +/- 1.35\n",
      "Eval num_timesteps=515000, episode_reward=-7.05 +/- 72.47\n",
      "Episode length: 1.85 +/- 1.19\n",
      "Eval num_timesteps=516000, episode_reward=-14.60 +/- 71.51\n",
      "Episode length: 1.70 +/- 1.10\n",
      "Eval num_timesteps=517000, episode_reward=16.66 +/- 78.91\n",
      "Episode length: 2.15 +/- 1.19\n",
      "Eval num_timesteps=518000, episode_reward=-420.43 +/- 1742.04\n",
      "Episode length: 1.90 +/- 1.37\n",
      "Eval num_timesteps=519000, episode_reward=-1222.90 +/- 2851.22\n",
      "Episode length: 2.05 +/- 1.43\n",
      "Eval num_timesteps=520000, episode_reward=-2039.68 +/- 3446.39\n",
      "Episode length: 1.90 +/- 1.37\n",
      "Eval num_timesteps=521000, episode_reward=-1237.70 +/- 2844.75\n",
      "Episode length: 1.75 +/- 1.30\n",
      "Eval num_timesteps=522000, episode_reward=-1222.78 +/- 2851.27\n",
      "Episode length: 2.05 +/- 1.43\n",
      "Eval num_timesteps=523000, episode_reward=-1230.12 +/- 2848.07\n",
      "Episode length: 1.90 +/- 1.37\n",
      "Eval num_timesteps=524000, episode_reward=-2415.30 +/- 3662.20\n",
      "Episode length: 2.50 +/- 1.50\n",
      "Eval num_timesteps=525000, episode_reward=-21.33 +/- 63.27\n",
      "Episode length: 2.55 +/- 1.75\n",
      "Eval num_timesteps=526000, episode_reward=11.35 +/- 71.72\n",
      "Episode length: 2.35 +/- 1.39\n",
      "Eval num_timesteps=527000, episode_reward=-6.33 +/- 75.10\n",
      "Episode length: 1.80 +/- 1.12\n",
      "Eval num_timesteps=528000, episode_reward=12.19 +/- 74.10\n",
      "Episode length: 2.30 +/- 1.35\n",
      "Eval num_timesteps=529000, episode_reward=-8.38 +/- 70.16\n",
      "Episode length: 1.90 +/- 1.26\n",
      "Eval num_timesteps=530000, episode_reward=-12.54 +/- 73.41\n",
      "Episode length: 1.65 +/- 1.01\n",
      "Eval num_timesteps=531000, episode_reward=-11.52 +/- 65.59\n",
      "Episode length: 2.00 +/- 1.38\n",
      "Eval num_timesteps=532000, episode_reward=23.24 +/- 77.83\n",
      "Episode length: 2.30 +/- 1.23\n",
      "Eval num_timesteps=533000, episode_reward=3.39 +/- 79.50\n",
      "Episode length: 1.85 +/- 1.06\n",
      "Eval num_timesteps=534000, episode_reward=36.00 +/- 77.81\n",
      "Episode length: 2.30 +/- 1.10\n",
      "Eval num_timesteps=535000, episode_reward=15.46 +/- 77.09\n",
      "Episode length: 2.20 +/- 1.25\n",
      "Eval num_timesteps=536000, episode_reward=-3.74 +/- 69.56\n",
      "Episode length: 2.10 +/- 1.37\n",
      "Eval num_timesteps=537000, episode_reward=-13.15 +/- 73.80\n",
      "Episode length: 1.65 +/- 1.01\n",
      "Eval num_timesteps=538000, episode_reward=-2.17 +/- 71.58\n",
      "Episode length: 2.05 +/- 1.32\n",
      "Eval num_timesteps=539000, episode_reward=16.32 +/- 76.26\n",
      "Episode length: 2.20 +/- 1.25\n",
      "Eval num_timesteps=540000, episode_reward=32.89 +/- 68.70\n",
      "Episode length: 2.75 +/- 1.34\n",
      "Eval num_timesteps=541000, episode_reward=11.94 +/- 74.34\n",
      "Episode length: 2.30 +/- 1.35\n",
      "Eval num_timesteps=542000, episode_reward=-1265.10 +/- 3047.59\n",
      "Episode length: 2.50 +/- 1.50\n",
      "Eval num_timesteps=543000, episode_reward=-1666.74 +/- 3372.16\n",
      "Episode length: 2.85 +/- 1.39\n",
      "Eval num_timesteps=544000, episode_reward=-2111.05 +/- 3702.12\n",
      "Episode length: 3.20 +/- 1.57\n",
      "Eval num_timesteps=545000, episode_reward=-848.39 +/- 2558.98\n",
      "Episode length: 2.45 +/- 1.53\n",
      "Eval num_timesteps=546000, episode_reward=-59.28 +/- 135.66\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=547000, episode_reward=-1737.95 +/- 3330.18\n",
      "Episode length: 2.75 +/- 1.58\n",
      "Eval num_timesteps=548000, episode_reward=-36.96 +/- 73.59\n",
      "Episode length: 1.65 +/- 0.85\n",
      "Eval num_timesteps=549000, episode_reward=-60.54 +/- 75.79\n",
      "Episode length: 1.95 +/- 0.92\n",
      "Eval num_timesteps=550000, episode_reward=-68.86 +/- 76.65\n",
      "Episode length: 2.15 +/- 0.73\n",
      "Eval num_timesteps=551000, episode_reward=-29.32 +/- 100.81\n",
      "Episode length: 2.40 +/- 0.73\n",
      "Eval num_timesteps=552000, episode_reward=-26.93 +/- 89.03\n",
      "Episode length: 2.10 +/- 0.94\n",
      "Eval num_timesteps=553000, episode_reward=-22.56 +/- 85.73\n",
      "Episode length: 1.95 +/- 0.97\n",
      "Eval num_timesteps=554000, episode_reward=-69.20 +/- 64.93\n",
      "Episode length: 2.00 +/- 0.89\n",
      "Eval num_timesteps=555000, episode_reward=-3641.14 +/- 4062.77\n",
      "Episode length: 2.50 +/- 0.87\n",
      "Eval num_timesteps=556000, episode_reward=-3201.98 +/- 3923.13\n",
      "Episode length: 2.45 +/- 0.80\n",
      "Eval num_timesteps=557000, episode_reward=-62.18 +/- 75.47\n",
      "Episode length: 2.10 +/- 0.83\n",
      "Eval num_timesteps=558000, episode_reward=-62.18 +/- 75.47\n",
      "Episode length: 2.10 +/- 0.83\n",
      "Eval num_timesteps=559000, episode_reward=-56.97 +/- 86.15\n",
      "Episode length: 2.35 +/- 0.85\n",
      "Eval num_timesteps=560000, episode_reward=-28.01 +/- 92.11\n",
      "Episode length: 2.15 +/- 0.85\n",
      "Eval num_timesteps=561000, episode_reward=-51.23 +/- 71.32\n",
      "Episode length: 1.80 +/- 0.87\n",
      "Eval num_timesteps=562000, episode_reward=-63.32 +/- 62.75\n",
      "Episode length: 1.80 +/- 0.93\n",
      "Eval num_timesteps=563000, episode_reward=-57.18 +/- 85.43\n",
      "Episode length: 2.30 +/- 0.78\n",
      "Eval num_timesteps=564000, episode_reward=-2876.93 +/- 3897.49\n",
      "Episode length: 2.30 +/- 1.55\n",
      "Eval num_timesteps=565000, episode_reward=-423.20 +/- 1741.91\n",
      "Episode length: 2.10 +/- 0.77\n",
      "Eval num_timesteps=566000, episode_reward=-37.62 +/- 87.69\n",
      "Episode length: 2.10 +/- 0.89\n",
      "Eval num_timesteps=567000, episode_reward=-74.93 +/- 49.71\n",
      "Episode length: 1.80 +/- 0.87\n",
      "Eval num_timesteps=568000, episode_reward=-50.85 +/- 83.52\n",
      "Episode length: 2.15 +/- 0.91\n",
      "Eval num_timesteps=569000, episode_reward=-48.19 +/- 81.30\n",
      "Episode length: 2.00 +/- 0.84\n",
      "Eval num_timesteps=570000, episode_reward=-62.52 +/- 60.96\n",
      "Episode length: 1.70 +/- 0.78\n",
      "Eval num_timesteps=571000, episode_reward=-1226.10 +/- 2849.05\n",
      "Episode length: 1.90 +/- 0.89\n",
      "Eval num_timesteps=572000, episode_reward=-60.16 +/- 74.12\n",
      "Episode length: 2.00 +/- 0.77\n",
      "Eval num_timesteps=573000, episode_reward=-28.53 +/- 92.96\n",
      "Episode length: 2.20 +/- 0.93\n",
      "Eval num_timesteps=574000, episode_reward=-54.35 +/- 73.06\n",
      "Episode length: 1.90 +/- 0.94\n",
      "Eval num_timesteps=575000, episode_reward=-19.00 +/- 95.74\n",
      "Episode length: 2.20 +/- 0.87\n",
      "Eval num_timesteps=576000, episode_reward=-20.40 +/- 69.73\n",
      "Episode length: 1.50 +/- 0.87\n",
      "Eval num_timesteps=577000, episode_reward=-1.79 +/- 72.46\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=578000, episode_reward=-48.46 +/- 36.73\n",
      "Episode length: 1.20 +/- 0.60\n",
      "Eval num_timesteps=579000, episode_reward=-7.42 +/- 71.69\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=580000, episode_reward=-5.80 +/- 75.16\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=581000, episode_reward=-23.26 +/- 66.28\n",
      "Episode length: 1.50 +/- 0.87\n",
      "Eval num_timesteps=582000, episode_reward=15.06 +/- 76.46\n",
      "Episode length: 2.00 +/- 1.00\n",
      "Eval num_timesteps=583000, episode_reward=5.25 +/- 66.97\n",
      "Episode length: 2.00 +/- 1.00\n",
      "Eval num_timesteps=584000, episode_reward=-11.25 +/- 69.59\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=585000, episode_reward=1.83 +/- 78.64\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=586000, episode_reward=6.08 +/- 75.45\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=587000, episode_reward=1.59 +/- 78.83\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=588000, episode_reward=-7.36 +/- 67.08\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=589000, episode_reward=10.93 +/- 79.98\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=590000, episode_reward=-21.02 +/- 70.08\n",
      "Episode length: 1.50 +/- 0.87\n",
      "Eval num_timesteps=591000, episode_reward=10.83 +/- 71.39\n",
      "Episode length: 2.00 +/- 1.00\n",
      "Eval num_timesteps=592000, episode_reward=-20.87 +/- 61.46\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=593000, episode_reward=6.33 +/- 75.24\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=594000, episode_reward=9.85 +/- 72.34\n",
      "Episode length: 2.00 +/- 1.00\n",
      "Eval num_timesteps=595000, episode_reward=-23.97 +/- 55.94\n",
      "Episode length: 1.60 +/- 0.92\n",
      "Eval num_timesteps=596000, episode_reward=-25.12 +/- 62.00\n",
      "Episode length: 1.50 +/- 0.87\n",
      "Eval num_timesteps=597000, episode_reward=0.45 +/- 75.29\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=598000, episode_reward=-21.61 +/- 60.32\n",
      "Episode length: 1.60 +/- 0.92\n",
      "Eval num_timesteps=599000, episode_reward=-2.77 +/- 73.24\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=600000, episode_reward=-9.01 +/- 72.82\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=601000, episode_reward=4.80 +/- 80.57\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=602000, episode_reward=-31.93 +/- 55.51\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=603000, episode_reward=10.27 +/- 72.53\n",
      "Episode length: 1.75 +/- 0.83\n",
      "Eval num_timesteps=604000, episode_reward=18.52 +/- 66.14\n",
      "Episode length: 2.05 +/- 0.92\n",
      "Eval num_timesteps=605000, episode_reward=16.65 +/- 64.05\n",
      "Episode length: 2.10 +/- 0.94\n",
      "Eval num_timesteps=606000, episode_reward=-3.49 +/- 70.46\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=607000, episode_reward=12.76 +/- 73.95\n",
      "Episode length: 1.70 +/- 0.78\n",
      "Eval num_timesteps=608000, episode_reward=18.02 +/- 73.54\n",
      "Episode length: 1.80 +/- 0.81\n",
      "Eval num_timesteps=609000, episode_reward=-7.72 +/- 65.56\n",
      "Episode length: 1.70 +/- 0.90\n",
      "Eval num_timesteps=610000, episode_reward=4.88 +/- 72.47\n",
      "Episode length: 1.65 +/- 0.79\n",
      "Eval num_timesteps=611000, episode_reward=-20.71 +/- 63.35\n",
      "Episode length: 1.50 +/- 0.81\n",
      "Eval num_timesteps=612000, episode_reward=-1.12 +/- 72.52\n",
      "Episode length: 1.55 +/- 0.74\n",
      "Eval num_timesteps=613000, episode_reward=-2.10 +/- 73.30\n",
      "Episode length: 1.55 +/- 0.74\n",
      "Eval num_timesteps=614000, episode_reward=28.14 +/- 74.80\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Eval num_timesteps=615000, episode_reward=-5.86 +/- 74.96\n",
      "Episode length: 1.40 +/- 0.58\n",
      "Eval num_timesteps=616000, episode_reward=-27.61 +/- 64.55\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=617000, episode_reward=-25.48 +/- 60.25\n",
      "Episode length: 1.40 +/- 0.73\n",
      "Eval num_timesteps=618000, episode_reward=12.45 +/- 74.48\n",
      "Episode length: 2.00 +/- 1.00\n",
      "Eval num_timesteps=619000, episode_reward=-17.61 +/- 75.86\n",
      "Episode length: 1.90 +/- 0.89\n",
      "Eval num_timesteps=620000, episode_reward=-6.22 +/- 78.31\n",
      "Episode length: 2.10 +/- 0.89\n",
      "Eval num_timesteps=621000, episode_reward=13.20 +/- 83.51\n",
      "Episode length: 2.10 +/- 0.94\n",
      "Eval num_timesteps=622000, episode_reward=-5.75 +/- 86.01\n",
      "Episode length: 2.00 +/- 0.89\n",
      "Eval num_timesteps=623000, episode_reward=-43.18 +/- 67.38\n",
      "Episode length: 1.70 +/- 0.78\n",
      "Eval num_timesteps=624000, episode_reward=-1.99 +/- 79.13\n",
      "Episode length: 2.05 +/- 0.92\n",
      "Eval num_timesteps=625000, episode_reward=-15.43 +/- 82.86\n",
      "Episode length: 2.10 +/- 0.83\n",
      "Eval num_timesteps=626000, episode_reward=-7.40 +/- 86.23\n",
      "Episode length: 2.20 +/- 0.81\n",
      "Eval num_timesteps=627000, episode_reward=3.40 +/- 77.84\n",
      "Episode length: 2.15 +/- 0.91\n",
      "Eval num_timesteps=628000, episode_reward=-9.18 +/- 81.97\n",
      "Episode length: 2.00 +/- 0.89\n",
      "Eval num_timesteps=629000, episode_reward=0.92 +/- 74.84\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=630000, episode_reward=14.23 +/- 79.90\n",
      "Episode length: 2.00 +/- 1.00\n",
      "Eval num_timesteps=631000, episode_reward=-42.23 +/- 43.50\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=632000, episode_reward=-15.43 +/- 66.98\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=633000, episode_reward=7.53 +/- 82.57\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=634000, episode_reward=4.46 +/- 73.61\n",
      "Episode length: 2.00 +/- 1.00\n",
      "Eval num_timesteps=635000, episode_reward=-21.43 +/- 54.03\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=636000, episode_reward=-28.99 +/- 54.11\n",
      "Episode length: 1.60 +/- 0.92\n",
      "Eval num_timesteps=637000, episode_reward=-17.02 +/- 81.79\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=638000, episode_reward=-54.18 +/- 57.56\n",
      "Episode length: 1.50 +/- 0.87\n",
      "Eval num_timesteps=639000, episode_reward=-48.88 +/- 69.20\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=640000, episode_reward=-43.82 +/- 78.82\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=641000, episode_reward=-54.81 +/- 72.88\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=642000, episode_reward=-53.81 +/- 57.53\n",
      "Episode length: 1.50 +/- 0.87\n",
      "Eval num_timesteps=643000, episode_reward=-12.58 +/- 88.58\n",
      "Episode length: 2.00 +/- 1.00\n",
      "Eval num_timesteps=644000, episode_reward=-21.58 +/- 85.28\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=645000, episode_reward=-24.07 +/- 87.58\n",
      "Episode length: 2.00 +/- 1.00\n",
      "Eval num_timesteps=646000, episode_reward=-43.21 +/- 78.69\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=647000, episode_reward=-47.77 +/- 69.01\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=648000, episode_reward=-36.90 +/- 74.26\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=649000, episode_reward=-23.33 +/- 87.27\n",
      "Episode length: 2.00 +/- 1.00\n",
      "Eval num_timesteps=650000, episode_reward=-48.14 +/- 69.08\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=651000, episode_reward=-53.32 +/- 57.48\n",
      "Episode length: 1.50 +/- 0.87\n",
      "Eval num_timesteps=652000, episode_reward=-43.58 +/- 78.76\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=653000, episode_reward=-61.86 +/- 75.89\n",
      "Episode length: 2.10 +/- 0.99\n",
      "Eval num_timesteps=654000, episode_reward=-31.11 +/- 82.33\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=655000, episode_reward=-26.28 +/- 67.50\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=656000, episode_reward=-5.17 +/- 76.08\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=657000, episode_reward=-19.21 +/- 67.44\n",
      "Episode length: 1.60 +/- 0.92\n",
      "Eval num_timesteps=658000, episode_reward=-28.18 +/- 82.10\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=659000, episode_reward=-44.29 +/- 68.30\n",
      "Episode length: 1.60 +/- 0.92\n",
      "Eval num_timesteps=660000, episode_reward=-27.81 +/- 81.95\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=661000, episode_reward=-49.64 +/- 84.13\n",
      "Episode length: 2.10 +/- 0.99\n",
      "Eval num_timesteps=662000, episode_reward=-29.87 +/- 70.69\n",
      "Episode length: 1.50 +/- 0.87\n",
      "Eval num_timesteps=663000, episode_reward=-51.39 +/- 56.59\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=664000, episode_reward=-44.78 +/- 68.40\n",
      "Episode length: 1.60 +/- 0.92\n",
      "Eval num_timesteps=665000, episode_reward=-77.39 +/- 27.80\n",
      "Episode length: 1.50 +/- 0.87\n",
      "Eval num_timesteps=666000, episode_reward=-47.08 +/- 53.82\n",
      "Episode length: 1.30 +/- 0.71\n",
      "Eval num_timesteps=667000, episode_reward=-9.66 +/- 59.60\n",
      "Episode length: 2.00 +/- 1.00\n",
      "Eval num_timesteps=668000, episode_reward=-6.95 +/- 71.26\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=669000, episode_reward=12.51 +/- 69.52\n",
      "Episode length: 2.20 +/- 0.98\n",
      "Eval num_timesteps=670000, episode_reward=-35.94 +/- 52.83\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=671000, episode_reward=-23.92 +/- 61.22\n",
      "Episode length: 1.60 +/- 0.92\n",
      "Eval num_timesteps=672000, episode_reward=8.97 +/- 71.23\n",
      "Episode length: 2.10 +/- 0.99\n",
      "Eval num_timesteps=673000, episode_reward=-13.43 +/- 60.50\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=674000, episode_reward=-20.26 +/- 61.08\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=675000, episode_reward=28.62 +/- 71.27\n",
      "Episode length: 2.40 +/- 0.92\n",
      "Eval num_timesteps=676000, episode_reward=-28.87 +/- 54.03\n",
      "Episode length: 1.60 +/- 0.92\n",
      "Eval num_timesteps=677000, episode_reward=-20.38 +/- 61.16\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=678000, episode_reward=-16.48 +/- 60.62\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=679000, episode_reward=-21.31 +/- 53.96\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=680000, episode_reward=2.85 +/- 61.78\n",
      "Episode length: 2.20 +/- 0.98\n",
      "Eval num_timesteps=681000, episode_reward=-6.15 +/- 76.77\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=682000, episode_reward=-12.95 +/- 60.11\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=683000, episode_reward=-17.41 +/- 53.41\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=684000, episode_reward=-49.42 +/- 21.36\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=685000, episode_reward=-18.07 +/- 58.12\n",
      "Episode length: 1.65 +/- 0.85\n",
      "Eval num_timesteps=686000, episode_reward=-17.55 +/- 64.40\n",
      "Episode length: 1.50 +/- 0.74\n",
      "Eval num_timesteps=687000, episode_reward=-16.82 +/- 63.91\n",
      "Episode length: 1.50 +/- 0.74\n",
      "Eval num_timesteps=688000, episode_reward=-22.24 +/- 58.89\n",
      "Episode length: 1.55 +/- 0.80\n",
      "Eval num_timesteps=689000, episode_reward=-29.35 +/- 58.85\n",
      "Episode length: 1.35 +/- 0.65\n",
      "Eval num_timesteps=690000, episode_reward=-3.44 +/- 61.07\n",
      "Episode length: 1.90 +/- 0.89\n",
      "Eval num_timesteps=691000, episode_reward=31.05 +/- 73.92\n",
      "Episode length: 1.80 +/- 0.68\n",
      "Eval num_timesteps=692000, episode_reward=-0.45 +/- 70.74\n",
      "Episode length: 1.60 +/- 0.73\n",
      "Eval num_timesteps=693000, episode_reward=-18.80 +/- 58.66\n",
      "Episode length: 1.65 +/- 0.85\n",
      "Eval num_timesteps=694000, episode_reward=-39.61 +/- 32.64\n",
      "Episode length: 1.60 +/- 0.92\n",
      "Eval num_timesteps=695000, episode_reward=-35.79 +/- 41.31\n",
      "Episode length: 1.45 +/- 0.80\n",
      "Eval num_timesteps=696000, episode_reward=-3.08 +/- 60.73\n",
      "Episode length: 1.90 +/- 0.89\n",
      "Eval num_timesteps=697000, episode_reward=-28.62 +/- 58.48\n",
      "Episode length: 1.35 +/- 0.65\n",
      "Eval num_timesteps=698000, episode_reward=-33.65 +/- 58.84\n",
      "Episode length: 1.25 +/- 0.54\n",
      "Eval num_timesteps=699000, episode_reward=6.81 +/- 73.40\n",
      "Episode length: 1.65 +/- 0.73\n",
      "Eval num_timesteps=700000, episode_reward=-23.36 +/- 52.36\n",
      "Episode length: 1.70 +/- 0.90\n",
      "Eval num_timesteps=701000, episode_reward=-7.12 +/- 62.35\n",
      "Episode length: 1.80 +/- 0.87\n",
      "Eval num_timesteps=702000, episode_reward=-16.55 +/- 69.80\n",
      "Episode length: 1.35 +/- 0.57\n",
      "Eval num_timesteps=703000, episode_reward=-6.49 +/- 68.06\n",
      "Episode length: 1.65 +/- 0.79\n",
      "Eval num_timesteps=704000, episode_reward=3.13 +/- 74.98\n",
      "Episode length: 1.55 +/- 0.67\n",
      "Eval num_timesteps=705000, episode_reward=-6.00 +/- 67.66\n",
      "Episode length: 1.65 +/- 0.79\n",
      "Eval num_timesteps=706000, episode_reward=-25.70 +/- 51.59\n",
      "Episode length: 1.60 +/- 0.86\n",
      "Eval num_timesteps=707000, episode_reward=-13.90 +/- 72.44\n",
      "Episode length: 1.60 +/- 0.92\n",
      "Eval num_timesteps=708000, episode_reward=-28.62 +/- 53.90\n",
      "Episode length: 1.60 +/- 0.92\n",
      "Eval num_timesteps=709000, episode_reward=-13.02 +/- 77.88\n",
      "Episode length: 1.60 +/- 0.92\n",
      "Eval num_timesteps=710000, episode_reward=-13.79 +/- 83.42\n",
      "Episode length: 1.50 +/- 0.87\n",
      "Eval num_timesteps=711000, episode_reward=-1.95 +/- 83.76\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=712000, episode_reward=13.61 +/- 87.86\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=713000, episode_reward=14.88 +/- 81.93\n",
      "Episode length: 2.00 +/- 1.00\n",
      "Eval num_timesteps=714000, episode_reward=-17.00 +/- 70.43\n",
      "Episode length: 1.60 +/- 0.92\n",
      "Eval num_timesteps=715000, episode_reward=4.39 +/- 82.96\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=716000, episode_reward=22.55 +/- 96.20\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=717000, episode_reward=-1.42 +/- 78.32\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=718000, episode_reward=13.49 +/- 87.97\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=719000, episode_reward=-6.30 +/- 77.69\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=720000, episode_reward=24.31 +/- 90.23\n",
      "Episode length: 2.00 +/- 1.00\n",
      "Eval num_timesteps=721000, episode_reward=-13.91 +/- 83.49\n",
      "Episode length: 1.50 +/- 0.87\n",
      "Eval num_timesteps=722000, episode_reward=13.61 +/- 87.86\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=723000, episode_reward=10.13 +/- 82.20\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=724000, episode_reward=19.19 +/- 91.30\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=725000, episode_reward=22.13 +/- 73.48\n",
      "Episode length: 2.20 +/- 0.98\n",
      "Eval num_timesteps=726000, episode_reward=-6.83 +/- 83.14\n",
      "Episode length: 1.60 +/- 0.92\n",
      "Eval num_timesteps=727000, episode_reward=-6.30 +/- 77.70\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=728000, episode_reward=8.82 +/- 78.16\n",
      "Episode length: 2.25 +/- 1.41\n",
      "Eval num_timesteps=729000, episode_reward=1.88 +/- 86.36\n",
      "Episode length: 1.85 +/- 1.19\n",
      "Eval num_timesteps=730000, episode_reward=-32.64 +/- 66.53\n",
      "Episode length: 1.35 +/- 0.85\n",
      "Eval num_timesteps=731000, episode_reward=4.55 +/- 83.06\n",
      "Episode length: 2.05 +/- 1.32\n",
      "Eval num_timesteps=732000, episode_reward=21.81 +/- 86.18\n",
      "Episode length: 2.30 +/- 1.35\n",
      "Eval num_timesteps=733000, episode_reward=1.28 +/- 78.40\n",
      "Episode length: 2.10 +/- 1.37\n",
      "Eval num_timesteps=734000, episode_reward=13.97 +/- 73.58\n",
      "Episode length: 2.10 +/- 1.30\n",
      "Eval num_timesteps=735000, episode_reward=-15.73 +/- 69.90\n",
      "Episode length: 1.85 +/- 1.31\n",
      "Eval num_timesteps=736000, episode_reward=23.94 +/- 90.57\n",
      "Episode length: 2.00 +/- 1.00\n",
      "Eval num_timesteps=737000, episode_reward=-12.28 +/- 77.45\n",
      "Episode length: 1.60 +/- 0.92\n",
      "Eval num_timesteps=738000, episode_reward=25.21 +/- 84.67\n",
      "Episode length: 2.10 +/- 0.99\n",
      "Eval num_timesteps=739000, episode_reward=-3.46 +/- 89.50\n",
      "Episode length: 1.60 +/- 0.92\n",
      "Eval num_timesteps=740000, episode_reward=-10.85 +/- 64.96\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=741000, episode_reward=-1225.88 +/- 3009.01\n",
      "Episode length: 2.35 +/- 1.15\n",
      "Eval num_timesteps=742000, episode_reward=4.43 +/- 77.83\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=743000, episode_reward=-23.71 +/- 69.61\n",
      "Episode length: 1.50 +/- 0.87\n",
      "Eval num_timesteps=744000, episode_reward=9.35 +/- 87.67\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=745000, episode_reward=-21.63 +/- 68.50\n",
      "Episode length: 1.50 +/- 0.87\n",
      "Eval num_timesteps=746000, episode_reward=3.66 +/- 83.53\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=747000, episode_reward=-8.55 +/- 84.21\n",
      "Episode length: 1.60 +/- 0.92\n",
      "Eval num_timesteps=748000, episode_reward=11.86 +/- 93.79\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=749000, episode_reward=4.56 +/- 77.74\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=750000, episode_reward=6.37 +/- 94.39\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=751000, episode_reward=2.52 +/- 89.12\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=752000, episode_reward=42.29 +/- 76.82\n",
      "Episode length: 2.40 +/- 0.92\n",
      "New best mean reward!\n",
      "Eval num_timesteps=753000, episode_reward=30.04 +/- 88.24\n",
      "Episode length: 2.10 +/- 0.99\n",
      "Eval num_timesteps=754000, episode_reward=1.41 +/- 89.87\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=755000, episode_reward=-7.32 +/- 83.46\n",
      "Episode length: 1.60 +/- 0.92\n",
      "Eval num_timesteps=756000, episode_reward=19.35 +/- 86.53\n",
      "Episode length: 2.00 +/- 1.00\n",
      "Eval num_timesteps=757000, episode_reward=28.04 +/- 94.67\n",
      "Episode length: 2.00 +/- 1.00\n",
      "Eval num_timesteps=758000, episode_reward=19.92 +/- 90.66\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=759000, episode_reward=-1.55 +/- 78.40\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=760000, episode_reward=-81.03 +/- 152.75\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=761000, episode_reward=6.50 +/- 77.19\n",
      "Episode length: 2.10 +/- 0.99\n",
      "Eval num_timesteps=762000, episode_reward=14.60 +/- 87.04\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=763000, episode_reward=9.52 +/- 82.73\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=764000, episode_reward=39.35 +/- 94.63\n",
      "Episode length: 2.10 +/- 0.99\n",
      "Eval num_timesteps=765000, episode_reward=4.92 +/- 77.43\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=766000, episode_reward=13.25 +/- 88.16\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=767000, episode_reward=29.02 +/- 93.75\n",
      "Episode length: 2.00 +/- 1.00\n",
      "Eval num_timesteps=768000, episode_reward=-6.46 +/- 82.92\n",
      "Episode length: 1.60 +/- 0.92\n",
      "Eval num_timesteps=769000, episode_reward=-17.00 +/- 70.42\n",
      "Episode length: 1.60 +/- 0.92\n",
      "Eval num_timesteps=770000, episode_reward=22.92 +/- 95.88\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=771000, episode_reward=44.72 +/- 92.21\n",
      "Episode length: 2.20 +/- 0.98\n",
      "New best mean reward!\n",
      "Eval num_timesteps=772000, episode_reward=17.06 +/- 97.39\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=773000, episode_reward=35.17 +/- 86.50\n",
      "Episode length: 2.20 +/- 0.98\n",
      "Eval num_timesteps=774000, episode_reward=6.74 +/- 94.14\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=775000, episode_reward=-17.53 +/- 76.33\n",
      "Episode length: 1.50 +/- 0.87\n",
      "Eval num_timesteps=776000, episode_reward=13.37 +/- 88.07\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=777000, episode_reward=-6.42 +/- 77.78\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=778000, episode_reward=-22.61 +/- 69.03\n",
      "Episode length: 1.50 +/- 0.87\n",
      "Eval num_timesteps=779000, episode_reward=2.88 +/- 88.86\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=780000, episode_reward=7.64 +/- 88.99\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=781000, episode_reward=-19.24 +/- 77.24\n",
      "Episode length: 1.50 +/- 0.87\n",
      "Eval num_timesteps=782000, episode_reward=-43.26 +/- 40.78\n",
      "Episode length: 1.30 +/- 0.71\n",
      "Eval num_timesteps=783000, episode_reward=4.43 +/- 77.83\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=784000, episode_reward=18.94 +/- 91.51\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=785000, episode_reward=13.74 +/- 87.76\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=786000, episode_reward=13.74 +/- 87.76\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=787000, episode_reward=12.47 +/- 93.32\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=788000, episode_reward=9.51 +/- 82.72\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=789000, episode_reward=48.53 +/- 99.88\n",
      "Episode length: 2.10 +/- 0.99\n",
      "New best mean reward!\n",
      "Eval num_timesteps=790000, episode_reward=-27.32 +/- 60.25\n",
      "Episode length: 1.50 +/- 0.87\n",
      "Eval num_timesteps=791000, episode_reward=13.08 +/- 92.83\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=792000, episode_reward=-7.44 +/- 83.54\n",
      "Episode length: 1.60 +/- 0.92\n",
      "Eval num_timesteps=793000, episode_reward=20.37 +/- 80.65\n",
      "Episode length: 2.10 +/- 0.99\n",
      "Eval num_timesteps=794000, episode_reward=12.60 +/- 93.21\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=795000, episode_reward=33.11 +/- 69.04\n",
      "Episode length: 2.40 +/- 0.92\n",
      "Eval num_timesteps=796000, episode_reward=11.57 +/- 98.26\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=797000, episode_reward=-2.57 +/- 84.17\n",
      "Episode length: 1.70 +/- 0.95\n",
      "Eval num_timesteps=798000, episode_reward=8.49 +/- 88.34\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=799000, episode_reward=28.78 +/- 93.99\n",
      "Episode length: 2.00 +/- 1.00\n",
      "Eval num_timesteps=800000, episode_reward=4.19 +/- 78.03\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=801000, episode_reward=-1.06 +/- 78.04\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=802000, episode_reward=-30.86 +/- 59.57\n",
      "Episode length: 1.65 +/- 0.91\n",
      "Eval num_timesteps=803000, episode_reward=11.71 +/- 90.85\n",
      "Episode length: 1.95 +/- 1.20\n",
      "Eval num_timesteps=804000, episode_reward=-5.29 +/- 77.21\n",
      "Episode length: 1.95 +/- 1.32\n",
      "Eval num_timesteps=805000, episode_reward=25.19 +/- 97.01\n",
      "Episode length: 2.00 +/- 1.14\n",
      "Eval num_timesteps=806000, episode_reward=52.05 +/- 94.27\n",
      "Episode length: 2.40 +/- 1.20\n",
      "New best mean reward!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 33\u001B[0m\n\u001B[0;32m     22\u001B[0m model \u001B[38;5;241m=\u001B[39m A2C(\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMlpPolicy\u001B[39m\u001B[38;5;124m'\u001B[39m, \n\u001B[0;32m     24\u001B[0m     env, \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     29\u001B[0m     n_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m,             \u001B[38;5;66;03m# 每个更新周期的步数\u001B[39;00m\n\u001B[0;32m     30\u001B[0m )\n\u001B[0;32m     32\u001B[0m \u001B[38;5;66;03m# 训练模型，并将TensorBoard日志存储到log目录中\u001B[39;00m\n\u001B[1;32m---> 33\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43meval_callback\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;66;03m# 保存模型\u001B[39;00m\n\u001B[0;32m     36\u001B[0m model\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.model/a2c_electric_vehicle\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\stable_baselines3\\a2c\\a2c.py:201\u001B[0m, in \u001B[0;36mA2C.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    192\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlearn\u001B[39m(\n\u001B[0;32m    193\u001B[0m     \u001B[38;5;28mself\u001B[39m: SelfA2C,\n\u001B[0;32m    194\u001B[0m     total_timesteps: \u001B[38;5;28mint\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    199\u001B[0m     progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    200\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m SelfA2C:\n\u001B[1;32m--> 201\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    203\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    204\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_interval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    205\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_log_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    206\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    207\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    208\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:300\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    297\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    299\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps \u001B[38;5;241m<\u001B[39m total_timesteps:\n\u001B[1;32m--> 300\u001B[0m     continue_training \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect_rollouts\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrollout_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_rollout_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_steps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    302\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m continue_training:\n\u001B[0;32m    303\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:179\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.collect_rollouts\u001B[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001B[0m\n\u001B[0;32m    176\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m th\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m    177\u001B[0m     \u001B[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001B[39;00m\n\u001B[0;32m    178\u001B[0m     obs_tensor \u001B[38;5;241m=\u001B[39m obs_as_tensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_last_obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m--> 179\u001B[0m     actions, values, log_probs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs_tensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    180\u001B[0m actions \u001B[38;5;241m=\u001B[39m actions\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m    182\u001B[0m \u001B[38;5;66;03m# Rescale and perform action\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\stable_baselines3\\common\\policies.py:647\u001B[0m, in \u001B[0;36mActorCriticPolicy.forward\u001B[1;34m(self, obs, deterministic)\u001B[0m\n\u001B[0;32m    645\u001B[0m features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mextract_features(obs)\n\u001B[0;32m    646\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshare_features_extractor:\n\u001B[1;32m--> 647\u001B[0m     latent_pi, latent_vf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmlp_extractor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    648\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    649\u001B[0m     pi_features, vf_features \u001B[38;5;241m=\u001B[39m features\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\stable_baselines3\\common\\torch_layers.py:222\u001B[0m, in \u001B[0;36mMlpExtractor.forward\u001B[1;34m(self, features)\u001B[0m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, features: th\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[th\u001B[38;5;241m.\u001B[39mTensor, th\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m    218\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    219\u001B[0m \u001B[38;5;124;03m    :return: latent_policy, latent_value of the specified network.\u001B[39;00m\n\u001B[0;32m    220\u001B[0m \u001B[38;5;124;03m        If all layers are shared, then ``latent_policy == latent_value``\u001B[39;00m\n\u001B[0;32m    221\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 222\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward_actor(features), \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward_critic\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\stable_baselines3\\common\\torch_layers.py:228\u001B[0m, in \u001B[0;36mMlpExtractor.forward_critic\u001B[1;34m(self, features)\u001B[0m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward_critic\u001B[39m(\u001B[38;5;28mself\u001B[39m, features: th\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m th\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m--> 228\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalue_net\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    213\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    214\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 215\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import DQN,A2C\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "eval_freq = 1000  # 评估频率\n",
    "eval_callback = EvalCallback(\n",
    "    env,\n",
    "    callback_on_new_best=None,\n",
    "    n_eval_episodes=20,\n",
    "    best_model_save_path=\"./best_model/A2C\",\n",
    "    log_path=\"./logs/A2C\",\n",
    "    eval_freq=eval_freq,\n",
    ")\n",
    "log_name = \"A2C\"\n",
    "\n",
    "\n",
    "\n",
    "model = A2C(\n",
    "    'MlpPolicy', \n",
    "    env, \n",
    "    verbose=0, \n",
    "    tensorboard_log=\"./logs/A2C\",\n",
    "    learning_rate=0.0007,  # 默认学习率为0.0007，可调整\n",
    "    ent_coef=0.01,         # 熵系数，控制探索的随机性，默认是0.0，可增大以增加探索\n",
    "    n_steps=5,             # 每个更新周期的步数\n",
    ")\n",
    "\n",
    "# 训练模型，并将TensorBoard日志存储到log目录中\n",
    "model.learn(total_timesteps=1000000,tb_log_name=log_name,callback=[eval_callback])\n",
    "\n",
    "# 保存模型\n",
    "model.save(\"model/a2c_electric_vehicle\")\n",
    "\n",
    "# 加载模型（如果需要）\n",
    "# model = DQN.load(\"dqn_electric_vehicle\")\n",
    "\n",
    "# 评估模型\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Mean reward: {mean_reward}, Std reward: {std_reward}\")\n",
    "\n",
    "# 测试训练好的模型\n",
    "obs = env.reset()\n",
    "for i in range(100):\n",
    "    action = model.predict(obs, deterministic=True)[0]\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        obs = env.reset()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-01T09:11:11.866986400Z",
     "start_time": "2024-09-01T08:40:48.532028800Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "94237093148bad7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
