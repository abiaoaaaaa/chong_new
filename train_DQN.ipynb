{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3 import PPO,DDPG,DQN\n",
    "from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import env\n",
    "env = env.ElectricVehicleEnv()\n",
    "#vec_env = make_vec_env(env, n_envs=4)\n",
    "check_env(env)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# 设置评估频率\n",
    "eval_freq = 1000  # 每训练500步评估一次\n",
    "\n",
    "# 配置EvalCallback\n",
    "eval_callback = EvalCallback(\n",
    "    env,\n",
    "    callback_on_new_best=None,\n",
    "    n_eval_episodes=20,\n",
    "    best_model_save_path=\"./best_model/DQN\",  # 更改保存路径为DQN\n",
    "    log_path=\"./logs/DQN\",  # 更改日志路径为DQN\n",
    "    eval_freq=eval_freq,\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-12T03:50:59.094242800Z",
     "start_time": "2024-09-12T03:50:58.765199700Z"
    }
   },
   "id": "376f27caa974f1e8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=-8412.84 +/- 145.73\n",
      "Episode length: 3.65 +/- 1.06\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-6936.26 +/- 2888.20\n",
      "Episode length: 2.25 +/- 0.70\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-7368.10 +/- 2402.49\n",
      "Episode length: 2.25 +/- 0.54\n",
      "Eval num_timesteps=4000, episode_reward=-6525.87 +/- 3248.09\n",
      "Episode length: 2.00 +/- 0.00\n",
      "New best mean reward!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# DQN的超参数设置\n",
    "model = DQN(\n",
    "    'MlpPolicy', \n",
    "    env, \n",
    "    verbose=0, \n",
    "    tensorboard_log=\"./logs/DQN\",\n",
    "    learning_rate=0.0005,  # 默认学习率，通常比A2C更小\n",
    "    buffer_size=100000,  # 经验回放缓冲区大小\n",
    "    learning_starts=1000,  # 开始学习前的步数\n",
    "    batch_size=32,  # 批量大小\n",
    "    tau=1.0,  # 目标网络软更新的系数\n",
    "    gamma=0.99,  # 折扣因子\n",
    "    train_freq=4,  # 训练的频率（每4步训练一次）\n",
    "    target_update_interval=1000,  # 目标网络更新的频率\n",
    "    exploration_fraction=0.1,  # 探索率衰减部分\n",
    "    exploration_final_eps=0.02,  # 最小探索率\n",
    "    max_grad_norm=10,  # 梯度裁剪的最大范数\n",
    ")\n",
    "\n",
    "\n",
    "# 训练模型，并将TensorBoard日志存储到log目录中\n",
    "model.learn(total_timesteps=1000000, tb_log_name=\"DQN\", callback=eval_callback)\n",
    "\n",
    "# 保存模型\n",
    "model.save(\"model/dqn_electric_vehicle\")\n",
    "\n",
    "# 加载模型（如果需要）\n",
    "# model = DQN.load(\"dqn_electric_vehicle\")\n",
    "\n",
    "# 评估模型\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward}, Std reward: {std_reward}\")\n",
    "\n",
    "# 测试训练好的模型\n",
    "obs = env.reset()\n",
    "for i in range(100):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        obs = env.reset()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-09-11T08:55:15.407822200Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=-5440.88 +/- 4307.78\n",
      "Episode length: 7.10 +/- 4.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-4466.34 +/- 4415.86\n",
      "Episode length: 6.00 +/- 4.39\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-7225.00 +/- 3501.53\n",
      "Episode length: 9.15 +/- 3.15\n",
      "Eval num_timesteps=4000, episode_reward=-7588.09 +/- 3168.74\n",
      "Episode length: 8.95 +/- 3.77\n",
      "Eval num_timesteps=5000, episode_reward=-6178.12 +/- 3974.14\n",
      "Episode length: 6.80 +/- 4.23\n",
      "Eval num_timesteps=6000, episode_reward=-6281.41 +/- 4033.77\n",
      "Episode length: 7.30 +/- 4.23\n",
      "Eval num_timesteps=7000, episode_reward=-9038.01 +/- 101.21\n",
      "Episode length: 10.90 +/- 0.44\n",
      "Eval num_timesteps=8000, episode_reward=-8471.67 +/- 1945.74\n",
      "Episode length: 9.30 +/- 2.97\n",
      "Eval num_timesteps=9000, episode_reward=-5985.93 +/- 3893.44\n",
      "Episode length: 6.00 +/- 3.95\n",
      "Eval num_timesteps=10000, episode_reward=-5827.77 +/- 4192.74\n",
      "Episode length: 7.00 +/- 4.46\n",
      "Eval num_timesteps=11000, episode_reward=-5731.44 +/- 4114.97\n",
      "Episode length: 5.65 +/- 3.47\n",
      "Eval num_timesteps=12000, episode_reward=-7413.16 +/- 3086.22\n",
      "Episode length: 7.10 +/- 4.23\n",
      "Eval num_timesteps=13000, episode_reward=-6690.86 +/- 3705.20\n",
      "Episode length: 7.30 +/- 3.91\n",
      "Eval num_timesteps=14000, episode_reward=-7225.68 +/- 3564.69\n",
      "Episode length: 8.85 +/- 3.80\n",
      "Eval num_timesteps=15000, episode_reward=-6686.75 +/- 3814.32\n",
      "Episode length: 7.65 +/- 4.07\n",
      "Eval num_timesteps=16000, episode_reward=-5684.39 +/- 4093.61\n",
      "Episode length: 5.55 +/- 4.19\n",
      "Eval num_timesteps=17000, episode_reward=-6217.24 +/- 4001.47\n",
      "Episode length: 7.30 +/- 4.08\n",
      "Eval num_timesteps=18000, episode_reward=-6593.56 +/- 3787.65\n",
      "Episode length: 6.75 +/- 4.31\n",
      "Eval num_timesteps=19000, episode_reward=-5074.16 +/- 4336.89\n",
      "Episode length: 7.50 +/- 3.93\n",
      "Eval num_timesteps=20000, episode_reward=-8285.47 +/- 1934.83\n",
      "Episode length: 8.10 +/- 3.79\n",
      "Eval num_timesteps=21000, episode_reward=-7548.10 +/- 3159.97\n",
      "Episode length: 8.15 +/- 4.13\n",
      "Eval num_timesteps=22000, episode_reward=-5507.73 +/- 4366.29\n",
      "Episode length: 7.65 +/- 4.11\n",
      "Eval num_timesteps=23000, episode_reward=-8121.95 +/- 2680.63\n",
      "Episode length: 9.55 +/- 3.46\n",
      "Eval num_timesteps=24000, episode_reward=-5461.18 +/- 4402.82\n",
      "Episode length: 7.25 +/- 4.60\n",
      "Eval num_timesteps=25000, episode_reward=-7661.24 +/- 3198.13\n",
      "Episode length: 9.60 +/- 3.34\n",
      "Eval num_timesteps=26000, episode_reward=-6661.77 +/- 3825.72\n",
      "Episode length: 7.35 +/- 4.49\n",
      "Eval num_timesteps=27000, episode_reward=-5843.41 +/- 4232.83\n",
      "Episode length: 7.90 +/- 4.13\n",
      "Eval num_timesteps=28000, episode_reward=-6089.74 +/- 3978.40\n",
      "Episode length: 5.50 +/- 4.27\n",
      "Eval num_timesteps=29000, episode_reward=-7986.28 +/- 2528.68\n",
      "Episode length: 9.10 +/- 3.19\n",
      "Eval num_timesteps=30000, episode_reward=-5385.67 +/- 4311.95\n",
      "Episode length: 6.45 +/- 4.67\n",
      "Eval num_timesteps=31000, episode_reward=-6349.17 +/- 4036.15\n",
      "Episode length: 8.40 +/- 4.07\n",
      "Eval num_timesteps=32000, episode_reward=-6188.69 +/- 4063.82\n",
      "Episode length: 6.65 +/- 4.48\n",
      "Eval num_timesteps=33000, episode_reward=-5330.22 +/- 4267.82\n",
      "Episode length: 6.65 +/- 4.05\n",
      "Eval num_timesteps=34000, episode_reward=-5801.03 +/- 4164.72\n",
      "Episode length: 7.05 +/- 4.09\n",
      "Eval num_timesteps=35000, episode_reward=-6330.43 +/- 4011.41\n",
      "Episode length: 8.20 +/- 3.60\n",
      "Eval num_timesteps=36000, episode_reward=-6156.41 +/- 3994.89\n",
      "Episode length: 6.25 +/- 4.39\n",
      "Eval num_timesteps=37000, episode_reward=-4838.47 +/- 4342.71\n",
      "Episode length: 5.75 +/- 4.52\n",
      "Eval num_timesteps=38000, episode_reward=-7495.18 +/- 3100.75\n",
      "Episode length: 8.05 +/- 4.09\n",
      "Eval num_timesteps=39000, episode_reward=-5930.22 +/- 4174.87\n",
      "Episode length: 8.45 +/- 3.56\n",
      "Eval num_timesteps=40000, episode_reward=-7569.85 +/- 3096.89\n",
      "Episode length: 9.40 +/- 3.28\n",
      "Eval num_timesteps=41000, episode_reward=-4037.65 +/- 4314.93\n",
      "Episode length: 5.60 +/- 4.31\n",
      "New best mean reward!\n",
      "Eval num_timesteps=42000, episode_reward=-5916.09 +/- 4337.14\n",
      "Episode length: 7.60 +/- 4.64\n",
      "Eval num_timesteps=43000, episode_reward=-6743.65 +/- 3851.36\n",
      "Episode length: 8.15 +/- 4.13\n",
      "Eval num_timesteps=44000, episode_reward=-6616.21 +/- 3784.68\n",
      "Episode length: 8.30 +/- 3.99\n",
      "Eval num_timesteps=45000, episode_reward=-7590.80 +/- 3169.49\n",
      "Episode length: 8.50 +/- 3.88\n",
      "Eval num_timesteps=46000, episode_reward=-6369.20 +/- 4069.71\n",
      "Episode length: 8.55 +/- 3.80\n",
      "Eval num_timesteps=47000, episode_reward=-6836.53 +/- 3416.00\n",
      "Episode length: 4.55 +/- 4.26\n",
      "Eval num_timesteps=48000, episode_reward=-5363.22 +/- 4288.03\n",
      "Episode length: 6.40 +/- 4.65\n",
      "Eval num_timesteps=49000, episode_reward=-6732.28 +/- 3788.69\n",
      "Episode length: 8.50 +/- 3.75\n",
      "Eval num_timesteps=50000, episode_reward=-6613.36 +/- 3723.45\n",
      "Episode length: 7.50 +/- 4.08\n",
      "Eval num_timesteps=51000, episode_reward=-6815.05 +/- 3916.80\n",
      "Episode length: 8.70 +/- 4.03\n",
      "Eval num_timesteps=52000, episode_reward=-7252.37 +/- 3601.00\n",
      "Episode length: 9.10 +/- 3.82\n",
      "Eval num_timesteps=53000, episode_reward=-7404.42 +/- 3129.42\n",
      "Episode length: 7.40 +/- 4.09\n",
      "Eval num_timesteps=54000, episode_reward=-7211.16 +/- 3573.04\n",
      "Episode length: 9.05 +/- 3.80\n",
      "Eval num_timesteps=55000, episode_reward=-6316.67 +/- 4107.59\n",
      "Episode length: 8.10 +/- 4.44\n",
      "Eval num_timesteps=56000, episode_reward=-5344.37 +/- 4324.30\n",
      "Episode length: 6.40 +/- 4.68\n",
      "Eval num_timesteps=57000, episode_reward=-4486.55 +/- 4393.44\n",
      "Episode length: 6.35 +/- 4.15\n",
      "Eval num_timesteps=58000, episode_reward=-4948.73 +/- 4378.57\n",
      "Episode length: 6.05 +/- 4.34\n",
      "Eval num_timesteps=59000, episode_reward=-4040.77 +/- 4499.94\n",
      "Episode length: 5.30 +/- 4.64\n",
      "Eval num_timesteps=60000, episode_reward=-6699.65 +/- 3786.94\n",
      "Episode length: 7.85 +/- 4.13\n",
      "Eval num_timesteps=61000, episode_reward=-4924.51 +/- 4408.15\n",
      "Episode length: 6.60 +/- 4.87\n",
      "Eval num_timesteps=62000, episode_reward=-4900.57 +/- 4392.58\n",
      "Episode length: 5.70 +/- 4.55\n",
      "Eval num_timesteps=63000, episode_reward=-8402.15 +/- 1952.72\n",
      "Episode length: 8.50 +/- 3.88\n",
      "Eval num_timesteps=64000, episode_reward=-7180.08 +/- 3562.55\n",
      "Episode length: 8.50 +/- 4.03\n",
      "Eval num_timesteps=65000, episode_reward=-7217.87 +/- 3601.52\n",
      "Episode length: 9.00 +/- 3.70\n",
      "Eval num_timesteps=66000, episode_reward=-6365.53 +/- 4084.56\n",
      "Episode length: 8.25 +/- 4.32\n",
      "Eval num_timesteps=67000, episode_reward=-6190.29 +/- 3977.86\n",
      "Episode length: 6.30 +/- 4.37\n",
      "Eval num_timesteps=68000, episode_reward=-5426.96 +/- 4292.53\n",
      "Episode length: 7.10 +/- 4.37\n",
      "Eval num_timesteps=69000, episode_reward=-5392.96 +/- 4315.87\n",
      "Episode length: 7.00 +/- 4.22\n",
      "Eval num_timesteps=70000, episode_reward=-4085.14 +/- 4360.77\n",
      "Episode length: 5.35 +/- 4.50\n",
      "Eval num_timesteps=71000, episode_reward=-6668.68 +/- 3789.48\n",
      "Episode length: 7.60 +/- 4.34\n",
      "Eval num_timesteps=72000, episode_reward=-5356.55 +/- 4331.00\n",
      "Episode length: 6.60 +/- 4.87\n",
      "Eval num_timesteps=73000, episode_reward=-5262.65 +/- 4224.03\n",
      "Episode length: 5.65 +/- 4.26\n",
      "Eval num_timesteps=74000, episode_reward=-5307.94 +/- 4255.79\n",
      "Episode length: 6.45 +/- 4.33\n",
      "Eval num_timesteps=75000, episode_reward=-5139.63 +/- 4120.89\n",
      "Episode length: 4.30 +/- 3.58\n",
      "Eval num_timesteps=76000, episode_reward=-3513.55 +/- 4305.63\n",
      "Episode length: 4.30 +/- 4.01\n",
      "New best mean reward!\n",
      "Eval num_timesteps=77000, episode_reward=-6682.19 +/- 3799.29\n",
      "Episode length: 7.60 +/- 4.31\n",
      "Eval num_timesteps=78000, episode_reward=-5421.28 +/- 4309.51\n",
      "Episode length: 7.40 +/- 4.09\n",
      "Eval num_timesteps=79000, episode_reward=-7148.93 +/- 3541.41\n",
      "Episode length: 8.15 +/- 4.08\n",
      "Eval num_timesteps=80000, episode_reward=-5010.39 +/- 4404.43\n",
      "Episode length: 7.05 +/- 4.67\n",
      "Eval num_timesteps=81000, episode_reward=-5346.19 +/- 4343.12\n",
      "Episode length: 7.10 +/- 4.78\n",
      "Eval num_timesteps=82000, episode_reward=-6706.11 +/- 3864.82\n",
      "Episode length: 7.75 +/- 4.45\n",
      "Eval num_timesteps=83000, episode_reward=-8119.12 +/- 2699.92\n",
      "Episode length: 10.10 +/- 2.70\n",
      "Eval num_timesteps=84000, episode_reward=-4130.74 +/- 4495.67\n",
      "Episode length: 6.00 +/- 4.84\n",
      "Eval num_timesteps=85000, episode_reward=-6290.88 +/- 4092.29\n",
      "Episode length: 7.80 +/- 4.14\n",
      "Eval num_timesteps=86000, episode_reward=-4066.26 +/- 4472.69\n",
      "Episode length: 5.30 +/- 4.68\n",
      "Eval num_timesteps=87000, episode_reward=-4073.76 +/- 4448.41\n",
      "Episode length: 5.65 +/- 4.38\n",
      "Eval num_timesteps=88000, episode_reward=-8064.27 +/- 2682.83\n",
      "Episode length: 9.35 +/- 3.40\n",
      "Eval num_timesteps=89000, episode_reward=-6290.59 +/- 4004.14\n",
      "Episode length: 7.30 +/- 4.20\n",
      "Eval num_timesteps=90000, episode_reward=-5923.43 +/- 4212.61\n",
      "Episode length: 8.00 +/- 4.02\n",
      "Eval num_timesteps=91000, episode_reward=-6179.02 +/- 4015.69\n",
      "Episode length: 6.25 +/- 4.35\n",
      "Eval num_timesteps=92000, episode_reward=-5977.76 +/- 4258.26\n",
      "Episode length: 8.25 +/- 4.25\n",
      "Eval num_timesteps=93000, episode_reward=-5783.06 +/- 3784.92\n",
      "Episode length: 3.10 +/- 1.73\n",
      "Eval num_timesteps=94000, episode_reward=-5708.59 +/- 4205.92\n",
      "Episode length: 6.45 +/- 4.48\n",
      "Eval num_timesteps=95000, episode_reward=-5303.69 +/- 4292.90\n",
      "Episode length: 5.75 +/- 4.38\n",
      "Eval num_timesteps=96000, episode_reward=-5853.68 +/- 4216.32\n",
      "Episode length: 7.60 +/- 4.29\n",
      "Eval num_timesteps=97000, episode_reward=-5404.68 +/- 4353.57\n",
      "Episode length: 6.65 +/- 4.68\n",
      "Eval num_timesteps=98000, episode_reward=-6606.48 +/- 3818.41\n",
      "Episode length: 7.30 +/- 4.54\n",
      "Eval num_timesteps=99000, episode_reward=-5866.74 +/- 4294.31\n",
      "Episode length: 7.20 +/- 4.66\n",
      "Eval num_timesteps=100000, episode_reward=-4443.37 +/- 4337.63\n",
      "Episode length: 5.00 +/- 4.21\n",
      "Eval num_timesteps=101000, episode_reward=-6305.64 +/- 4099.13\n",
      "Episode length: 7.85 +/- 4.10\n",
      "Eval num_timesteps=102000, episode_reward=-3037.56 +/- 4018.98\n",
      "Episode length: 3.45 +/- 3.60\n",
      "New best mean reward!\n",
      "Eval num_timesteps=103000, episode_reward=-4491.00 +/- 4495.62\n",
      "Episode length: 6.30 +/- 4.56\n",
      "Eval num_timesteps=104000, episode_reward=-7421.85 +/- 3079.38\n",
      "Episode length: 6.80 +/- 4.02\n",
      "Eval num_timesteps=105000, episode_reward=-5421.49 +/- 4433.00\n",
      "Episode length: 7.00 +/- 4.72\n",
      "Eval num_timesteps=106000, episode_reward=-5440.53 +/- 4460.35\n",
      "Episode length: 7.05 +/- 4.75\n",
      "Eval num_timesteps=107000, episode_reward=-6258.14 +/- 4068.35\n",
      "Episode length: 6.80 +/- 4.62\n",
      "Eval num_timesteps=108000, episode_reward=-5444.20 +/- 4387.55\n",
      "Episode length: 6.85 +/- 4.63\n",
      "Eval num_timesteps=109000, episode_reward=-5669.48 +/- 4170.96\n",
      "Episode length: 6.10 +/- 4.30\n",
      "Eval num_timesteps=110000, episode_reward=-3079.11 +/- 4177.46\n",
      "Episode length: 3.75 +/- 3.86\n",
      "Eval num_timesteps=111000, episode_reward=-4058.22 +/- 4454.49\n",
      "Episode length: 5.45 +/- 4.66\n",
      "Eval num_timesteps=112000, episode_reward=-4909.68 +/- 4331.74\n",
      "Episode length: 6.00 +/- 4.70\n",
      "Eval num_timesteps=113000, episode_reward=-4499.82 +/- 4455.96\n",
      "Episode length: 5.85 +/- 4.52\n",
      "Eval num_timesteps=114000, episode_reward=-6169.39 +/- 4017.95\n",
      "Episode length: 5.80 +/- 4.72\n",
      "Eval num_timesteps=115000, episode_reward=-4029.24 +/- 4415.24\n",
      "Episode length: 4.85 +/- 4.56\n",
      "Eval num_timesteps=116000, episode_reward=-6786.08 +/- 3868.97\n",
      "Episode length: 8.55 +/- 4.25\n",
      "Eval num_timesteps=117000, episode_reward=-4059.75 +/- 4430.54\n",
      "Episode length: 5.45 +/- 4.58\n",
      "Eval num_timesteps=118000, episode_reward=-6786.38 +/- 3873.65\n",
      "Episode length: 8.60 +/- 4.16\n",
      "Eval num_timesteps=119000, episode_reward=-5327.49 +/- 4284.53\n",
      "Episode length: 6.75 +/- 4.65\n",
      "Eval num_timesteps=120000, episode_reward=-4977.46 +/- 4456.10\n",
      "Episode length: 6.80 +/- 4.69\n",
      "Eval num_timesteps=121000, episode_reward=-5784.79 +/- 4162.08\n",
      "Episode length: 6.30 +/- 4.21\n",
      "Eval num_timesteps=122000, episode_reward=-5451.56 +/- 4391.09\n",
      "Episode length: 7.05 +/- 4.67\n",
      "Eval num_timesteps=123000, episode_reward=-5429.91 +/- 4391.39\n",
      "Episode length: 6.45 +/- 4.68\n",
      "Eval num_timesteps=124000, episode_reward=-4953.47 +/- 4418.19\n",
      "Episode length: 6.25 +/- 4.76\n",
      "Eval num_timesteps=125000, episode_reward=-6751.16 +/- 3909.39\n",
      "Episode length: 8.20 +/- 4.30\n",
      "Eval num_timesteps=126000, episode_reward=-4100.97 +/- 4479.51\n",
      "Episode length: 5.45 +/- 4.72\n",
      "Eval num_timesteps=127000, episode_reward=-6350.11 +/- 4107.15\n",
      "Episode length: 7.90 +/- 4.46\n",
      "Eval num_timesteps=128000, episode_reward=-5281.20 +/- 4240.46\n",
      "Episode length: 5.45 +/- 4.26\n",
      "Eval num_timesteps=129000, episode_reward=-4917.41 +/- 4403.29\n",
      "Episode length: 6.55 +/- 4.46\n",
      "Eval num_timesteps=130000, episode_reward=-3925.23 +/- 4387.63\n",
      "Episode length: 5.40 +/- 4.35\n",
      "Eval num_timesteps=131000, episode_reward=-4426.69 +/- 4343.93\n",
      "Episode length: 4.55 +/- 4.31\n",
      "Eval num_timesteps=132000, episode_reward=-4415.88 +/- 4308.95\n",
      "Episode length: 4.70 +/- 3.85\n",
      "Eval num_timesteps=133000, episode_reward=-3229.40 +/- 4244.24\n",
      "Episode length: 4.95 +/- 4.24\n",
      "Eval num_timesteps=134000, episode_reward=-6208.77 +/- 4068.54\n",
      "Episode length: 6.45 +/- 4.44\n",
      "Eval num_timesteps=135000, episode_reward=-6173.82 +/- 4006.98\n",
      "Episode length: 5.85 +/- 4.68\n",
      "Eval num_timesteps=136000, episode_reward=-4792.37 +/- 4304.48\n",
      "Episode length: 4.35 +/- 4.00\n",
      "Eval num_timesteps=137000, episode_reward=-5483.54 +/- 4398.08\n",
      "Episode length: 7.15 +/- 4.74\n",
      "Eval num_timesteps=138000, episode_reward=-5294.00 +/- 4266.58\n",
      "Episode length: 5.30 +/- 4.26\n",
      "Eval num_timesteps=139000, episode_reward=-5705.97 +/- 4075.03\n",
      "Episode length: 5.75 +/- 3.48\n",
      "Eval num_timesteps=140000, episode_reward=-5605.11 +/- 4054.87\n",
      "Episode length: 4.15 +/- 4.00\n",
      "Eval num_timesteps=141000, episode_reward=-5284.19 +/- 4260.51\n",
      "Episode length: 5.25 +/- 4.39\n",
      "Eval num_timesteps=142000, episode_reward=-5413.86 +/- 4351.07\n",
      "Episode length: 6.80 +/- 4.49\n",
      "Eval num_timesteps=143000, episode_reward=-5885.65 +/- 4279.48\n",
      "Episode length: 6.95 +/- 4.55\n",
      "Eval num_timesteps=144000, episode_reward=-4975.02 +/- 4433.87\n",
      "Episode length: 6.40 +/- 4.68\n",
      "Eval num_timesteps=145000, episode_reward=-4996.26 +/- 4346.03\n",
      "Episode length: 6.40 +/- 4.54\n",
      "Eval num_timesteps=146000, episode_reward=-3594.24 +/- 4315.96\n",
      "Episode length: 4.60 +/- 4.37\n",
      "Eval num_timesteps=147000, episode_reward=-3268.07 +/- 4282.11\n",
      "Episode length: 5.00 +/- 4.56\n",
      "Eval num_timesteps=148000, episode_reward=-3184.95 +/- 4239.04\n",
      "Episode length: 4.40 +/- 4.35\n",
      "Eval num_timesteps=149000, episode_reward=-5280.96 +/- 4277.37\n",
      "Episode length: 5.20 +/- 4.46\n",
      "Eval num_timesteps=150000, episode_reward=-3532.06 +/- 4305.10\n",
      "Episode length: 3.95 +/- 4.06\n",
      "Eval num_timesteps=151000, episode_reward=-4361.85 +/- 4322.67\n",
      "Episode length: 3.90 +/- 4.19\n",
      "Eval num_timesteps=152000, episode_reward=-5642.49 +/- 4100.65\n",
      "Episode length: 4.40 +/- 3.76\n",
      "Eval num_timesteps=153000, episode_reward=-4034.40 +/- 4389.02\n",
      "Episode length: 5.40 +/- 4.63\n",
      "Eval num_timesteps=154000, episode_reward=-5828.80 +/- 4198.16\n",
      "Episode length: 6.95 +/- 4.46\n",
      "Eval num_timesteps=155000, episode_reward=-4103.58 +/- 4458.57\n",
      "Episode length: 5.60 +/- 4.73\n",
      "Eval num_timesteps=156000, episode_reward=-3498.97 +/- 4186.02\n",
      "Episode length: 3.60 +/- 3.61\n",
      "Eval num_timesteps=157000, episode_reward=-4107.30 +/- 4498.15\n",
      "Episode length: 5.55 +/- 4.76\n",
      "Eval num_timesteps=158000, episode_reward=-5008.84 +/- 4409.49\n",
      "Episode length: 6.45 +/- 4.59\n",
      "Eval num_timesteps=159000, episode_reward=-6662.85 +/- 3789.43\n",
      "Episode length: 6.95 +/- 4.28\n",
      "Eval num_timesteps=160000, episode_reward=-1829.40 +/- 3513.22\n",
      "Episode length: 2.85 +/- 3.45\n",
      "New best mean reward!\n",
      "Eval num_timesteps=161000, episode_reward=-6132.78 +/- 3976.59\n",
      "Episode length: 5.40 +/- 4.25\n",
      "Eval num_timesteps=162000, episode_reward=-4164.18 +/- 4484.06\n",
      "Episode length: 5.75 +/- 4.79\n",
      "Eval num_timesteps=163000, episode_reward=-5763.93 +/- 4173.40\n",
      "Episode length: 6.05 +/- 4.30\n",
      "Eval num_timesteps=164000, episode_reward=-5433.63 +/- 4419.63\n",
      "Episode length: 7.05 +/- 4.84\n",
      "Eval num_timesteps=165000, episode_reward=-4796.95 +/- 4364.90\n",
      "Episode length: 5.00 +/- 4.15\n",
      "Eval num_timesteps=166000, episode_reward=-4087.99 +/- 4484.32\n",
      "Episode length: 5.25 +/- 4.77\n",
      "Eval num_timesteps=167000, episode_reward=-4965.88 +/- 4463.99\n",
      "Episode length: 5.90 +/- 4.59\n",
      "Eval num_timesteps=168000, episode_reward=-3091.04 +/- 4133.13\n",
      "Episode length: 3.65 +/- 3.84\n",
      "Eval num_timesteps=169000, episode_reward=-5841.97 +/- 4202.19\n",
      "Episode length: 7.15 +/- 4.43\n",
      "Eval num_timesteps=170000, episode_reward=-3468.04 +/- 4216.56\n",
      "Episode length: 3.25 +/- 3.48\n",
      "Eval num_timesteps=171000, episode_reward=-4400.39 +/- 4401.52\n",
      "Episode length: 4.70 +/- 4.06\n",
      "Eval num_timesteps=172000, episode_reward=-5886.98 +/- 4287.78\n",
      "Episode length: 7.35 +/- 4.70\n",
      "Eval num_timesteps=173000, episode_reward=-4400.27 +/- 4426.07\n",
      "Episode length: 5.00 +/- 4.59\n",
      "Eval num_timesteps=174000, episode_reward=-5382.35 +/- 4364.63\n",
      "Episode length: 6.40 +/- 4.65\n",
      "Eval num_timesteps=175000, episode_reward=-5790.69 +/- 4232.97\n",
      "Episode length: 6.75 +/- 4.43\n",
      "Eval num_timesteps=176000, episode_reward=-6304.70 +/- 4130.20\n",
      "Episode length: 7.45 +/- 4.48\n",
      "Eval num_timesteps=177000, episode_reward=-5895.84 +/- 4298.94\n",
      "Episode length: 7.55 +/- 4.71\n",
      "Eval num_timesteps=178000, episode_reward=-4438.03 +/- 4415.79\n",
      "Episode length: 5.15 +/- 4.44\n",
      "Eval num_timesteps=179000, episode_reward=-4310.76 +/- 4289.78\n",
      "Episode length: 3.75 +/- 3.88\n",
      "Eval num_timesteps=180000, episode_reward=-5779.78 +/- 4252.47\n",
      "Episode length: 6.45 +/- 4.55\n",
      "Eval num_timesteps=181000, episode_reward=-4057.70 +/- 4425.13\n",
      "Episode length: 5.45 +/- 4.58\n",
      "Eval num_timesteps=182000, episode_reward=-4467.03 +/- 4461.63\n",
      "Episode length: 5.30 +/- 4.56\n",
      "Eval num_timesteps=183000, episode_reward=-3169.29 +/- 4183.95\n",
      "Episode length: 4.55 +/- 4.10\n",
      "Eval num_timesteps=184000, episode_reward=-4036.75 +/- 4479.01\n",
      "Episode length: 4.95 +/- 4.66\n",
      "Eval num_timesteps=185000, episode_reward=-3947.58 +/- 4376.98\n",
      "Episode length: 4.30 +/- 4.23\n",
      "Eval num_timesteps=186000, episode_reward=-4772.63 +/- 4310.71\n",
      "Episode length: 4.35 +/- 4.02\n",
      "Eval num_timesteps=187000, episode_reward=-4931.20 +/- 4438.74\n",
      "Episode length: 5.80 +/- 4.65\n",
      "Eval num_timesteps=188000, episode_reward=-4414.58 +/- 4357.50\n",
      "Episode length: 4.75 +/- 4.21\n",
      "Eval num_timesteps=189000, episode_reward=-4768.25 +/- 4164.08\n",
      "Episode length: 4.45 +/- 3.67\n",
      "Eval num_timesteps=190000, episode_reward=-4097.06 +/- 4469.86\n",
      "Episode length: 5.40 +/- 4.81\n",
      "Eval num_timesteps=191000, episode_reward=-4065.97 +/- 4445.31\n",
      "Episode length: 5.20 +/- 4.81\n",
      "Eval num_timesteps=192000, episode_reward=-2591.22 +/- 3995.49\n",
      "Episode length: 3.35 +/- 3.66\n",
      "Eval num_timesteps=193000, episode_reward=-4057.33 +/- 4461.90\n",
      "Episode length: 5.00 +/- 4.62\n",
      "Eval num_timesteps=194000, episode_reward=-2679.12 +/- 4039.33\n",
      "Episode length: 3.40 +/- 3.93\n",
      "Eval num_timesteps=195000, episode_reward=-4904.34 +/- 4438.05\n",
      "Episode length: 5.80 +/- 4.62\n",
      "Eval num_timesteps=196000, episode_reward=-5318.05 +/- 4339.92\n",
      "Episode length: 5.80 +/- 4.59\n",
      "Eval num_timesteps=197000, episode_reward=-3835.72 +/- 4169.61\n",
      "Episode length: 3.25 +/- 3.16\n",
      "Eval num_timesteps=198000, episode_reward=-3654.09 +/- 4415.98\n",
      "Episode length: 5.25 +/- 4.74\n",
      "Eval num_timesteps=199000, episode_reward=-4507.31 +/- 4446.85\n",
      "Episode length: 5.70 +/- 4.50\n",
      "Eval num_timesteps=200000, episode_reward=-4064.67 +/- 4262.16\n",
      "Episode length: 5.30 +/- 4.22\n",
      "Eval num_timesteps=201000, episode_reward=-4443.89 +/- 4372.85\n",
      "Episode length: 4.95 +/- 4.35\n",
      "Eval num_timesteps=202000, episode_reward=-1726.57 +/- 3522.23\n",
      "Episode length: 2.95 +/- 3.04\n",
      "New best mean reward!\n",
      "Eval num_timesteps=203000, episode_reward=-4069.33 +/- 4404.73\n",
      "Episode length: 5.60 +/- 4.81\n",
      "Eval num_timesteps=204000, episode_reward=-4783.46 +/- 4336.75\n",
      "Episode length: 5.45 +/- 3.85\n",
      "Eval num_timesteps=205000, episode_reward=-4927.00 +/- 4375.37\n",
      "Episode length: 5.65 +/- 4.30\n",
      "Eval num_timesteps=206000, episode_reward=-4526.00 +/- 4513.10\n",
      "Episode length: 6.30 +/- 4.78\n",
      "Eval num_timesteps=207000, episode_reward=-4154.73 +/- 4442.42\n",
      "Episode length: 6.30 +/- 4.48\n",
      "Eval num_timesteps=208000, episode_reward=-4976.39 +/- 4446.62\n",
      "Episode length: 6.25 +/- 4.80\n",
      "Eval num_timesteps=209000, episode_reward=-4429.96 +/- 4430.34\n",
      "Episode length: 5.00 +/- 4.43\n",
      "Eval num_timesteps=210000, episode_reward=-4431.01 +/- 4366.14\n",
      "Episode length: 4.55 +/- 4.39\n",
      "Eval num_timesteps=211000, episode_reward=-3528.92 +/- 4295.41\n",
      "Episode length: 3.80 +/- 4.25\n",
      "Eval num_timesteps=212000, episode_reward=-2686.50 +/- 4044.12\n",
      "Episode length: 4.10 +/- 3.77\n",
      "Eval num_timesteps=213000, episode_reward=-2863.97 +/- 4082.47\n",
      "Episode length: 5.15 +/- 4.21\n",
      "Eval num_timesteps=214000, episode_reward=-3545.49 +/- 4310.77\n",
      "Episode length: 4.15 +/- 4.27\n",
      "Eval num_timesteps=215000, episode_reward=-3550.33 +/- 4293.75\n",
      "Episode length: 4.80 +/- 3.97\n",
      "Eval num_timesteps=216000, episode_reward=-4512.58 +/- 4419.92\n",
      "Episode length: 5.80 +/- 4.50\n",
      "Eval num_timesteps=217000, episode_reward=-6124.23 +/- 3973.58\n",
      "Episode length: 5.50 +/- 4.28\n",
      "Eval num_timesteps=218000, episode_reward=-5786.65 +/- 4242.41\n",
      "Episode length: 6.65 +/- 4.05\n",
      "Eval num_timesteps=219000, episode_reward=-5172.94 +/- 4183.97\n",
      "Episode length: 4.05 +/- 3.19\n",
      "Eval num_timesteps=220000, episode_reward=-6225.13 +/- 4042.92\n",
      "Episode length: 6.55 +/- 4.13\n",
      "Eval num_timesteps=221000, episode_reward=-3975.68 +/- 4361.58\n",
      "Episode length: 4.25 +/- 3.92\n",
      "Eval num_timesteps=222000, episode_reward=-4867.53 +/- 4342.73\n",
      "Episode length: 4.85 +/- 4.55\n",
      "Eval num_timesteps=223000, episode_reward=-4334.66 +/- 4297.45\n",
      "Episode length: 4.40 +/- 4.10\n",
      "Eval num_timesteps=224000, episode_reward=-3956.69 +/- 4411.17\n",
      "Episode length: 4.35 +/- 3.79\n",
      "Eval num_timesteps=225000, episode_reward=-4017.86 +/- 4408.95\n",
      "Episode length: 4.80 +/- 4.58\n",
      "Eval num_timesteps=226000, episode_reward=-4050.75 +/- 4397.76\n",
      "Episode length: 5.30 +/- 4.37\n",
      "Eval num_timesteps=227000, episode_reward=-3993.32 +/- 4394.25\n",
      "Episode length: 4.60 +/- 4.53\n",
      "Eval num_timesteps=228000, episode_reward=-5381.64 +/- 4361.93\n",
      "Episode length: 6.50 +/- 4.64\n",
      "Eval num_timesteps=229000, episode_reward=-3629.58 +/- 4416.04\n",
      "Episode length: 5.05 +/- 4.68\n",
      "Eval num_timesteps=230000, episode_reward=-4502.74 +/- 4385.36\n",
      "Episode length: 5.75 +/- 4.77\n",
      "Eval num_timesteps=231000, episode_reward=-2298.19 +/- 3884.76\n",
      "Episode length: 3.65 +/- 4.07\n",
      "Eval num_timesteps=232000, episode_reward=-2707.88 +/- 4144.07\n",
      "Episode length: 4.00 +/- 4.39\n",
      "Eval num_timesteps=233000, episode_reward=-5460.92 +/- 4371.66\n",
      "Episode length: 7.00 +/- 4.59\n",
      "Eval num_timesteps=234000, episode_reward=-3697.75 +/- 4406.08\n",
      "Episode length: 5.40 +/- 4.65\n",
      "Eval num_timesteps=235000, episode_reward=-4452.28 +/- 4390.26\n",
      "Episode length: 5.25 +/- 4.02\n",
      "Eval num_timesteps=236000, episode_reward=-4686.85 +/- 4188.60\n",
      "Episode length: 3.15 +/- 3.29\n",
      "Eval num_timesteps=237000, episode_reward=-3991.38 +/- 4245.01\n",
      "Episode length: 5.10 +/- 3.83\n",
      "Eval num_timesteps=238000, episode_reward=-4542.48 +/- 4541.33\n",
      "Episode length: 6.10 +/- 4.91\n",
      "Eval num_timesteps=239000, episode_reward=-4005.07 +/- 4240.25\n",
      "Episode length: 5.05 +/- 4.38\n",
      "Eval num_timesteps=240000, episode_reward=-5475.99 +/- 4408.01\n",
      "Episode length: 7.10 +/- 4.79\n",
      "Eval num_timesteps=241000, episode_reward=-3540.90 +/- 4369.75\n",
      "Episode length: 4.70 +/- 4.67\n",
      "Eval num_timesteps=242000, episode_reward=-4773.97 +/- 4306.81\n",
      "Episode length: 4.50 +/- 4.21\n",
      "Eval num_timesteps=243000, episode_reward=-2250.78 +/- 3859.17\n",
      "Episode length: 3.20 +/- 3.93\n",
      "Eval num_timesteps=244000, episode_reward=-3082.89 +/- 4159.25\n",
      "Episode length: 3.80 +/- 3.97\n",
      "Eval num_timesteps=245000, episode_reward=-4430.05 +/- 4330.08\n",
      "Episode length: 5.05 +/- 4.13\n",
      "Eval num_timesteps=246000, episode_reward=-3560.49 +/- 4382.46\n",
      "Episode length: 4.55 +/- 4.42\n",
      "Eval num_timesteps=247000, episode_reward=-4732.30 +/- 4323.74\n",
      "Episode length: 4.35 +/- 4.05\n",
      "Eval num_timesteps=248000, episode_reward=-3567.44 +/- 4359.04\n",
      "Episode length: 4.50 +/- 4.52\n",
      "Eval num_timesteps=249000, episode_reward=-2331.04 +/- 3916.08\n",
      "Episode length: 3.95 +/- 4.20\n",
      "Eval num_timesteps=250000, episode_reward=-2174.64 +/- 3756.04\n",
      "Episode length: 2.95 +/- 3.41\n",
      "Eval num_timesteps=251000, episode_reward=-1819.49 +/- 3612.86\n",
      "Episode length: 3.35 +/- 3.75\n",
      "Eval num_timesteps=252000, episode_reward=-5876.20 +/- 4200.40\n",
      "Episode length: 7.30 +/- 4.50\n",
      "Eval num_timesteps=253000, episode_reward=-4079.24 +/- 4416.64\n",
      "Episode length: 5.75 +/- 4.62\n",
      "Eval num_timesteps=254000, episode_reward=-5789.49 +/- 4239.25\n",
      "Episode length: 6.80 +/- 4.72\n",
      "Eval num_timesteps=255000, episode_reward=-1883.02 +/- 3552.23\n",
      "Episode length: 3.80 +/- 3.64\n",
      "Eval num_timesteps=256000, episode_reward=-4002.95 +/- 4404.68\n",
      "Episode length: 4.90 +/- 4.33\n",
      "Eval num_timesteps=257000, episode_reward=-4071.31 +/- 4428.02\n",
      "Episode length: 5.60 +/- 4.77\n",
      "Eval num_timesteps=258000, episode_reward=-2736.12 +/- 4101.67\n",
      "Episode length: 4.10 +/- 4.44\n",
      "Eval num_timesteps=259000, episode_reward=-3587.59 +/- 4426.17\n",
      "Episode length: 5.40 +/- 4.57\n",
      "Eval num_timesteps=260000, episode_reward=-4443.10 +/- 4525.67\n",
      "Episode length: 5.70 +/- 4.62\n",
      "Eval num_timesteps=261000, episode_reward=-4884.99 +/- 4437.35\n",
      "Episode length: 5.90 +/- 4.54\n",
      "Eval num_timesteps=262000, episode_reward=-4887.15 +/- 4421.31\n",
      "Episode length: 5.85 +/- 4.49\n",
      "Eval num_timesteps=263000, episode_reward=-3604.16 +/- 4459.68\n",
      "Episode length: 5.10 +/- 4.83\n",
      "Eval num_timesteps=264000, episode_reward=-3135.08 +/- 4280.32\n",
      "Episode length: 4.40 +/- 4.47\n",
      "Eval num_timesteps=265000, episode_reward=-3981.65 +/- 4435.03\n",
      "Episode length: 4.70 +/- 4.64\n",
      "Eval num_timesteps=266000, episode_reward=-3659.66 +/- 4331.20\n",
      "Episode length: 5.45 +/- 4.47\n",
      "Eval num_timesteps=267000, episode_reward=-3184.92 +/- 4250.72\n",
      "Episode length: 4.60 +/- 4.72\n",
      "Eval num_timesteps=268000, episode_reward=-2672.85 +/- 4067.21\n",
      "Episode length: 4.00 +/- 4.14\n",
      "Eval num_timesteps=269000, episode_reward=-4468.23 +/- 4401.68\n",
      "Episode length: 5.65 +/- 4.48\n",
      "Eval num_timesteps=270000, episode_reward=-3120.54 +/- 4226.75\n",
      "Episode length: 4.40 +/- 4.51\n",
      "Eval num_timesteps=271000, episode_reward=-4509.04 +/- 4459.03\n",
      "Episode length: 5.95 +/- 4.62\n",
      "Eval num_timesteps=272000, episode_reward=-2661.87 +/- 3967.32\n",
      "Episode length: 2.90 +/- 3.53\n",
      "Eval num_timesteps=273000, episode_reward=-2664.28 +/- 4071.39\n",
      "Episode length: 3.70 +/- 4.10\n",
      "Eval num_timesteps=274000, episode_reward=-1396.24 +/- 3136.28\n",
      "Episode length: 3.30 +/- 3.16\n",
      "New best mean reward!\n",
      "Eval num_timesteps=275000, episode_reward=-2640.55 +/- 4003.49\n",
      "Episode length: 3.30 +/- 3.69\n",
      "Eval num_timesteps=276000, episode_reward=-2317.28 +/- 3853.88\n",
      "Episode length: 3.95 +/- 3.96\n",
      "Eval num_timesteps=277000, episode_reward=-2246.24 +/- 3845.09\n",
      "Episode length: 3.30 +/- 3.73\n",
      "Eval num_timesteps=278000, episode_reward=-2706.10 +/- 4078.37\n",
      "Episode length: 4.10 +/- 4.16\n",
      "Eval num_timesteps=279000, episode_reward=-3014.39 +/- 4017.46\n",
      "Episode length: 3.25 +/- 3.39\n",
      "Eval num_timesteps=280000, episode_reward=-2604.46 +/- 3983.42\n",
      "Episode length: 3.90 +/- 3.60\n",
      "Eval num_timesteps=281000, episode_reward=-4002.51 +/- 4332.25\n",
      "Episode length: 5.35 +/- 4.28\n",
      "Eval num_timesteps=282000, episode_reward=-2259.94 +/- 3834.98\n",
      "Episode length: 3.65 +/- 4.02\n",
      "Eval num_timesteps=283000, episode_reward=-3597.51 +/- 4261.46\n",
      "Episode length: 5.25 +/- 3.66\n",
      "Eval num_timesteps=284000, episode_reward=-3621.83 +/- 4394.86\n",
      "Episode length: 4.90 +/- 4.61\n",
      "Eval num_timesteps=285000, episode_reward=-4482.78 +/- 4401.31\n",
      "Episode length: 5.75 +/- 4.52\n",
      "Eval num_timesteps=286000, episode_reward=-4399.01 +/- 4438.47\n",
      "Episode length: 5.20 +/- 4.42\n",
      "Eval num_timesteps=287000, episode_reward=-3083.04 +/- 4102.26\n",
      "Episode length: 3.55 +/- 3.83\n",
      "Eval num_timesteps=288000, episode_reward=-4785.99 +/- 4329.49\n",
      "Episode length: 4.85 +/- 3.90\n",
      "Eval num_timesteps=289000, episode_reward=-3691.45 +/- 4337.80\n",
      "Episode length: 5.95 +/- 4.44\n",
      "Eval num_timesteps=290000, episode_reward=-3106.85 +/- 4194.62\n",
      "Episode length: 4.15 +/- 4.27\n",
      "Eval num_timesteps=291000, episode_reward=-3098.34 +/- 4131.70\n",
      "Episode length: 3.35 +/- 3.90\n",
      "Eval num_timesteps=292000, episode_reward=-1811.52 +/- 3408.27\n",
      "Episode length: 3.35 +/- 3.13\n",
      "Eval num_timesteps=293000, episode_reward=-4028.71 +/- 4290.71\n",
      "Episode length: 5.75 +/- 4.17\n",
      "Eval num_timesteps=294000, episode_reward=-3530.01 +/- 4189.00\n",
      "Episode length: 3.95 +/- 3.89\n",
      "Eval num_timesteps=295000, episode_reward=-3157.18 +/- 4216.74\n",
      "Episode length: 4.25 +/- 4.48\n",
      "Eval num_timesteps=296000, episode_reward=-4497.11 +/- 4471.96\n",
      "Episode length: 6.00 +/- 4.45\n",
      "Eval num_timesteps=297000, episode_reward=-2600.80 +/- 3895.77\n",
      "Episode length: 2.75 +/- 3.30\n",
      "Eval num_timesteps=298000, episode_reward=-2629.94 +/- 3933.57\n",
      "Episode length: 3.25 +/- 3.22\n",
      "Eval num_timesteps=299000, episode_reward=-1873.69 +/- 3589.93\n",
      "Episode length: 3.40 +/- 3.56\n",
      "Eval num_timesteps=300000, episode_reward=-3304.56 +/- 4278.78\n",
      "Episode length: 5.30 +/- 4.68\n",
      "Eval num_timesteps=301000, episode_reward=-2726.37 +/- 4069.10\n",
      "Episode length: 3.65 +/- 3.93\n",
      "Eval num_timesteps=302000, episode_reward=-1381.95 +/- 3175.55\n",
      "Episode length: 2.30 +/- 2.83\n",
      "New best mean reward!\n",
      "Eval num_timesteps=303000, episode_reward=-3605.19 +/- 4382.91\n",
      "Episode length: 4.70 +/- 4.65\n",
      "Eval num_timesteps=304000, episode_reward=-2317.57 +/- 3893.08\n",
      "Episode length: 4.10 +/- 4.16\n",
      "Eval num_timesteps=305000, episode_reward=-3155.23 +/- 4230.46\n",
      "Episode length: 4.30 +/- 4.10\n",
      "Eval num_timesteps=306000, episode_reward=-582.68 +/- 1923.96\n",
      "Episode length: 2.50 +/- 2.87\n",
      "New best mean reward!\n",
      "Eval num_timesteps=307000, episode_reward=-4409.39 +/- 4430.13\n",
      "Episode length: 4.85 +/- 4.57\n",
      "Eval num_timesteps=308000, episode_reward=-2549.85 +/- 3862.14\n",
      "Episode length: 2.25 +/- 2.32\n",
      "Eval num_timesteps=309000, episode_reward=-1348.41 +/- 3177.53\n",
      "Episode length: 2.30 +/- 2.98\n",
      "Eval num_timesteps=310000, episode_reward=-3064.77 +/- 4111.47\n",
      "Episode length: 3.20 +/- 3.17\n",
      "Eval num_timesteps=311000, episode_reward=-1785.77 +/- 3510.15\n",
      "Episode length: 3.00 +/- 3.27\n",
      "Eval num_timesteps=312000, episode_reward=-3216.26 +/- 4290.15\n",
      "Episode length: 4.60 +/- 4.71\n",
      "Eval num_timesteps=313000, episode_reward=-3064.03 +/- 4141.54\n",
      "Episode length: 3.60 +/- 3.47\n",
      "Eval num_timesteps=314000, episode_reward=-3575.60 +/- 4281.45\n",
      "Episode length: 4.40 +/- 4.18\n",
      "Eval num_timesteps=315000, episode_reward=-4039.76 +/- 4374.91\n",
      "Episode length: 5.20 +/- 4.46\n",
      "Eval num_timesteps=316000, episode_reward=-5260.17 +/- 4261.43\n",
      "Episode length: 5.55 +/- 4.40\n",
      "Eval num_timesteps=317000, episode_reward=-3134.29 +/- 4213.42\n",
      "Episode length: 3.85 +/- 4.20\n",
      "Eval num_timesteps=318000, episode_reward=-2329.45 +/- 3850.30\n",
      "Episode length: 4.50 +/- 4.10\n",
      "Eval num_timesteps=319000, episode_reward=-2657.49 +/- 4023.38\n",
      "Episode length: 3.75 +/- 3.77\n",
      "Eval num_timesteps=320000, episode_reward=-2230.71 +/- 3768.82\n",
      "Episode length: 3.00 +/- 3.44\n",
      "Eval num_timesteps=321000, episode_reward=-3556.62 +/- 4247.91\n",
      "Episode length: 4.35 +/- 4.22\n",
      "Eval num_timesteps=322000, episode_reward=-5370.67 +/- 4273.73\n",
      "Episode length: 6.10 +/- 4.52\n",
      "Eval num_timesteps=323000, episode_reward=-3725.19 +/- 4238.63\n",
      "Episode length: 6.25 +/- 3.86\n",
      "Eval num_timesteps=324000, episode_reward=-3994.96 +/- 4172.13\n",
      "Episode length: 4.15 +/- 3.62\n",
      "Eval num_timesteps=325000, episode_reward=-2278.29 +/- 3831.11\n",
      "Episode length: 3.45 +/- 3.99\n",
      "Eval num_timesteps=326000, episode_reward=-3575.90 +/- 4236.65\n",
      "Episode length: 4.35 +/- 3.82\n",
      "Eval num_timesteps=327000, episode_reward=-3128.51 +/- 4178.53\n",
      "Episode length: 4.50 +/- 3.98\n",
      "Eval num_timesteps=328000, episode_reward=-4814.32 +/- 4305.42\n",
      "Episode length: 4.55 +/- 3.97\n",
      "Eval num_timesteps=329000, episode_reward=-5786.73 +/- 4269.31\n",
      "Episode length: 6.40 +/- 4.57\n",
      "Eval num_timesteps=330000, episode_reward=-2317.65 +/- 3834.96\n",
      "Episode length: 3.80 +/- 3.52\n",
      "Eval num_timesteps=331000, episode_reward=-1843.82 +/- 3445.84\n",
      "Episode length: 3.10 +/- 2.96\n",
      "Eval num_timesteps=332000, episode_reward=-3631.28 +/- 4312.64\n",
      "Episode length: 4.80 +/- 4.19\n",
      "Eval num_timesteps=333000, episode_reward=-5324.91 +/- 4351.49\n",
      "Episode length: 6.20 +/- 4.57\n",
      "Eval num_timesteps=334000, episode_reward=-3223.61 +/- 4326.08\n",
      "Episode length: 5.05 +/- 4.42\n",
      "Eval num_timesteps=335000, episode_reward=-1414.43 +/- 3155.54\n",
      "Episode length: 3.10 +/- 3.00\n",
      "Eval num_timesteps=336000, episode_reward=-3837.20 +/- 4110.31\n",
      "Episode length: 2.65 +/- 2.29\n",
      "Eval num_timesteps=337000, episode_reward=-2713.09 +/- 3951.08\n",
      "Episode length: 4.35 +/- 3.45\n",
      "Eval num_timesteps=338000, episode_reward=-2254.84 +/- 3763.70\n",
      "Episode length: 3.30 +/- 3.33\n",
      "Eval num_timesteps=339000, episode_reward=-2686.80 +/- 3936.08\n",
      "Episode length: 3.40 +/- 3.43\n",
      "Eval num_timesteps=340000, episode_reward=-981.01 +/- 2641.42\n",
      "Episode length: 2.30 +/- 2.43\n",
      "Eval num_timesteps=341000, episode_reward=-1804.22 +/- 3558.33\n",
      "Episode length: 3.05 +/- 3.43\n",
      "Eval num_timesteps=342000, episode_reward=-1458.08 +/- 3219.16\n",
      "Episode length: 3.30 +/- 3.39\n",
      "Eval num_timesteps=343000, episode_reward=-2226.93 +/- 3794.97\n",
      "Episode length: 3.25 +/- 3.33\n",
      "Eval num_timesteps=344000, episode_reward=-2102.54 +/- 3589.38\n",
      "Episode length: 2.15 +/- 1.49\n",
      "Eval num_timesteps=345000, episode_reward=-1351.16 +/- 3144.01\n",
      "Episode length: 3.10 +/- 2.90\n",
      "Eval num_timesteps=346000, episode_reward=-1461.36 +/- 3160.59\n",
      "Episode length: 3.65 +/- 3.38\n",
      "Eval num_timesteps=347000, episode_reward=-82.35 +/- 141.74\n",
      "Episode length: 2.30 +/- 1.62\n",
      "New best mean reward!\n",
      "Eval num_timesteps=348000, episode_reward=-1809.01 +/- 3340.83\n",
      "Episode length: 2.90 +/- 1.84\n",
      "Eval num_timesteps=349000, episode_reward=-3716.72 +/- 4407.40\n",
      "Episode length: 5.70 +/- 4.45\n",
      "Eval num_timesteps=350000, episode_reward=-1775.69 +/- 3357.50\n",
      "Episode length: 2.95 +/- 2.42\n",
      "Eval num_timesteps=351000, episode_reward=-3614.96 +/- 4179.95\n",
      "Episode length: 4.60 +/- 3.67\n",
      "Eval num_timesteps=352000, episode_reward=-3167.47 +/- 4209.38\n",
      "Episode length: 4.50 +/- 3.93\n",
      "Eval num_timesteps=353000, episode_reward=-1409.48 +/- 3059.89\n",
      "Episode length: 2.95 +/- 2.48\n",
      "Eval num_timesteps=354000, episode_reward=-1803.44 +/- 3457.59\n",
      "Episode length: 2.95 +/- 2.67\n",
      "Eval num_timesteps=355000, episode_reward=-3417.51 +/- 4113.15\n",
      "Episode length: 2.80 +/- 2.56\n",
      "Eval num_timesteps=356000, episode_reward=-2299.45 +/- 3881.98\n",
      "Episode length: 3.65 +/- 3.90\n",
      "Eval num_timesteps=357000, episode_reward=-1755.93 +/- 3308.79\n",
      "Episode length: 2.65 +/- 2.35\n",
      "Eval num_timesteps=358000, episode_reward=-1915.05 +/- 3518.98\n",
      "Episode length: 3.80 +/- 3.23\n",
      "Eval num_timesteps=359000, episode_reward=-2240.28 +/- 3799.60\n",
      "Episode length: 3.25 +/- 3.14\n",
      "Eval num_timesteps=360000, episode_reward=-2198.58 +/- 3630.75\n",
      "Episode length: 2.65 +/- 2.31\n",
      "Eval num_timesteps=361000, episode_reward=-1751.99 +/- 3418.53\n",
      "Episode length: 2.45 +/- 1.80\n",
      "Eval num_timesteps=362000, episode_reward=-1787.74 +/- 3461.92\n",
      "Episode length: 2.90 +/- 2.88\n",
      "Eval num_timesteps=363000, episode_reward=-2333.23 +/- 3860.42\n",
      "Episode length: 4.15 +/- 4.04\n",
      "Eval num_timesteps=364000, episode_reward=-2231.49 +/- 3823.39\n",
      "Episode length: 3.40 +/- 3.50\n",
      "Eval num_timesteps=365000, episode_reward=-3128.51 +/- 4212.78\n",
      "Episode length: 4.30 +/- 4.45\n",
      "Eval num_timesteps=366000, episode_reward=-1803.97 +/- 3451.55\n",
      "Episode length: 3.30 +/- 2.90\n",
      "Eval num_timesteps=367000, episode_reward=-3122.63 +/- 4193.63\n",
      "Episode length: 3.90 +/- 4.16\n",
      "Eval num_timesteps=368000, episode_reward=-2648.06 +/- 3933.44\n",
      "Episode length: 3.10 +/- 3.37\n",
      "Eval num_timesteps=369000, episode_reward=-2691.69 +/- 3962.66\n",
      "Episode length: 3.90 +/- 3.79\n",
      "Eval num_timesteps=370000, episode_reward=-1891.89 +/- 3608.65\n",
      "Episode length: 3.50 +/- 3.79\n",
      "Eval num_timesteps=371000, episode_reward=-1817.88 +/- 3491.28\n",
      "Episode length: 3.20 +/- 3.41\n",
      "Eval num_timesteps=372000, episode_reward=-1837.80 +/- 3327.84\n",
      "Episode length: 3.15 +/- 2.46\n",
      "Eval num_timesteps=373000, episode_reward=-67.92 +/- 123.01\n",
      "Episode length: 1.65 +/- 0.85\n",
      "New best mean reward!\n",
      "Eval num_timesteps=374000, episode_reward=-1337.70 +/- 3130.90\n",
      "Episode length: 2.40 +/- 2.96\n",
      "Eval num_timesteps=375000, episode_reward=-65.70 +/- 128.00\n",
      "Episode length: 1.80 +/- 1.12\n",
      "New best mean reward!\n",
      "Eval num_timesteps=376000, episode_reward=-1397.28 +/- 3238.84\n",
      "Episode length: 3.10 +/- 3.45\n",
      "Eval num_timesteps=377000, episode_reward=-1024.03 +/- 2709.35\n",
      "Episode length: 2.75 +/- 3.00\n",
      "Eval num_timesteps=378000, episode_reward=-2210.46 +/- 3761.36\n",
      "Episode length: 3.15 +/- 3.44\n",
      "Eval num_timesteps=379000, episode_reward=-2351.95 +/- 3765.40\n",
      "Episode length: 3.95 +/- 3.51\n",
      "Eval num_timesteps=380000, episode_reward=-3188.41 +/- 4218.06\n",
      "Episode length: 4.65 +/- 4.27\n",
      "Eval num_timesteps=381000, episode_reward=-1924.74 +/- 3555.92\n",
      "Episode length: 4.00 +/- 3.65\n",
      "Eval num_timesteps=382000, episode_reward=-1407.52 +/- 3192.88\n",
      "Episode length: 3.00 +/- 3.49\n",
      "Eval num_timesteps=383000, episode_reward=-1898.68 +/- 3560.52\n",
      "Episode length: 3.70 +/- 3.74\n",
      "Eval num_timesteps=384000, episode_reward=-1797.84 +/- 3489.86\n",
      "Episode length: 2.60 +/- 2.97\n",
      "Eval num_timesteps=385000, episode_reward=-2686.88 +/- 3995.85\n",
      "Episode length: 3.75 +/- 3.19\n",
      "Eval num_timesteps=386000, episode_reward=-1872.51 +/- 3595.96\n",
      "Episode length: 3.55 +/- 3.56\n",
      "Eval num_timesteps=387000, episode_reward=-2282.35 +/- 3827.66\n",
      "Episode length: 3.55 +/- 3.92\n",
      "Eval num_timesteps=388000, episode_reward=-938.37 +/- 2471.22\n",
      "Episode length: 2.65 +/- 1.80\n",
      "Eval num_timesteps=389000, episode_reward=-2755.80 +/- 4106.76\n",
      "Episode length: 4.30 +/- 4.43\n",
      "Eval num_timesteps=390000, episode_reward=-989.46 +/- 2694.75\n",
      "Episode length: 2.85 +/- 3.07\n",
      "Eval num_timesteps=391000, episode_reward=-1394.47 +/- 3205.38\n",
      "Episode length: 2.75 +/- 3.51\n",
      "Eval num_timesteps=392000, episode_reward=-2710.19 +/- 4055.78\n",
      "Episode length: 4.30 +/- 4.09\n",
      "Eval num_timesteps=393000, episode_reward=-626.61 +/- 1953.06\n",
      "Episode length: 2.90 +/- 2.55\n",
      "Eval num_timesteps=394000, episode_reward=-545.98 +/- 1976.15\n",
      "Episode length: 2.55 +/- 2.29\n",
      "Eval num_timesteps=395000, episode_reward=-2332.62 +/- 3908.78\n",
      "Episode length: 3.80 +/- 4.23\n",
      "Eval num_timesteps=396000, episode_reward=-1405.93 +/- 3114.95\n",
      "Episode length: 2.50 +/- 2.58\n",
      "Eval num_timesteps=397000, episode_reward=-1893.74 +/- 3611.68\n",
      "Episode length: 3.60 +/- 3.83\n",
      "Eval num_timesteps=398000, episode_reward=-1927.43 +/- 3599.62\n",
      "Episode length: 4.05 +/- 3.76\n",
      "Eval num_timesteps=399000, episode_reward=-1793.21 +/- 3549.32\n",
      "Episode length: 2.80 +/- 3.50\n",
      "Eval num_timesteps=400000, episode_reward=-524.67 +/- 1971.39\n",
      "Episode length: 1.75 +/- 2.19\n",
      "Eval num_timesteps=401000, episode_reward=-2268.40 +/- 3736.52\n",
      "Episode length: 3.10 +/- 3.45\n",
      "Eval num_timesteps=402000, episode_reward=-885.26 +/- 2695.59\n",
      "Episode length: 2.35 +/- 2.48\n",
      "Eval num_timesteps=403000, episode_reward=-1391.12 +/- 3225.87\n",
      "Episode length: 2.70 +/- 3.55\n",
      "Eval num_timesteps=404000, episode_reward=-1847.33 +/- 3465.42\n",
      "Episode length: 3.35 +/- 3.00\n",
      "Eval num_timesteps=405000, episode_reward=-2313.11 +/- 3810.53\n",
      "Episode length: 4.20 +/- 3.66\n",
      "Eval num_timesteps=406000, episode_reward=-3605.11 +/- 4342.07\n",
      "Episode length: 4.75 +/- 4.24\n",
      "Eval num_timesteps=407000, episode_reward=-938.40 +/- 2566.95\n",
      "Episode length: 2.25 +/- 1.87\n",
      "Eval num_timesteps=408000, episode_reward=-1002.17 +/- 2706.75\n",
      "Episode length: 2.65 +/- 2.89\n",
      "Eval num_timesteps=409000, episode_reward=-1029.48 +/- 2674.33\n",
      "Episode length: 2.95 +/- 2.87\n",
      "Eval num_timesteps=410000, episode_reward=-444.60 +/- 1783.38\n",
      "Episode length: 1.35 +/- 0.57\n",
      "Eval num_timesteps=411000, episode_reward=-1408.32 +/- 3061.26\n",
      "Episode length: 3.05 +/- 2.42\n",
      "Eval num_timesteps=412000, episode_reward=-871.25 +/- 2465.29\n",
      "Episode length: 1.60 +/- 0.97\n",
      "Eval num_timesteps=413000, episode_reward=-4015.07 +/- 4363.13\n",
      "Episode length: 4.90 +/- 3.66\n",
      "Eval num_timesteps=414000, episode_reward=-969.13 +/- 2635.73\n",
      "Episode length: 2.45 +/- 2.46\n",
      "Eval num_timesteps=415000, episode_reward=-1804.48 +/- 3433.12\n",
      "Episode length: 3.05 +/- 2.99\n",
      "Eval num_timesteps=416000, episode_reward=-2637.99 +/- 3918.40\n",
      "Episode length: 3.15 +/- 3.13\n",
      "Eval num_timesteps=417000, episode_reward=-943.52 +/- 2711.93\n",
      "Episode length: 2.70 +/- 2.95\n",
      "Eval num_timesteps=418000, episode_reward=-3175.28 +/- 4233.00\n",
      "Episode length: 4.50 +/- 3.92\n",
      "Eval num_timesteps=419000, episode_reward=-1423.45 +/- 3203.41\n",
      "Episode length: 3.15 +/- 3.41\n",
      "Eval num_timesteps=420000, episode_reward=-905.63 +/- 2582.57\n",
      "Episode length: 1.90 +/- 2.21\n",
      "Eval num_timesteps=421000, episode_reward=-1362.52 +/- 3178.37\n",
      "Episode length: 2.85 +/- 2.94\n",
      "Eval num_timesteps=422000, episode_reward=-2273.14 +/- 3927.54\n",
      "Episode length: 4.20 +/- 4.03\n",
      "Eval num_timesteps=423000, episode_reward=-951.29 +/- 2721.12\n",
      "Episode length: 2.55 +/- 2.99\n",
      "Eval num_timesteps=424000, episode_reward=-437.40 +/- 1939.33\n",
      "Episode length: 2.05 +/- 1.66\n",
      "Eval num_timesteps=425000, episode_reward=-1362.54 +/- 3147.37\n",
      "Episode length: 2.90 +/- 2.91\n",
      "Eval num_timesteps=426000, episode_reward=-2705.39 +/- 4072.03\n",
      "Episode length: 3.95 +/- 4.15\n",
      "Eval num_timesteps=427000, episode_reward=-1793.89 +/- 3407.48\n",
      "Episode length: 2.70 +/- 2.43\n",
      "Eval num_timesteps=428000, episode_reward=-547.28 +/- 1963.64\n",
      "Episode length: 2.35 +/- 2.10\n",
      "Eval num_timesteps=429000, episode_reward=-1862.72 +/- 3623.76\n",
      "Episode length: 3.55 +/- 3.79\n",
      "Eval num_timesteps=430000, episode_reward=-1332.55 +/- 3162.45\n",
      "Episode length: 2.90 +/- 2.96\n",
      "Eval num_timesteps=431000, episode_reward=-2214.42 +/- 3769.74\n",
      "Episode length: 3.00 +/- 3.42\n",
      "Eval num_timesteps=432000, episode_reward=-2755.76 +/- 4004.03\n",
      "Episode length: 4.10 +/- 3.56\n",
      "Eval num_timesteps=433000, episode_reward=-948.48 +/- 2573.77\n",
      "Episode length: 2.20 +/- 2.27\n",
      "Eval num_timesteps=434000, episode_reward=-2680.69 +/- 4106.71\n",
      "Episode length: 3.85 +/- 3.94\n",
      "Eval num_timesteps=435000, episode_reward=-539.52 +/- 1968.59\n",
      "Episode length: 2.10 +/- 2.57\n",
      "Eval num_timesteps=436000, episode_reward=-1314.22 +/- 3053.80\n",
      "Episode length: 2.20 +/- 2.11\n",
      "Eval num_timesteps=437000, episode_reward=-86.68 +/- 130.41\n",
      "Episode length: 1.70 +/- 1.19\n",
      "Eval num_timesteps=438000, episode_reward=-2389.52 +/- 3911.31\n",
      "Episode length: 4.50 +/- 3.97\n",
      "Eval num_timesteps=439000, episode_reward=-1406.15 +/- 3255.85\n",
      "Episode length: 3.20 +/- 3.43\n",
      "Eval num_timesteps=440000, episode_reward=-2276.24 +/- 3703.01\n",
      "Episode length: 3.95 +/- 3.22\n",
      "Eval num_timesteps=441000, episode_reward=-466.17 +/- 1828.91\n",
      "Episode length: 1.45 +/- 0.97\n",
      "Eval num_timesteps=442000, episode_reward=-910.27 +/- 2627.59\n",
      "Episode length: 2.45 +/- 2.29\n",
      "Eval num_timesteps=443000, episode_reward=-516.69 +/- 1949.11\n",
      "Episode length: 2.40 +/- 2.40\n",
      "Eval num_timesteps=444000, episode_reward=-529.02 +/- 1910.58\n",
      "Episode length: 2.55 +/- 1.72\n",
      "Eval num_timesteps=445000, episode_reward=-1480.98 +/- 3202.41\n",
      "Episode length: 3.75 +/- 3.43\n",
      "Eval num_timesteps=446000, episode_reward=-547.96 +/- 1949.01\n",
      "Episode length: 2.55 +/- 2.33\n",
      "Eval num_timesteps=447000, episode_reward=-1372.43 +/- 3255.75\n",
      "Episode length: 2.75 +/- 3.51\n",
      "Eval num_timesteps=448000, episode_reward=-973.71 +/- 2710.69\n",
      "Episode length: 2.50 +/- 3.01\n",
      "Eval num_timesteps=449000, episode_reward=-959.27 +/- 2715.68\n",
      "Episode length: 2.45 +/- 3.01\n",
      "Eval num_timesteps=450000, episode_reward=-977.28 +/- 2709.84\n",
      "Episode length: 2.65 +/- 3.00\n",
      "Eval num_timesteps=451000, episode_reward=-2282.33 +/- 3885.08\n",
      "Episode length: 3.80 +/- 3.88\n",
      "Eval num_timesteps=452000, episode_reward=-2282.27 +/- 3865.26\n",
      "Episode length: 3.50 +/- 3.88\n",
      "Eval num_timesteps=453000, episode_reward=-976.86 +/- 2612.42\n",
      "Episode length: 2.40 +/- 2.31\n",
      "Eval num_timesteps=454000, episode_reward=-1422.14 +/- 3221.84\n",
      "Episode length: 3.10 +/- 3.43\n",
      "Eval num_timesteps=455000, episode_reward=-960.70 +/- 2560.53\n",
      "Episode length: 2.65 +/- 2.33\n",
      "Eval num_timesteps=456000, episode_reward=-1848.92 +/- 3455.89\n",
      "Episode length: 3.20 +/- 2.84\n",
      "Eval num_timesteps=457000, episode_reward=-45.58 +/- 171.32\n",
      "Episode length: 1.80 +/- 1.21\n",
      "New best mean reward!\n",
      "Eval num_timesteps=458000, episode_reward=-522.94 +/- 1775.47\n",
      "Episode length: 2.30 +/- 2.05\n",
      "Eval num_timesteps=459000, episode_reward=-91.43 +/- 152.52\n",
      "Episode length: 2.05 +/- 1.20\n",
      "Eval num_timesteps=460000, episode_reward=-89.51 +/- 160.02\n",
      "Episode length: 2.10 +/- 1.26\n",
      "Eval num_timesteps=461000, episode_reward=-948.20 +/- 2703.93\n",
      "Episode length: 2.55 +/- 2.97\n",
      "Eval num_timesteps=462000, episode_reward=-980.72 +/- 2707.97\n",
      "Episode length: 2.65 +/- 2.92\n",
      "Eval num_timesteps=463000, episode_reward=-77.88 +/- 161.37\n",
      "Episode length: 2.00 +/- 1.10\n",
      "Eval num_timesteps=464000, episode_reward=-1403.09 +/- 3236.77\n",
      "Episode length: 2.90 +/- 3.46\n",
      "Eval num_timesteps=465000, episode_reward=-473.07 +/- 1777.78\n",
      "Episode length: 1.70 +/- 0.84\n",
      "Eval num_timesteps=466000, episode_reward=-496.94 +/- 1963.71\n",
      "Episode length: 1.80 +/- 2.20\n",
      "Eval num_timesteps=467000, episode_reward=-492.41 +/- 1966.52\n",
      "Episode length: 2.05 +/- 2.20\n",
      "Eval num_timesteps=468000, episode_reward=-925.32 +/- 2608.79\n",
      "Episode length: 2.45 +/- 2.36\n",
      "Eval num_timesteps=469000, episode_reward=-1040.52 +/- 2682.00\n",
      "Episode length: 3.80 +/- 3.66\n",
      "Eval num_timesteps=470000, episode_reward=-466.64 +/- 1779.07\n",
      "Episode length: 1.95 +/- 0.97\n",
      "Eval num_timesteps=471000, episode_reward=-481.59 +/- 1984.57\n",
      "Episode length: 1.85 +/- 2.20\n",
      "Eval num_timesteps=472000, episode_reward=-1730.25 +/- 3428.69\n",
      "Episode length: 2.80 +/- 3.03\n",
      "Eval num_timesteps=473000, episode_reward=-110.24 +/- 130.67\n",
      "Episode length: 2.00 +/- 1.10\n",
      "Eval num_timesteps=474000, episode_reward=-2650.57 +/- 3976.20\n",
      "Episode length: 3.85 +/- 3.80\n",
      "Eval num_timesteps=475000, episode_reward=-528.22 +/- 1947.08\n",
      "Episode length: 2.10 +/- 2.21\n",
      "Eval num_timesteps=476000, episode_reward=-949.84 +/- 2612.80\n",
      "Episode length: 2.25 +/- 2.64\n",
      "Eval num_timesteps=477000, episode_reward=-2216.51 +/- 3706.88\n",
      "Episode length: 2.80 +/- 2.98\n",
      "Eval num_timesteps=478000, episode_reward=-976.94 +/- 2695.61\n",
      "Episode length: 2.55 +/- 2.96\n",
      "Eval num_timesteps=479000, episode_reward=-926.38 +/- 2579.56\n",
      "Episode length: 2.00 +/- 2.19\n",
      "Eval num_timesteps=480000, episode_reward=-862.98 +/- 2448.84\n",
      "Episode length: 1.75 +/- 0.89\n",
      "Eval num_timesteps=481000, episode_reward=-1381.89 +/- 3122.85\n",
      "Episode length: 2.70 +/- 2.88\n",
      "Eval num_timesteps=482000, episode_reward=-1742.74 +/- 3322.96\n",
      "Episode length: 2.45 +/- 2.29\n",
      "Eval num_timesteps=483000, episode_reward=-1862.92 +/- 3625.84\n",
      "Episode length: 3.70 +/- 3.85\n",
      "Eval num_timesteps=484000, episode_reward=-1392.98 +/- 3080.81\n",
      "Episode length: 3.20 +/- 2.89\n",
      "Eval num_timesteps=485000, episode_reward=-1318.97 +/- 3015.91\n",
      "Episode length: 2.30 +/- 2.19\n",
      "Eval num_timesteps=486000, episode_reward=-2194.94 +/- 3707.95\n",
      "Episode length: 2.75 +/- 3.05\n",
      "Eval num_timesteps=487000, episode_reward=-2720.36 +/- 3959.79\n",
      "Episode length: 4.05 +/- 3.68\n",
      "Eval num_timesteps=488000, episode_reward=-1385.72 +/- 3021.43\n",
      "Episode length: 2.95 +/- 2.36\n",
      "Eval num_timesteps=489000, episode_reward=-505.80 +/- 1956.52\n",
      "Episode length: 2.30 +/- 2.22\n",
      "Eval num_timesteps=490000, episode_reward=-1084.31 +/- 2683.47\n",
      "Episode length: 3.50 +/- 3.50\n",
      "Eval num_timesteps=491000, episode_reward=-1045.58 +/- 2537.29\n",
      "Episode length: 4.15 +/- 3.26\n",
      "Eval num_timesteps=492000, episode_reward=-586.56 +/- 1970.37\n",
      "Episode length: 3.15 +/- 2.87\n",
      "Eval num_timesteps=493000, episode_reward=-2347.41 +/- 3868.75\n",
      "Episode length: 4.90 +/- 4.16\n",
      "Eval num_timesteps=494000, episode_reward=-1886.04 +/- 3489.48\n",
      "Episode length: 3.95 +/- 3.71\n",
      "Eval num_timesteps=495000, episode_reward=-948.80 +/- 2729.89\n",
      "Episode length: 2.55 +/- 2.91\n",
      "Eval num_timesteps=496000, episode_reward=-949.44 +/- 2727.38\n",
      "Episode length: 2.55 +/- 2.97\n",
      "Eval num_timesteps=497000, episode_reward=-1371.13 +/- 3044.33\n",
      "Episode length: 2.85 +/- 2.80\n",
      "Eval num_timesteps=498000, episode_reward=-2607.81 +/- 3927.76\n",
      "Episode length: 2.95 +/- 3.14\n",
      "Eval num_timesteps=499000, episode_reward=-514.05 +/- 1972.87\n",
      "Episode length: 2.40 +/- 2.18\n",
      "Eval num_timesteps=500000, episode_reward=-1393.65 +/- 3231.40\n",
      "Episode length: 2.85 +/- 3.53\n",
      "After retraining: mean_reward:-1009.68 +/- 2698.80\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# 重新加载模型\n",
    "model = DQN(\n",
    "    \"MlpPolicy\", \n",
    "    env, \n",
    "    verbose=0, \n",
    "    tensorboard_log=\"./tensorboard/DQN/\",\n",
    "    #policy_kwargs=policy_kwargs,\n",
    "    learning_rate=0.0005,  # 重新定义学习率\n",
    "    buffer_size=100000,\n",
    "    learning_starts=1000,\n",
    "    batch_size=32,\n",
    "    tau=1.0,\n",
    "    gamma=0.99,\n",
    "    train_freq=4,\n",
    "    target_update_interval=1000,\n",
    "    exploration_fraction=0.1,\n",
    "    exploration_final_eps=0.02,\n",
    "    max_grad_norm=10,\n",
    "    device=\"cuda\"  # 确保在GPU上训练\n",
    ")\n",
    "\n",
    "# 加载之前训练的最佳模型参数\n",
    "model.set_parameters(\"model/dqn_retrained_electric_vehicle.zip\")  # best_model 是之前保存的模型路径\n",
    "\n",
    "\n",
    "# 开始继续训练\n",
    "log_name = \"DQN_Retrain\"\n",
    "model.learn(\n",
    "    total_timesteps=500000,  # 继续训练200,000步\n",
    "    tb_log_name=log_name,\n",
    "    callback=eval_callback\n",
    ")\n",
    "\n",
    "# 保存重新训练的模型\n",
    "model.save(\"model/dqn_retrained_electric_vehicle\")\n",
    "\n",
    "# 评估再训练后的模型表现\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"After retraining: mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "# 测试训练好的模型\n",
    "obs = env.reset()\n",
    "for i in range(100):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        obs = env.reset()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-12T05:16:20.541429700Z",
     "start_time": "2024-09-12T03:51:01.794541300Z"
    }
   },
   "id": "53af7461969c03c7"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=-2266.95 +/- 3834.24\n",
      "Episode length: 3.75 +/- 3.74\n",
      "Eval num_timesteps=2000, episode_reward=-553.80 +/- 1963.85\n",
      "Episode length: 2.70 +/- 2.30\n",
      "Eval num_timesteps=3000, episode_reward=-477.86 +/- 1952.12\n",
      "Episode length: 2.15 +/- 2.24\n",
      "Eval num_timesteps=4000, episode_reward=-477.87 +/- 1797.20\n",
      "Episode length: 1.55 +/- 0.74\n",
      "Eval num_timesteps=5000, episode_reward=-497.09 +/- 1980.43\n",
      "Episode length: 2.15 +/- 2.37\n",
      "Eval num_timesteps=6000, episode_reward=-950.62 +/- 2599.56\n",
      "Episode length: 2.30 +/- 2.49\n",
      "Eval num_timesteps=7000, episode_reward=-509.32 +/- 1977.95\n",
      "Episode length: 2.10 +/- 2.23\n",
      "Eval num_timesteps=8000, episode_reward=-1412.45 +/- 3112.03\n",
      "Episode length: 2.95 +/- 2.89\n",
      "Eval num_timesteps=9000, episode_reward=-446.83 +/- 1836.46\n",
      "Episode length: 1.75 +/- 1.18\n",
      "Eval num_timesteps=10000, episode_reward=-1295.42 +/- 2997.57\n",
      "Episode length: 2.25 +/- 1.41\n",
      "Eval num_timesteps=11000, episode_reward=-1713.46 +/- 3377.72\n",
      "Episode length: 2.30 +/- 2.24\n",
      "Eval num_timesteps=12000, episode_reward=-2703.93 +/- 3893.87\n",
      "Episode length: 3.85 +/- 3.07\n",
      "Eval num_timesteps=13000, episode_reward=-1769.56 +/- 3188.21\n",
      "Episode length: 2.65 +/- 1.19\n",
      "Eval num_timesteps=14000, episode_reward=-3061.48 +/- 4045.90\n",
      "Episode length: 3.20 +/- 2.99\n",
      "Eval num_timesteps=15000, episode_reward=-1919.00 +/- 3440.90\n",
      "Episode length: 4.20 +/- 3.70\n",
      "Eval num_timesteps=16000, episode_reward=-3799.50 +/- 4084.05\n",
      "Episode length: 3.05 +/- 2.46\n",
      "Eval num_timesteps=17000, episode_reward=-2642.45 +/- 3853.54\n",
      "Episode length: 3.40 +/- 2.80\n",
      "Eval num_timesteps=18000, episode_reward=-1822.67 +/- 3520.61\n",
      "Episode length: 3.40 +/- 3.37\n",
      "Eval num_timesteps=19000, episode_reward=-550.76 +/- 1967.42\n",
      "Episode length: 2.80 +/- 2.36\n",
      "Eval num_timesteps=20000, episode_reward=-2612.35 +/- 3865.04\n",
      "Episode length: 3.45 +/- 2.89\n",
      "Eval num_timesteps=21000, episode_reward=-1783.39 +/- 3400.33\n",
      "Episode length: 3.00 +/- 2.88\n",
      "Eval num_timesteps=22000, episode_reward=-839.07 +/- 2422.67\n",
      "Episode length: 1.50 +/- 0.81\n",
      "Eval num_timesteps=23000, episode_reward=-457.96 +/- 1788.30\n",
      "Episode length: 1.90 +/- 1.14\n",
      "Eval num_timesteps=24000, episode_reward=-1432.85 +/- 3199.22\n",
      "Episode length: 3.70 +/- 3.38\n",
      "Eval num_timesteps=25000, episode_reward=-1036.28 +/- 2609.64\n",
      "Episode length: 3.20 +/- 2.68\n",
      "Eval num_timesteps=26000, episode_reward=-94.58 +/- 104.68\n",
      "Episode length: 1.85 +/- 1.01\n",
      "Eval num_timesteps=27000, episode_reward=-1029.87 +/- 2696.97\n",
      "Episode length: 3.20 +/- 3.09\n",
      "Eval num_timesteps=28000, episode_reward=-1809.06 +/- 3328.77\n",
      "Episode length: 3.15 +/- 2.73\n",
      "Eval num_timesteps=29000, episode_reward=-1068.75 +/- 2704.91\n",
      "Episode length: 3.90 +/- 3.53\n",
      "Eval num_timesteps=30000, episode_reward=-192.51 +/- 286.84\n",
      "Episode length: 2.85 +/- 2.61\n",
      "Eval num_timesteps=31000, episode_reward=-564.23 +/- 1930.21\n",
      "Episode length: 3.15 +/- 2.43\n",
      "Eval num_timesteps=32000, episode_reward=-994.29 +/- 2640.37\n",
      "Episode length: 2.60 +/- 2.63\n",
      "Eval num_timesteps=33000, episode_reward=-76.36 +/- 117.36\n",
      "Episode length: 1.85 +/- 1.15\n",
      "Eval num_timesteps=34000, episode_reward=-81.78 +/- 180.18\n",
      "Episode length: 2.05 +/- 1.24\n",
      "Eval num_timesteps=35000, episode_reward=-112.62 +/- 183.37\n",
      "Episode length: 2.20 +/- 1.25\n",
      "Eval num_timesteps=36000, episode_reward=-114.56 +/- 145.79\n",
      "Episode length: 2.35 +/- 1.15\n",
      "Eval num_timesteps=37000, episode_reward=-526.71 +/- 1974.01\n",
      "Episode length: 2.50 +/- 2.22\n",
      "Eval num_timesteps=38000, episode_reward=-892.93 +/- 2582.85\n",
      "Episode length: 2.35 +/- 2.43\n",
      "Eval num_timesteps=39000, episode_reward=-156.70 +/- 209.86\n",
      "Episode length: 2.60 +/- 1.96\n",
      "Eval num_timesteps=40000, episode_reward=-959.90 +/- 2561.39\n",
      "Episode length: 2.75 +/- 2.53\n",
      "Eval num_timesteps=41000, episode_reward=-982.04 +/- 2717.96\n",
      "Episode length: 2.70 +/- 2.93\n",
      "Eval num_timesteps=42000, episode_reward=-198.91 +/- 265.20\n",
      "Episode length: 3.35 +/- 2.31\n",
      "Eval num_timesteps=43000, episode_reward=-132.95 +/- 274.55\n",
      "Episode length: 3.00 +/- 2.45\n",
      "Eval num_timesteps=44000, episode_reward=-216.95 +/- 255.88\n",
      "Episode length: 3.60 +/- 2.35\n",
      "Eval num_timesteps=45000, episode_reward=-890.96 +/- 2462.65\n",
      "Episode length: 2.10 +/- 1.48\n",
      "Eval num_timesteps=46000, episode_reward=-909.69 +/- 2633.96\n",
      "Episode length: 2.95 +/- 2.62\n",
      "Eval num_timesteps=47000, episode_reward=-955.54 +/- 2548.76\n",
      "Episode length: 2.75 +/- 2.61\n",
      "Eval num_timesteps=48000, episode_reward=-914.07 +/- 2494.93\n",
      "Episode length: 2.15 +/- 1.49\n",
      "Eval num_timesteps=49000, episode_reward=-76.98 +/- 115.62\n",
      "Episode length: 1.70 +/- 0.78\n",
      "Eval num_timesteps=50000, episode_reward=-921.06 +/- 2535.13\n",
      "Episode length: 2.40 +/- 2.22\n",
      "Eval num_timesteps=51000, episode_reward=-1030.66 +/- 2606.24\n",
      "Episode length: 3.30 +/- 2.33\n",
      "Eval num_timesteps=52000, episode_reward=-105.06 +/- 109.81\n",
      "Episode length: 1.95 +/- 1.28\n",
      "Eval num_timesteps=53000, episode_reward=-1775.04 +/- 3308.94\n",
      "Episode length: 3.15 +/- 2.33\n",
      "Eval num_timesteps=54000, episode_reward=-535.30 +/- 1966.62\n",
      "Episode length: 2.05 +/- 2.18\n",
      "Eval num_timesteps=55000, episode_reward=-549.58 +/- 1772.58\n",
      "Episode length: 2.35 +/- 2.22\n",
      "Eval num_timesteps=56000, episode_reward=-547.87 +/- 1764.77\n",
      "Episode length: 2.55 +/- 2.13\n",
      "Eval num_timesteps=57000, episode_reward=-949.72 +/- 2401.22\n",
      "Episode length: 2.45 +/- 1.66\n",
      "Eval num_timesteps=58000, episode_reward=-553.07 +/- 1760.54\n",
      "Episode length: 2.70 +/- 1.49\n",
      "Eval num_timesteps=59000, episode_reward=-1066.38 +/- 2682.72\n",
      "Episode length: 3.60 +/- 2.96\n",
      "Eval num_timesteps=60000, episode_reward=-2267.37 +/- 3680.74\n",
      "Episode length: 3.55 +/- 2.91\n",
      "Eval num_timesteps=61000, episode_reward=-1828.91 +/- 3417.99\n",
      "Episode length: 3.50 +/- 2.73\n",
      "Eval num_timesteps=62000, episode_reward=-1867.35 +/- 3367.56\n",
      "Episode length: 4.35 +/- 3.29\n",
      "Eval num_timesteps=63000, episode_reward=-512.55 +/- 1770.73\n",
      "Episode length: 1.90 +/- 1.81\n",
      "Eval num_timesteps=64000, episode_reward=-911.42 +/- 2575.58\n",
      "Episode length: 2.45 +/- 2.27\n",
      "Eval num_timesteps=65000, episode_reward=-1400.97 +/- 3155.27\n",
      "Episode length: 3.30 +/- 3.12\n",
      "Eval num_timesteps=66000, episode_reward=-1811.44 +/- 3627.29\n",
      "Episode length: 3.45 +/- 3.81\n",
      "Eval num_timesteps=67000, episode_reward=-2154.60 +/- 3714.58\n",
      "Episode length: 3.10 +/- 3.39\n",
      "Eval num_timesteps=68000, episode_reward=-858.37 +/- 2482.80\n",
      "Episode length: 2.30 +/- 1.76\n",
      "Eval num_timesteps=69000, episode_reward=-856.34 +/- 2590.31\n",
      "Episode length: 2.35 +/- 2.31\n",
      "Eval num_timesteps=70000, episode_reward=-1756.93 +/- 3406.65\n",
      "Episode length: 3.20 +/- 2.98\n",
      "Eval num_timesteps=71000, episode_reward=-1045.58 +/- 2480.87\n",
      "Episode length: 3.45 +/- 2.94\n",
      "Eval num_timesteps=72000, episode_reward=-5408.14 +/- 4371.16\n",
      "Episode length: 7.20 +/- 4.23\n",
      "Eval num_timesteps=73000, episode_reward=-2731.22 +/- 4086.93\n",
      "Episode length: 4.55 +/- 4.04\n",
      "Eval num_timesteps=74000, episode_reward=-81.44 +/- 186.42\n",
      "Episode length: 2.40 +/- 1.53\n",
      "Eval num_timesteps=75000, episode_reward=-2269.44 +/- 3733.59\n",
      "Episode length: 4.15 +/- 3.50\n",
      "Eval num_timesteps=76000, episode_reward=-1714.26 +/- 3388.68\n",
      "Episode length: 2.65 +/- 2.52\n",
      "Eval num_timesteps=77000, episode_reward=-587.94 +/- 1818.13\n",
      "Episode length: 3.20 +/- 2.54\n",
      "Eval num_timesteps=78000, episode_reward=-1397.66 +/- 3236.49\n",
      "Episode length: 3.70 +/- 3.55\n",
      "Eval num_timesteps=79000, episode_reward=-1446.00 +/- 3180.87\n",
      "Episode length: 3.60 +/- 3.46\n",
      "Eval num_timesteps=80000, episode_reward=-973.45 +/- 2608.74\n",
      "Episode length: 3.15 +/- 2.85\n",
      "Eval num_timesteps=81000, episode_reward=-916.01 +/- 2537.72\n",
      "Episode length: 2.05 +/- 2.01\n",
      "Eval num_timesteps=82000, episode_reward=-978.53 +/- 2594.96\n",
      "Episode length: 3.50 +/- 2.77\n",
      "Eval num_timesteps=83000, episode_reward=-515.30 +/- 1785.91\n",
      "Episode length: 2.80 +/- 2.20\n",
      "Eval num_timesteps=84000, episode_reward=-1385.17 +/- 3233.51\n",
      "Episode length: 3.30 +/- 3.33\n",
      "Eval num_timesteps=85000, episode_reward=-923.10 +/- 2714.21\n",
      "Episode length: 2.65 +/- 2.90\n",
      "Eval num_timesteps=86000, episode_reward=-954.18 +/- 2584.79\n",
      "Episode length: 2.75 +/- 2.26\n",
      "Eval num_timesteps=87000, episode_reward=-929.62 +/- 2521.10\n",
      "Episode length: 2.35 +/- 2.33\n",
      "Eval num_timesteps=88000, episode_reward=-1312.48 +/- 2984.23\n",
      "Episode length: 2.85 +/- 1.56\n",
      "Eval num_timesteps=89000, episode_reward=-493.23 +/- 1760.32\n",
      "Episode length: 2.65 +/- 1.90\n",
      "Eval num_timesteps=90000, episode_reward=-925.35 +/- 2588.50\n",
      "Episode length: 2.55 +/- 2.62\n",
      "Eval num_timesteps=91000, episode_reward=-33.35 +/- 135.69\n",
      "Episode length: 1.85 +/- 1.01\n",
      "New best mean reward!\n",
      "Eval num_timesteps=92000, episode_reward=-13.39 +/- 118.23\n",
      "Episode length: 2.10 +/- 0.94\n",
      "New best mean reward!\n",
      "Eval num_timesteps=93000, episode_reward=-49.74 +/- 180.69\n",
      "Episode length: 2.15 +/- 1.46\n",
      "Eval num_timesteps=94000, episode_reward=-91.20 +/- 182.94\n",
      "Episode length: 2.30 +/- 2.33\n",
      "Eval num_timesteps=95000, episode_reward=-43.02 +/- 102.19\n",
      "Episode length: 1.80 +/- 0.68\n",
      "Eval num_timesteps=96000, episode_reward=-436.08 +/- 1828.50\n",
      "Episode length: 2.55 +/- 1.28\n",
      "Eval num_timesteps=97000, episode_reward=-459.88 +/- 1933.89\n",
      "Episode length: 2.30 +/- 2.24\n",
      "Eval num_timesteps=98000, episode_reward=-62.36 +/- 187.67\n",
      "Episode length: 2.35 +/- 1.35\n",
      "Eval num_timesteps=99000, episode_reward=-10.55 +/- 122.07\n",
      "Episode length: 1.80 +/- 0.93\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=39.31 +/- 150.43\n",
      "Episode length: 1.70 +/- 0.90\n",
      "New best mean reward!\n",
      "Eval num_timesteps=101000, episode_reward=-5.12 +/- 160.95\n",
      "Episode length: 2.50 +/- 1.36\n",
      "Eval num_timesteps=102000, episode_reward=-31.00 +/- 117.05\n",
      "Episode length: 1.85 +/- 0.79\n",
      "Eval num_timesteps=103000, episode_reward=-387.35 +/- 1803.41\n",
      "Episode length: 2.05 +/- 0.97\n",
      "Eval num_timesteps=104000, episode_reward=-98.26 +/- 173.32\n",
      "Episode length: 3.00 +/- 2.63\n",
      "Eval num_timesteps=105000, episode_reward=-854.55 +/- 2434.06\n",
      "Episode length: 2.15 +/- 1.19\n",
      "Eval num_timesteps=106000, episode_reward=-528.34 +/- 1952.62\n",
      "Episode length: 2.80 +/- 2.42\n",
      "Eval num_timesteps=107000, episode_reward=23.54 +/- 121.18\n",
      "Episode length: 1.75 +/- 0.94\n",
      "Eval num_timesteps=108000, episode_reward=-4.98 +/- 129.29\n",
      "Episode length: 2.10 +/- 1.14\n",
      "Eval num_timesteps=109000, episode_reward=-843.48 +/- 2419.82\n",
      "Episode length: 2.20 +/- 1.03\n",
      "Eval num_timesteps=110000, episode_reward=48.60 +/- 125.30\n",
      "Episode length: 1.65 +/- 0.65\n",
      "New best mean reward!\n",
      "Eval num_timesteps=111000, episode_reward=-61.10 +/- 69.98\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=112000, episode_reward=-477.10 +/- 1987.69\n",
      "Episode length: 2.85 +/- 2.39\n",
      "Eval num_timesteps=113000, episode_reward=-62.31 +/- 230.36\n",
      "Episode length: 2.30 +/- 1.76\n",
      "Eval num_timesteps=114000, episode_reward=-30.51 +/- 190.48\n",
      "Episode length: 2.20 +/- 1.08\n",
      "Eval num_timesteps=115000, episode_reward=-35.51 +/- 150.49\n",
      "Episode length: 2.20 +/- 1.25\n",
      "Eval num_timesteps=116000, episode_reward=-38.34 +/- 165.41\n",
      "Episode length: 2.35 +/- 1.31\n",
      "Eval num_timesteps=117000, episode_reward=9.21 +/- 119.18\n",
      "Episode length: 2.05 +/- 0.80\n",
      "Eval num_timesteps=118000, episode_reward=-59.96 +/- 144.04\n",
      "Episode length: 2.10 +/- 1.37\n",
      "Eval num_timesteps=119000, episode_reward=-456.32 +/- 1971.26\n",
      "Episode length: 2.50 +/- 2.13\n",
      "Eval num_timesteps=120000, episode_reward=0.16 +/- 108.02\n",
      "Episode length: 2.00 +/- 1.05\n",
      "Eval num_timesteps=121000, episode_reward=-883.01 +/- 2453.84\n",
      "Episode length: 2.30 +/- 2.10\n",
      "Eval num_timesteps=122000, episode_reward=-438.85 +/- 1784.18\n",
      "Episode length: 2.00 +/- 1.05\n",
      "Eval num_timesteps=123000, episode_reward=-427.06 +/- 1788.58\n",
      "Episode length: 2.05 +/- 1.16\n",
      "Eval num_timesteps=124000, episode_reward=-2.58 +/- 167.37\n",
      "Episode length: 2.25 +/- 1.26\n",
      "Eval num_timesteps=125000, episode_reward=19.35 +/- 169.75\n",
      "Episode length: 2.10 +/- 1.09\n",
      "Eval num_timesteps=126000, episode_reward=-14.25 +/- 153.64\n",
      "Episode length: 1.95 +/- 0.97\n",
      "Eval num_timesteps=127000, episode_reward=-30.14 +/- 130.04\n",
      "Episode length: 1.90 +/- 1.14\n",
      "Eval num_timesteps=128000, episode_reward=-15.63 +/- 161.35\n",
      "Episode length: 2.40 +/- 1.20\n",
      "Eval num_timesteps=129000, episode_reward=-417.59 +/- 1803.18\n",
      "Episode length: 1.95 +/- 1.12\n",
      "Eval num_timesteps=130000, episode_reward=-499.27 +/- 1919.62\n",
      "Episode length: 3.45 +/- 2.85\n",
      "Eval num_timesteps=131000, episode_reward=-850.13 +/- 2678.74\n",
      "Episode length: 2.90 +/- 2.81\n",
      "Eval num_timesteps=132000, episode_reward=-411.48 +/- 1805.67\n",
      "Episode length: 2.20 +/- 1.21\n",
      "Eval num_timesteps=133000, episode_reward=-33.37 +/- 149.42\n",
      "Episode length: 1.95 +/- 0.92\n",
      "Eval num_timesteps=134000, episode_reward=14.95 +/- 148.56\n",
      "Episode length: 2.20 +/- 1.12\n",
      "Eval num_timesteps=135000, episode_reward=45.11 +/- 139.21\n",
      "Episode length: 2.40 +/- 1.11\n",
      "Eval num_timesteps=136000, episode_reward=6.31 +/- 83.02\n",
      "Episode length: 1.75 +/- 1.26\n",
      "Eval num_timesteps=137000, episode_reward=-921.23 +/- 2730.05\n",
      "Episode length: 2.60 +/- 2.91\n",
      "Eval num_timesteps=138000, episode_reward=-10.40 +/- 114.24\n",
      "Episode length: 1.60 +/- 0.97\n",
      "Eval num_timesteps=139000, episode_reward=56.20 +/- 110.04\n",
      "Episode length: 1.75 +/- 0.83\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=46.22 +/- 97.97\n",
      "Episode length: 1.45 +/- 0.74\n",
      "Eval num_timesteps=141000, episode_reward=-25.02 +/- 126.71\n",
      "Episode length: 2.10 +/- 1.04\n",
      "Eval num_timesteps=142000, episode_reward=7.72 +/- 160.55\n",
      "Episode length: 2.20 +/- 1.17\n",
      "Eval num_timesteps=143000, episode_reward=-423.52 +/- 1791.37\n",
      "Episode length: 1.85 +/- 0.96\n",
      "Eval num_timesteps=144000, episode_reward=-66.29 +/- 231.89\n",
      "Episode length: 2.05 +/- 1.99\n",
      "Eval num_timesteps=145000, episode_reward=19.86 +/- 110.65\n",
      "Episode length: 2.05 +/- 1.24\n",
      "Eval num_timesteps=146000, episode_reward=-859.41 +/- 2679.53\n",
      "Episode length: 2.65 +/- 2.94\n",
      "Eval num_timesteps=147000, episode_reward=-40.41 +/- 129.65\n",
      "Episode length: 1.75 +/- 0.94\n",
      "Eval num_timesteps=148000, episode_reward=0.63 +/- 102.08\n",
      "Episode length: 1.65 +/- 0.85\n",
      "Eval num_timesteps=149000, episode_reward=17.47 +/- 161.89\n",
      "Episode length: 2.85 +/- 1.56\n",
      "Eval num_timesteps=150000, episode_reward=-801.18 +/- 2411.85\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=151000, episode_reward=34.57 +/- 151.58\n",
      "Episode length: 2.70 +/- 1.23\n",
      "Eval num_timesteps=152000, episode_reward=-3.58 +/- 172.12\n",
      "Episode length: 2.25 +/- 1.13\n",
      "Eval num_timesteps=153000, episode_reward=70.78 +/- 116.47\n",
      "Episode length: 1.75 +/- 0.89\n",
      "New best mean reward!\n",
      "Eval num_timesteps=154000, episode_reward=3.18 +/- 98.48\n",
      "Episode length: 1.70 +/- 1.10\n",
      "Eval num_timesteps=155000, episode_reward=0.70 +/- 138.35\n",
      "Episode length: 2.10 +/- 0.83\n",
      "Eval num_timesteps=156000, episode_reward=-31.34 +/- 164.30\n",
      "Episode length: 2.05 +/- 1.07\n",
      "Eval num_timesteps=157000, episode_reward=61.88 +/- 107.12\n",
      "Episode length: 1.65 +/- 0.65\n",
      "Eval num_timesteps=158000, episode_reward=-1.58 +/- 139.85\n",
      "Episode length: 1.90 +/- 0.89\n",
      "Eval num_timesteps=159000, episode_reward=-17.36 +/- 123.21\n",
      "Episode length: 1.95 +/- 0.97\n",
      "Eval num_timesteps=160000, episode_reward=12.70 +/- 117.75\n",
      "Episode length: 2.15 +/- 1.01\n",
      "Eval num_timesteps=161000, episode_reward=38.60 +/- 98.50\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=162000, episode_reward=60.79 +/- 127.96\n",
      "Episode length: 1.90 +/- 0.94\n",
      "Eval num_timesteps=163000, episode_reward=-11.07 +/- 143.92\n",
      "Episode length: 2.10 +/- 1.09\n",
      "Eval num_timesteps=164000, episode_reward=40.63 +/- 123.18\n",
      "Episode length: 2.20 +/- 0.93\n",
      "Eval num_timesteps=165000, episode_reward=11.10 +/- 120.01\n",
      "Episode length: 1.75 +/- 0.77\n",
      "Eval num_timesteps=166000, episode_reward=32.04 +/- 109.37\n",
      "Episode length: 1.70 +/- 0.84\n",
      "Eval num_timesteps=167000, episode_reward=-32.84 +/- 188.28\n",
      "Episode length: 2.30 +/- 1.45\n",
      "Eval num_timesteps=168000, episode_reward=80.94 +/- 137.64\n",
      "Episode length: 1.80 +/- 0.75\n",
      "New best mean reward!\n",
      "Eval num_timesteps=169000, episode_reward=-25.58 +/- 132.72\n",
      "Episode length: 1.90 +/- 0.94\n",
      "Eval num_timesteps=170000, episode_reward=88.64 +/- 140.67\n",
      "Episode length: 2.05 +/- 0.92\n",
      "New best mean reward!\n",
      "Eval num_timesteps=171000, episode_reward=-1.12 +/- 136.55\n",
      "Episode length: 1.50 +/- 0.81\n",
      "Eval num_timesteps=172000, episode_reward=39.06 +/- 126.57\n",
      "Episode length: 2.00 +/- 0.89\n",
      "Eval num_timesteps=173000, episode_reward=79.97 +/- 115.43\n",
      "Episode length: 2.10 +/- 0.89\n",
      "Eval num_timesteps=174000, episode_reward=18.18 +/- 96.62\n",
      "Episode length: 1.60 +/- 0.92\n",
      "Eval num_timesteps=175000, episode_reward=20.30 +/- 102.60\n",
      "Episode length: 1.95 +/- 0.92\n",
      "Eval num_timesteps=176000, episode_reward=-22.32 +/- 152.73\n",
      "Episode length: 2.40 +/- 1.28\n",
      "Eval num_timesteps=177000, episode_reward=-11.53 +/- 159.46\n",
      "Episode length: 1.85 +/- 0.96\n",
      "Eval num_timesteps=178000, episode_reward=-27.09 +/- 145.76\n",
      "Episode length: 1.75 +/- 0.83\n",
      "Eval num_timesteps=179000, episode_reward=52.06 +/- 108.85\n",
      "Episode length: 2.25 +/- 0.83\n",
      "Eval num_timesteps=180000, episode_reward=17.68 +/- 129.26\n",
      "Episode length: 1.95 +/- 0.97\n",
      "Eval num_timesteps=181000, episode_reward=-13.16 +/- 126.33\n",
      "Episode length: 2.10 +/- 1.09\n",
      "Eval num_timesteps=182000, episode_reward=30.34 +/- 92.36\n",
      "Episode length: 2.25 +/- 1.09\n",
      "Eval num_timesteps=183000, episode_reward=21.65 +/- 134.22\n",
      "Episode length: 2.05 +/- 0.92\n",
      "Eval num_timesteps=184000, episode_reward=52.22 +/- 113.52\n",
      "Episode length: 1.85 +/- 1.01\n",
      "Eval num_timesteps=185000, episode_reward=48.02 +/- 95.09\n",
      "Episode length: 1.60 +/- 0.66\n",
      "Eval num_timesteps=186000, episode_reward=28.25 +/- 114.90\n",
      "Episode length: 2.10 +/- 1.14\n",
      "Eval num_timesteps=187000, episode_reward=27.92 +/- 124.90\n",
      "Episode length: 2.20 +/- 0.87\n",
      "Eval num_timesteps=188000, episode_reward=44.04 +/- 120.30\n",
      "Episode length: 2.05 +/- 0.80\n",
      "Eval num_timesteps=189000, episode_reward=14.49 +/- 133.62\n",
      "Episode length: 1.95 +/- 0.97\n",
      "Eval num_timesteps=190000, episode_reward=10.39 +/- 116.76\n",
      "Episode length: 2.25 +/- 0.83\n",
      "Eval num_timesteps=191000, episode_reward=-7.88 +/- 128.70\n",
      "Episode length: 2.00 +/- 0.89\n",
      "Eval num_timesteps=192000, episode_reward=3.44 +/- 131.09\n",
      "Episode length: 1.80 +/- 0.87\n",
      "Eval num_timesteps=193000, episode_reward=1.76 +/- 157.96\n",
      "Episode length: 1.90 +/- 0.94\n",
      "Eval num_timesteps=194000, episode_reward=16.20 +/- 114.27\n",
      "Episode length: 1.75 +/- 0.77\n",
      "Eval num_timesteps=195000, episode_reward=-382.69 +/- 1960.32\n",
      "Episode length: 2.75 +/- 2.19\n",
      "Eval num_timesteps=196000, episode_reward=9.38 +/- 161.94\n",
      "Episode length: 2.25 +/- 1.09\n",
      "Eval num_timesteps=197000, episode_reward=37.66 +/- 97.37\n",
      "Episode length: 1.80 +/- 1.08\n",
      "Eval num_timesteps=198000, episode_reward=61.00 +/- 100.50\n",
      "Episode length: 2.25 +/- 1.04\n",
      "Eval num_timesteps=199000, episode_reward=43.39 +/- 137.97\n",
      "Episode length: 2.00 +/- 0.77\n",
      "Eval num_timesteps=200000, episode_reward=14.96 +/- 103.80\n",
      "Episode length: 1.90 +/- 0.99\n"
     ]
    }
   ],
   "source": [
    "model.learn(\n",
    "    total_timesteps=200000,  # 继续训练200,000步\n",
    "    tb_log_name=log_name,\n",
    "    callback=eval_callback\n",
    ")\n",
    "# 保存重新训练的模型\n",
    "model.save(\"model/dqn_retrained_electric_vehicle1\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-12T06:51:31.056838400Z",
     "start_time": "2024-09-12T06:11:02.389506300Z"
    }
   },
   "id": "7819b74b90b3ddf"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=-28.24 +/- 119.18\n",
      "Episode length: 2.55 +/- 1.40\n",
      "Eval num_timesteps=2000, episode_reward=-9.53 +/- 103.10\n",
      "Episode length: 2.00 +/- 1.22\n",
      "Eval num_timesteps=3000, episode_reward=-436.16 +/- 1950.48\n",
      "Episode length: 2.95 +/- 2.09\n",
      "Eval num_timesteps=4000, episode_reward=-421.24 +/- 1791.48\n",
      "Episode length: 1.90 +/- 0.99\n",
      "Eval num_timesteps=5000, episode_reward=-459.08 +/- 1944.34\n",
      "Episode length: 2.40 +/- 2.22\n",
      "Eval num_timesteps=6000, episode_reward=-398.62 +/- 1797.50\n",
      "Episode length: 2.00 +/- 1.14\n",
      "Eval num_timesteps=7000, episode_reward=20.79 +/- 133.35\n",
      "Episode length: 2.20 +/- 0.98\n",
      "Eval num_timesteps=8000, episode_reward=-353.87 +/- 1782.82\n",
      "Episode length: 2.25 +/- 1.09\n",
      "Eval num_timesteps=9000, episode_reward=-401.71 +/- 1851.86\n",
      "Episode length: 2.65 +/- 1.59\n",
      "Eval num_timesteps=10000, episode_reward=-378.46 +/- 1778.20\n",
      "Episode length: 2.45 +/- 1.20\n",
      "Eval num_timesteps=11000, episode_reward=40.81 +/- 114.22\n",
      "Episode length: 2.35 +/- 1.11\n",
      "Eval num_timesteps=12000, episode_reward=-351.14 +/- 1780.21\n",
      "Episode length: 1.80 +/- 1.08\n",
      "Eval num_timesteps=13000, episode_reward=-4.62 +/- 146.42\n",
      "Episode length: 2.00 +/- 1.22\n",
      "Eval num_timesteps=14000, episode_reward=-45.78 +/- 134.03\n",
      "Episode length: 2.60 +/- 0.97\n",
      "Eval num_timesteps=15000, episode_reward=-781.53 +/- 2461.90\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Eval num_timesteps=16000, episode_reward=13.51 +/- 137.13\n",
      "Episode length: 2.55 +/- 0.67\n",
      "Eval num_timesteps=17000, episode_reward=-11.20 +/- 97.76\n",
      "Episode length: 1.95 +/- 1.07\n",
      "Eval num_timesteps=18000, episode_reward=-12.51 +/- 200.49\n",
      "Episode length: 2.40 +/- 1.59\n",
      "Eval num_timesteps=19000, episode_reward=-459.01 +/- 1959.37\n",
      "Episode length: 2.40 +/- 2.15\n",
      "Eval num_timesteps=20000, episode_reward=-1702.20 +/- 3226.92\n",
      "Episode length: 2.95 +/- 1.02\n",
      "Eval num_timesteps=21000, episode_reward=-424.70 +/- 1760.18\n",
      "Episode length: 2.05 +/- 1.24\n",
      "Eval num_timesteps=22000, episode_reward=15.16 +/- 120.26\n",
      "Episode length: 2.10 +/- 0.94\n",
      "Eval num_timesteps=23000, episode_reward=-769.77 +/- 2372.31\n",
      "Episode length: 2.20 +/- 1.03\n",
      "Eval num_timesteps=24000, episode_reward=6.09 +/- 100.50\n",
      "Episode length: 1.75 +/- 0.89\n",
      "Eval num_timesteps=25000, episode_reward=-398.48 +/- 1742.55\n",
      "Episode length: 2.05 +/- 1.12\n",
      "Eval num_timesteps=26000, episode_reward=-26.57 +/- 139.26\n",
      "Episode length: 2.40 +/- 1.39\n",
      "Eval num_timesteps=27000, episode_reward=-803.93 +/- 2394.88\n",
      "Episode length: 2.45 +/- 1.32\n",
      "Eval num_timesteps=28000, episode_reward=-18.80 +/- 166.80\n",
      "Episode length: 2.55 +/- 1.12\n",
      "Eval num_timesteps=29000, episode_reward=-11.06 +/- 102.49\n",
      "Episode length: 2.20 +/- 1.12\n",
      "Eval num_timesteps=30000, episode_reward=-459.59 +/- 1771.73\n",
      "Episode length: 2.70 +/- 1.38\n",
      "Eval num_timesteps=31000, episode_reward=-512.84 +/- 1755.39\n",
      "Episode length: 2.60 +/- 0.86\n",
      "Eval num_timesteps=32000, episode_reward=-1272.51 +/- 2889.27\n",
      "Episode length: 2.80 +/- 1.25\n",
      "Eval num_timesteps=33000, episode_reward=-1335.93 +/- 3009.02\n",
      "Episode length: 2.70 +/- 2.22\n",
      "Eval num_timesteps=34000, episode_reward=-2.37 +/- 176.37\n",
      "Episode length: 2.05 +/- 0.92\n",
      "Eval num_timesteps=35000, episode_reward=-815.77 +/- 2432.47\n",
      "Episode length: 2.45 +/- 1.07\n",
      "Eval num_timesteps=36000, episode_reward=-19.52 +/- 161.71\n",
      "Episode length: 2.45 +/- 1.36\n",
      "Eval num_timesteps=37000, episode_reward=-1658.99 +/- 3328.31\n",
      "Episode length: 2.45 +/- 2.13\n",
      "Eval num_timesteps=38000, episode_reward=-1277.12 +/- 2854.02\n",
      "Episode length: 2.80 +/- 1.40\n",
      "Eval num_timesteps=39000, episode_reward=-91.93 +/- 150.68\n",
      "Episode length: 2.85 +/- 1.28\n",
      "Eval num_timesteps=40000, episode_reward=-1769.49 +/- 3489.39\n",
      "Episode length: 3.65 +/- 2.85\n",
      "Eval num_timesteps=41000, episode_reward=-420.65 +/- 1764.17\n",
      "Episode length: 2.90 +/- 1.37\n",
      "Eval num_timesteps=42000, episode_reward=-1383.03 +/- 2992.75\n",
      "Episode length: 3.75 +/- 2.57\n",
      "Eval num_timesteps=43000, episode_reward=-852.64 +/- 2417.47\n",
      "Episode length: 2.70 +/- 1.93\n",
      "Eval num_timesteps=44000, episode_reward=-104.57 +/- 316.77\n",
      "Episode length: 3.25 +/- 2.75\n",
      "Eval num_timesteps=45000, episode_reward=-11.36 +/- 211.19\n",
      "Episode length: 2.70 +/- 1.31\n",
      "Eval num_timesteps=46000, episode_reward=-23.61 +/- 184.21\n",
      "Episode length: 2.75 +/- 1.30\n",
      "Eval num_timesteps=47000, episode_reward=-382.76 +/- 1960.90\n",
      "Episode length: 3.05 +/- 2.18\n",
      "Eval num_timesteps=48000, episode_reward=-2.77 +/- 133.47\n",
      "Episode length: 2.30 +/- 0.90\n",
      "Eval num_timesteps=49000, episode_reward=-836.31 +/- 2566.10\n",
      "Episode length: 3.05 +/- 2.31\n",
      "Eval num_timesteps=50000, episode_reward=-43.77 +/- 191.29\n",
      "Episode length: 2.60 +/- 1.16\n",
      "Eval num_timesteps=51000, episode_reward=-445.74 +/- 1847.01\n",
      "Episode length: 2.65 +/- 1.74\n",
      "Eval num_timesteps=52000, episode_reward=-468.79 +/- 1782.54\n",
      "Episode length: 2.60 +/- 1.59\n",
      "Eval num_timesteps=53000, episode_reward=-805.38 +/- 2473.22\n",
      "Episode length: 2.70 +/- 1.19\n",
      "Eval num_timesteps=54000, episode_reward=-870.04 +/- 2677.99\n",
      "Episode length: 3.15 +/- 2.90\n",
      "Eval num_timesteps=55000, episode_reward=42.88 +/- 157.57\n",
      "Episode length: 2.20 +/- 1.21\n",
      "Eval num_timesteps=56000, episode_reward=-4.01 +/- 130.28\n",
      "Episode length: 2.30 +/- 1.27\n",
      "Eval num_timesteps=57000, episode_reward=48.90 +/- 134.95\n",
      "Episode length: 2.10 +/- 1.09\n",
      "Eval num_timesteps=58000, episode_reward=-49.31 +/- 213.56\n",
      "Episode length: 2.70 +/- 1.90\n",
      "Eval num_timesteps=59000, episode_reward=-68.34 +/- 178.79\n",
      "Episode length: 3.25 +/- 1.58\n",
      "Eval num_timesteps=60000, episode_reward=-11.90 +/- 143.59\n",
      "Episode length: 2.30 +/- 1.19\n",
      "Eval num_timesteps=61000, episode_reward=-48.89 +/- 273.97\n",
      "Episode length: 3.05 +/- 2.31\n",
      "Eval num_timesteps=62000, episode_reward=42.92 +/- 122.64\n",
      "Episode length: 2.30 +/- 1.05\n",
      "Eval num_timesteps=63000, episode_reward=-797.22 +/- 2422.38\n",
      "Episode length: 2.05 +/- 0.92\n",
      "Eval num_timesteps=64000, episode_reward=-5.34 +/- 109.22\n",
      "Episode length: 1.95 +/- 1.36\n",
      "Eval num_timesteps=65000, episode_reward=28.14 +/- 113.73\n",
      "Episode length: 2.25 +/- 1.64\n",
      "Eval num_timesteps=66000, episode_reward=46.37 +/- 150.26\n",
      "Episode length: 2.10 +/- 0.99\n",
      "Eval num_timesteps=67000, episode_reward=-309.07 +/- 1802.85\n",
      "Episode length: 2.45 +/- 0.86\n",
      "Eval num_timesteps=68000, episode_reward=-360.41 +/- 1790.41\n",
      "Episode length: 2.35 +/- 1.28\n",
      "Eval num_timesteps=69000, episode_reward=-57.64 +/- 148.95\n",
      "Episode length: 3.05 +/- 1.50\n",
      "Eval num_timesteps=70000, episode_reward=-385.70 +/- 1804.67\n",
      "Episode length: 2.85 +/- 1.46\n",
      "Eval num_timesteps=71000, episode_reward=-416.15 +/- 1815.43\n",
      "Episode length: 2.30 +/- 1.55\n",
      "Eval num_timesteps=72000, episode_reward=-773.68 +/- 2460.80\n",
      "Episode length: 2.05 +/- 0.86\n",
      "Eval num_timesteps=73000, episode_reward=-50.87 +/- 148.56\n",
      "Episode length: 2.95 +/- 1.56\n",
      "Eval num_timesteps=74000, episode_reward=16.12 +/- 117.95\n",
      "Episode length: 2.20 +/- 1.12\n",
      "Eval num_timesteps=75000, episode_reward=-1.04 +/- 125.99\n",
      "Episode length: 2.20 +/- 0.98\n",
      "Eval num_timesteps=76000, episode_reward=-6.78 +/- 112.11\n",
      "Episode length: 2.30 +/- 1.31\n",
      "Eval num_timesteps=77000, episode_reward=7.71 +/- 155.11\n",
      "Episode length: 2.45 +/- 1.28\n",
      "Eval num_timesteps=78000, episode_reward=-458.88 +/- 1783.17\n",
      "Episode length: 2.35 +/- 1.74\n",
      "Eval num_timesteps=79000, episode_reward=82.66 +/- 183.97\n",
      "Episode length: 2.60 +/- 1.20\n",
      "Eval num_timesteps=80000, episode_reward=39.58 +/- 97.29\n",
      "Episode length: 2.15 +/- 1.11\n",
      "Eval num_timesteps=81000, episode_reward=25.21 +/- 143.09\n",
      "Episode length: 2.65 +/- 1.46\n",
      "Eval num_timesteps=82000, episode_reward=-813.55 +/- 2447.06\n",
      "Episode length: 3.10 +/- 1.73\n",
      "Eval num_timesteps=83000, episode_reward=-365.69 +/- 1776.79\n",
      "Episode length: 2.20 +/- 1.08\n",
      "Eval num_timesteps=84000, episode_reward=-341.66 +/- 1783.25\n",
      "Episode length: 2.45 +/- 1.07\n",
      "Eval num_timesteps=85000, episode_reward=40.07 +/- 103.06\n",
      "Episode length: 2.90 +/- 1.26\n",
      "Eval num_timesteps=86000, episode_reward=33.42 +/- 92.31\n",
      "Episode length: 2.00 +/- 0.89\n",
      "Eval num_timesteps=87000, episode_reward=-1222.06 +/- 2917.85\n",
      "Episode length: 2.20 +/- 0.81\n",
      "Eval num_timesteps=88000, episode_reward=1.90 +/- 141.05\n",
      "Episode length: 2.20 +/- 1.50\n",
      "Eval num_timesteps=89000, episode_reward=-385.51 +/- 1793.07\n",
      "Episode length: 2.45 +/- 0.80\n",
      "Eval num_timesteps=90000, episode_reward=-335.06 +/- 1781.30\n",
      "Episode length: 2.45 +/- 0.86\n",
      "Eval num_timesteps=91000, episode_reward=62.76 +/- 110.86\n",
      "Episode length: 2.35 +/- 0.73\n",
      "Eval num_timesteps=92000, episode_reward=20.26 +/- 132.48\n",
      "Episode length: 2.50 +/- 1.02\n",
      "Eval num_timesteps=93000, episode_reward=-353.26 +/- 1816.24\n",
      "Episode length: 2.25 +/- 1.30\n",
      "Eval num_timesteps=94000, episode_reward=103.46 +/- 122.00\n",
      "Episode length: 2.50 +/- 0.74\n",
      "New best mean reward!\n",
      "Eval num_timesteps=95000, episode_reward=61.49 +/- 141.28\n",
      "Episode length: 2.20 +/- 1.21\n",
      "Eval num_timesteps=96000, episode_reward=53.11 +/- 84.80\n",
      "Episode length: 2.40 +/- 1.07\n",
      "Eval num_timesteps=97000, episode_reward=60.37 +/- 131.03\n",
      "Episode length: 2.25 +/- 0.89\n",
      "Eval num_timesteps=98000, episode_reward=39.18 +/- 157.39\n",
      "Episode length: 2.20 +/- 0.98\n",
      "Eval num_timesteps=99000, episode_reward=47.44 +/- 102.22\n",
      "Episode length: 2.00 +/- 1.00\n",
      "Eval num_timesteps=100000, episode_reward=-427.47 +/- 1995.93\n",
      "Episode length: 2.40 +/- 2.08\n",
      "Eval num_timesteps=101000, episode_reward=-5.33 +/- 111.88\n",
      "Episode length: 2.35 +/- 0.91\n",
      "Eval num_timesteps=102000, episode_reward=-1.14 +/- 164.95\n",
      "Episode length: 3.30 +/- 1.76\n",
      "Eval num_timesteps=103000, episode_reward=39.57 +/- 147.42\n",
      "Episode length: 2.50 +/- 1.12\n",
      "Eval num_timesteps=104000, episode_reward=2.23 +/- 125.05\n",
      "Episode length: 1.85 +/- 0.79\n",
      "Eval num_timesteps=105000, episode_reward=64.31 +/- 115.01\n",
      "Episode length: 2.25 +/- 0.99\n",
      "Eval num_timesteps=106000, episode_reward=30.72 +/- 137.20\n",
      "Episode length: 2.40 +/- 0.92\n",
      "Eval num_timesteps=107000, episode_reward=108.96 +/- 131.69\n",
      "Episode length: 1.95 +/- 0.74\n",
      "New best mean reward!\n",
      "Eval num_timesteps=108000, episode_reward=53.92 +/- 122.25\n",
      "Episode length: 2.40 +/- 1.28\n",
      "Eval num_timesteps=109000, episode_reward=-349.68 +/- 1752.01\n",
      "Episode length: 2.55 +/- 0.86\n",
      "Eval num_timesteps=110000, episode_reward=60.42 +/- 137.59\n",
      "Episode length: 2.35 +/- 1.15\n",
      "Eval num_timesteps=111000, episode_reward=49.84 +/- 166.72\n",
      "Episode length: 2.50 +/- 1.07\n",
      "Eval num_timesteps=112000, episode_reward=88.99 +/- 102.82\n",
      "Episode length: 2.15 +/- 0.65\n",
      "Eval num_timesteps=113000, episode_reward=58.85 +/- 141.66\n",
      "Episode length: 2.20 +/- 0.87\n",
      "Eval num_timesteps=114000, episode_reward=85.03 +/- 112.69\n",
      "Episode length: 2.20 +/- 0.93\n",
      "Eval num_timesteps=115000, episode_reward=88.97 +/- 103.20\n",
      "Episode length: 2.40 +/- 0.97\n",
      "Eval num_timesteps=116000, episode_reward=42.04 +/- 109.61\n",
      "Episode length: 1.95 +/- 0.92\n",
      "Eval num_timesteps=117000, episode_reward=62.86 +/- 151.77\n",
      "Episode length: 2.40 +/- 1.28\n",
      "Eval num_timesteps=118000, episode_reward=45.07 +/- 94.90\n",
      "Episode length: 2.00 +/- 0.71\n",
      "Eval num_timesteps=119000, episode_reward=69.03 +/- 138.14\n",
      "Episode length: 2.15 +/- 0.96\n",
      "Eval num_timesteps=120000, episode_reward=-393.63 +/- 1755.65\n",
      "Episode length: 2.30 +/- 1.00\n",
      "Eval num_timesteps=121000, episode_reward=20.41 +/- 115.71\n",
      "Episode length: 1.85 +/- 1.15\n",
      "Eval num_timesteps=122000, episode_reward=-353.31 +/- 1785.54\n",
      "Episode length: 2.80 +/- 0.93\n",
      "Eval num_timesteps=123000, episode_reward=80.75 +/- 101.13\n",
      "Episode length: 2.30 +/- 0.95\n",
      "Eval num_timesteps=124000, episode_reward=-16.53 +/- 155.37\n",
      "Episode length: 2.75 +/- 1.26\n",
      "Eval num_timesteps=125000, episode_reward=46.96 +/- 133.72\n",
      "Episode length: 3.05 +/- 0.86\n",
      "Eval num_timesteps=126000, episode_reward=34.79 +/- 136.97\n",
      "Episode length: 2.40 +/- 0.92\n",
      "Eval num_timesteps=127000, episode_reward=69.71 +/- 103.39\n",
      "Episode length: 2.05 +/- 0.80\n",
      "Eval num_timesteps=128000, episode_reward=22.87 +/- 171.62\n",
      "Episode length: 2.55 +/- 0.86\n",
      "Eval num_timesteps=129000, episode_reward=35.41 +/- 105.22\n",
      "Episode length: 2.50 +/- 1.12\n",
      "Eval num_timesteps=130000, episode_reward=-1204.81 +/- 2990.94\n",
      "Episode length: 3.35 +/- 2.01\n",
      "Eval num_timesteps=131000, episode_reward=-1206.88 +/- 3043.77\n",
      "Episode length: 2.70 +/- 2.22\n",
      "Eval num_timesteps=132000, episode_reward=-326.93 +/- 1783.76\n",
      "Episode length: 2.15 +/- 0.91\n",
      "Eval num_timesteps=133000, episode_reward=78.82 +/- 145.32\n",
      "Episode length: 2.50 +/- 0.81\n",
      "Eval num_timesteps=134000, episode_reward=-334.22 +/- 1807.49\n",
      "Episode length: 2.15 +/- 0.85\n",
      "Eval num_timesteps=135000, episode_reward=60.73 +/- 108.13\n",
      "Episode length: 2.45 +/- 1.12\n",
      "Eval num_timesteps=136000, episode_reward=40.85 +/- 145.64\n",
      "Episode length: 2.60 +/- 1.20\n",
      "Eval num_timesteps=137000, episode_reward=-751.67 +/- 2417.76\n",
      "Episode length: 2.70 +/- 0.84\n",
      "Eval num_timesteps=138000, episode_reward=87.64 +/- 108.89\n",
      "Episode length: 2.60 +/- 0.92\n",
      "Eval num_timesteps=139000, episode_reward=90.83 +/- 101.05\n",
      "Episode length: 2.40 +/- 0.73\n",
      "Eval num_timesteps=140000, episode_reward=58.86 +/- 135.07\n",
      "Episode length: 2.50 +/- 1.12\n",
      "Eval num_timesteps=141000, episode_reward=65.13 +/- 139.39\n",
      "Episode length: 2.25 +/- 1.18\n",
      "Eval num_timesteps=142000, episode_reward=-356.08 +/- 1756.12\n",
      "Episode length: 2.20 +/- 1.03\n",
      "Eval num_timesteps=143000, episode_reward=120.04 +/- 113.36\n",
      "Episode length: 2.50 +/- 0.97\n",
      "New best mean reward!\n",
      "Eval num_timesteps=144000, episode_reward=53.71 +/- 94.46\n",
      "Episode length: 2.05 +/- 1.02\n",
      "Eval num_timesteps=145000, episode_reward=67.44 +/- 107.73\n",
      "Episode length: 1.85 +/- 0.85\n",
      "Eval num_timesteps=146000, episode_reward=-363.06 +/- 1976.15\n",
      "Episode length: 2.50 +/- 2.16\n",
      "Eval num_timesteps=147000, episode_reward=56.99 +/- 92.59\n",
      "Episode length: 2.55 +/- 0.97\n",
      "Eval num_timesteps=148000, episode_reward=-374.13 +/- 1780.99\n",
      "Episode length: 2.15 +/- 1.11\n",
      "Eval num_timesteps=149000, episode_reward=-338.06 +/- 1784.93\n",
      "Episode length: 2.00 +/- 0.89\n",
      "Eval num_timesteps=150000, episode_reward=-328.54 +/- 1775.81\n",
      "Episode length: 2.20 +/- 0.93\n",
      "Eval num_timesteps=151000, episode_reward=68.89 +/- 91.65\n",
      "Episode length: 2.50 +/- 1.07\n",
      "Eval num_timesteps=152000, episode_reward=-382.47 +/- 1762.12\n",
      "Episode length: 2.25 +/- 0.99\n",
      "Eval num_timesteps=153000, episode_reward=54.52 +/- 103.67\n",
      "Episode length: 2.20 +/- 0.98\n",
      "Eval num_timesteps=154000, episode_reward=39.60 +/- 91.00\n",
      "Episode length: 1.95 +/- 0.86\n",
      "Eval num_timesteps=155000, episode_reward=45.52 +/- 89.80\n",
      "Episode length: 2.05 +/- 1.02\n",
      "Eval num_timesteps=156000, episode_reward=66.31 +/- 94.09\n",
      "Episode length: 1.85 +/- 0.79\n",
      "Eval num_timesteps=157000, episode_reward=105.07 +/- 105.14\n",
      "Episode length: 2.30 +/- 0.90\n",
      "Eval num_timesteps=158000, episode_reward=80.71 +/- 111.27\n",
      "Episode length: 2.20 +/- 0.93\n",
      "Eval num_timesteps=159000, episode_reward=81.59 +/- 92.45\n",
      "Episode length: 2.05 +/- 0.67\n",
      "Eval num_timesteps=160000, episode_reward=25.56 +/- 124.40\n",
      "Episode length: 2.50 +/- 0.81\n",
      "Eval num_timesteps=161000, episode_reward=93.90 +/- 96.93\n",
      "Episode length: 2.30 +/- 0.95\n",
      "Eval num_timesteps=162000, episode_reward=65.13 +/- 128.24\n",
      "Episode length: 2.15 +/- 0.91\n",
      "Eval num_timesteps=163000, episode_reward=69.56 +/- 93.83\n",
      "Episode length: 2.45 +/- 0.80\n",
      "Eval num_timesteps=164000, episode_reward=76.43 +/- 95.39\n",
      "Episode length: 2.20 +/- 0.81\n",
      "Eval num_timesteps=165000, episode_reward=62.49 +/- 106.26\n",
      "Episode length: 2.60 +/- 1.07\n",
      "Eval num_timesteps=166000, episode_reward=41.14 +/- 129.15\n",
      "Episode length: 2.40 +/- 1.02\n",
      "Eval num_timesteps=167000, episode_reward=71.84 +/- 121.79\n",
      "Episode length: 2.35 +/- 1.11\n",
      "Eval num_timesteps=168000, episode_reward=62.19 +/- 121.89\n",
      "Episode length: 2.80 +/- 0.98\n",
      "Eval num_timesteps=169000, episode_reward=57.28 +/- 136.42\n",
      "Episode length: 2.50 +/- 1.32\n",
      "Eval num_timesteps=170000, episode_reward=85.33 +/- 77.61\n",
      "Episode length: 2.20 +/- 0.81\n",
      "Eval num_timesteps=171000, episode_reward=69.70 +/- 149.12\n",
      "Episode length: 2.85 +/- 0.91\n",
      "Eval num_timesteps=172000, episode_reward=90.62 +/- 100.92\n",
      "Episode length: 2.30 +/- 0.95\n",
      "Eval num_timesteps=173000, episode_reward=112.29 +/- 122.58\n",
      "Episode length: 2.45 +/- 0.97\n",
      "Eval num_timesteps=174000, episode_reward=40.89 +/- 92.73\n",
      "Episode length: 2.20 +/- 1.17\n",
      "Eval num_timesteps=175000, episode_reward=44.34 +/- 120.80\n",
      "Episode length: 3.10 +/- 1.37\n",
      "Eval num_timesteps=176000, episode_reward=48.14 +/- 129.27\n",
      "Episode length: 1.90 +/- 0.89\n",
      "Eval num_timesteps=177000, episode_reward=85.60 +/- 106.50\n",
      "Episode length: 2.50 +/- 1.20\n",
      "Eval num_timesteps=178000, episode_reward=40.60 +/- 139.35\n",
      "Episode length: 2.15 +/- 1.01\n",
      "Eval num_timesteps=179000, episode_reward=120.94 +/- 113.87\n",
      "Episode length: 2.50 +/- 0.81\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=75.10 +/- 97.18\n",
      "Episode length: 2.00 +/- 0.89\n",
      "Eval num_timesteps=181000, episode_reward=71.04 +/- 96.49\n",
      "Episode length: 2.15 +/- 0.96\n",
      "Eval num_timesteps=182000, episode_reward=-313.67 +/- 1829.29\n",
      "Episode length: 2.60 +/- 1.20\n",
      "Eval num_timesteps=183000, episode_reward=51.00 +/- 104.02\n",
      "Episode length: 2.30 +/- 1.05\n",
      "Eval num_timesteps=184000, episode_reward=63.74 +/- 105.97\n",
      "Episode length: 2.40 +/- 1.16\n",
      "Eval num_timesteps=185000, episode_reward=108.44 +/- 91.18\n",
      "Episode length: 2.45 +/- 1.07\n",
      "Eval num_timesteps=186000, episode_reward=52.41 +/- 97.00\n",
      "Episode length: 2.25 +/- 1.13\n",
      "Eval num_timesteps=187000, episode_reward=122.68 +/- 78.34\n",
      "Episode length: 2.20 +/- 0.81\n",
      "New best mean reward!\n",
      "Eval num_timesteps=188000, episode_reward=54.42 +/- 94.85\n",
      "Episode length: 2.30 +/- 1.00\n",
      "Eval num_timesteps=189000, episode_reward=82.29 +/- 105.78\n",
      "Episode length: 2.25 +/- 0.94\n",
      "Eval num_timesteps=190000, episode_reward=55.71 +/- 118.78\n",
      "Episode length: 2.15 +/- 0.91\n",
      "Eval num_timesteps=191000, episode_reward=79.08 +/- 93.90\n",
      "Episode length: 2.90 +/- 1.04\n",
      "Eval num_timesteps=192000, episode_reward=79.25 +/- 116.85\n",
      "Episode length: 2.65 +/- 0.96\n",
      "Eval num_timesteps=193000, episode_reward=110.11 +/- 133.43\n",
      "Episode length: 2.35 +/- 0.65\n",
      "Eval num_timesteps=194000, episode_reward=76.92 +/- 100.21\n",
      "Episode length: 2.70 +/- 1.00\n",
      "Eval num_timesteps=195000, episode_reward=-330.83 +/- 1763.20\n",
      "Episode length: 2.95 +/- 1.07\n",
      "Eval num_timesteps=196000, episode_reward=41.80 +/- 166.52\n",
      "Episode length: 2.85 +/- 1.28\n",
      "Eval num_timesteps=197000, episode_reward=110.50 +/- 74.23\n",
      "Episode length: 2.75 +/- 1.04\n",
      "Eval num_timesteps=198000, episode_reward=93.68 +/- 95.26\n",
      "Episode length: 2.50 +/- 0.97\n",
      "Eval num_timesteps=199000, episode_reward=90.11 +/- 93.51\n",
      "Episode length: 2.25 +/- 0.89\n",
      "Eval num_timesteps=200000, episode_reward=45.96 +/- 113.57\n",
      "Episode length: 2.85 +/- 0.73\n",
      "Eval num_timesteps=201000, episode_reward=57.68 +/- 120.46\n",
      "Episode length: 2.50 +/- 1.16\n",
      "Eval num_timesteps=202000, episode_reward=96.22 +/- 138.61\n",
      "Episode length: 2.35 +/- 1.15\n",
      "Eval num_timesteps=203000, episode_reward=71.95 +/- 119.39\n",
      "Episode length: 2.55 +/- 0.97\n",
      "Eval num_timesteps=204000, episode_reward=69.50 +/- 104.42\n",
      "Episode length: 2.40 +/- 0.86\n",
      "Eval num_timesteps=205000, episode_reward=69.96 +/- 78.49\n",
      "Episode length: 3.00 +/- 1.18\n",
      "Eval num_timesteps=206000, episode_reward=27.89 +/- 192.40\n",
      "Episode length: 2.75 +/- 1.64\n",
      "Eval num_timesteps=207000, episode_reward=118.51 +/- 92.67\n",
      "Episode length: 2.60 +/- 0.97\n",
      "Eval num_timesteps=208000, episode_reward=-328.16 +/- 1805.60\n",
      "Episode length: 2.50 +/- 0.97\n",
      "Eval num_timesteps=209000, episode_reward=78.11 +/- 110.27\n",
      "Episode length: 2.70 +/- 0.95\n",
      "Eval num_timesteps=210000, episode_reward=88.27 +/- 79.37\n",
      "Episode length: 2.55 +/- 0.92\n",
      "Eval num_timesteps=211000, episode_reward=58.52 +/- 95.15\n",
      "Episode length: 2.15 +/- 0.85\n",
      "Eval num_timesteps=212000, episode_reward=105.21 +/- 87.85\n",
      "Episode length: 2.55 +/- 0.80\n",
      "Eval num_timesteps=213000, episode_reward=98.69 +/- 86.16\n",
      "Episode length: 2.65 +/- 0.91\n",
      "Eval num_timesteps=214000, episode_reward=66.69 +/- 90.02\n",
      "Episode length: 2.40 +/- 0.86\n",
      "Eval num_timesteps=215000, episode_reward=90.33 +/- 102.85\n",
      "Episode length: 2.35 +/- 1.06\n",
      "Eval num_timesteps=216000, episode_reward=96.89 +/- 96.11\n",
      "Episode length: 2.65 +/- 0.85\n",
      "Eval num_timesteps=217000, episode_reward=109.25 +/- 107.35\n",
      "Episode length: 2.50 +/- 0.87\n",
      "Eval num_timesteps=218000, episode_reward=34.58 +/- 94.18\n",
      "Episode length: 2.00 +/- 0.95\n",
      "Eval num_timesteps=219000, episode_reward=107.33 +/- 75.14\n",
      "Episode length: 2.50 +/- 0.92\n",
      "Eval num_timesteps=220000, episode_reward=95.41 +/- 79.26\n",
      "Episode length: 2.40 +/- 0.86\n",
      "Eval num_timesteps=221000, episode_reward=112.85 +/- 94.77\n",
      "Episode length: 2.30 +/- 0.78\n",
      "Eval num_timesteps=222000, episode_reward=148.22 +/- 98.38\n",
      "Episode length: 2.25 +/- 0.83\n",
      "New best mean reward!\n",
      "Eval num_timesteps=223000, episode_reward=98.15 +/- 67.37\n",
      "Episode length: 2.40 +/- 0.86\n",
      "Eval num_timesteps=224000, episode_reward=86.35 +/- 83.52\n",
      "Episode length: 2.45 +/- 0.80\n",
      "Eval num_timesteps=225000, episode_reward=104.17 +/- 100.70\n",
      "Episode length: 2.45 +/- 0.86\n",
      "Eval num_timesteps=226000, episode_reward=60.40 +/- 98.69\n",
      "Episode length: 2.55 +/- 1.07\n",
      "Eval num_timesteps=227000, episode_reward=100.32 +/- 92.82\n",
      "Episode length: 2.50 +/- 0.92\n",
      "Eval num_timesteps=228000, episode_reward=39.05 +/- 99.49\n",
      "Episode length: 2.35 +/- 1.28\n",
      "Eval num_timesteps=229000, episode_reward=76.01 +/- 107.55\n",
      "Episode length: 2.30 +/- 0.95\n",
      "Eval num_timesteps=230000, episode_reward=-331.92 +/- 1805.02\n",
      "Episode length: 2.60 +/- 1.07\n",
      "Eval num_timesteps=231000, episode_reward=98.27 +/- 79.85\n",
      "Episode length: 2.35 +/- 0.91\n",
      "Eval num_timesteps=232000, episode_reward=58.69 +/- 70.80\n",
      "Episode length: 2.35 +/- 0.85\n",
      "Eval num_timesteps=233000, episode_reward=89.88 +/- 97.28\n",
      "Episode length: 2.40 +/- 0.80\n",
      "Eval num_timesteps=234000, episode_reward=94.82 +/- 89.58\n",
      "Episode length: 2.25 +/- 0.77\n",
      "Eval num_timesteps=235000, episode_reward=82.76 +/- 103.82\n",
      "Episode length: 2.20 +/- 0.93\n",
      "Eval num_timesteps=236000, episode_reward=99.15 +/- 82.02\n",
      "Episode length: 2.40 +/- 0.86\n",
      "Eval num_timesteps=237000, episode_reward=94.28 +/- 114.22\n",
      "Episode length: 2.20 +/- 1.08\n",
      "Eval num_timesteps=238000, episode_reward=62.93 +/- 92.98\n",
      "Episode length: 2.40 +/- 0.97\n",
      "Eval num_timesteps=239000, episode_reward=114.80 +/- 97.24\n",
      "Episode length: 2.40 +/- 0.80\n",
      "Eval num_timesteps=240000, episode_reward=111.21 +/- 82.07\n",
      "Episode length: 2.40 +/- 0.97\n",
      "Eval num_timesteps=241000, episode_reward=109.18 +/- 80.80\n",
      "Episode length: 2.65 +/- 1.06\n",
      "Eval num_timesteps=242000, episode_reward=-284.94 +/- 1812.49\n",
      "Episode length: 2.50 +/- 0.81\n",
      "Eval num_timesteps=243000, episode_reward=89.65 +/- 81.90\n",
      "Episode length: 2.40 +/- 1.16\n",
      "Eval num_timesteps=244000, episode_reward=74.78 +/- 75.93\n",
      "Episode length: 2.30 +/- 0.90\n",
      "Eval num_timesteps=245000, episode_reward=73.42 +/- 75.13\n",
      "Episode length: 2.45 +/- 1.02\n",
      "Eval num_timesteps=246000, episode_reward=115.83 +/- 93.95\n",
      "Episode length: 2.55 +/- 0.97\n",
      "Eval num_timesteps=247000, episode_reward=-313.89 +/- 1838.75\n",
      "Episode length: 3.10 +/- 0.94\n",
      "Eval num_timesteps=248000, episode_reward=55.03 +/- 101.60\n",
      "Episode length: 2.40 +/- 1.11\n",
      "Eval num_timesteps=249000, episode_reward=56.62 +/- 82.16\n",
      "Episode length: 2.65 +/- 0.85\n",
      "Eval num_timesteps=250000, episode_reward=30.93 +/- 93.47\n",
      "Episode length: 2.55 +/- 1.07\n",
      "Eval num_timesteps=251000, episode_reward=128.04 +/- 82.26\n",
      "Episode length: 2.40 +/- 1.02\n",
      "Eval num_timesteps=252000, episode_reward=87.97 +/- 103.44\n",
      "Episode length: 2.15 +/- 1.01\n",
      "Eval num_timesteps=253000, episode_reward=96.02 +/- 78.63\n",
      "Episode length: 2.45 +/- 0.80\n",
      "Eval num_timesteps=254000, episode_reward=64.06 +/- 100.17\n",
      "Episode length: 2.50 +/- 1.12\n",
      "Eval num_timesteps=255000, episode_reward=77.28 +/- 91.03\n",
      "Episode length: 2.70 +/- 0.95\n",
      "Eval num_timesteps=256000, episode_reward=90.52 +/- 105.47\n",
      "Episode length: 2.60 +/- 1.11\n",
      "Eval num_timesteps=257000, episode_reward=59.71 +/- 68.41\n",
      "Episode length: 2.80 +/- 1.12\n",
      "Eval num_timesteps=258000, episode_reward=98.76 +/- 103.32\n",
      "Episode length: 2.65 +/- 1.11\n",
      "Eval num_timesteps=259000, episode_reward=59.57 +/- 115.54\n",
      "Episode length: 2.60 +/- 0.66\n",
      "Eval num_timesteps=260000, episode_reward=20.97 +/- 112.39\n",
      "Episode length: 2.65 +/- 1.06\n",
      "Eval num_timesteps=261000, episode_reward=95.93 +/- 92.77\n",
      "Episode length: 2.55 +/- 0.80\n",
      "Eval num_timesteps=262000, episode_reward=88.81 +/- 104.74\n",
      "Episode length: 2.40 +/- 0.86\n",
      "Eval num_timesteps=263000, episode_reward=87.08 +/- 84.72\n",
      "Episode length: 2.55 +/- 1.12\n",
      "Eval num_timesteps=264000, episode_reward=122.90 +/- 91.03\n",
      "Episode length: 2.70 +/- 1.10\n",
      "Eval num_timesteps=265000, episode_reward=62.92 +/- 130.14\n",
      "Episode length: 3.05 +/- 1.56\n",
      "Eval num_timesteps=266000, episode_reward=108.51 +/- 79.31\n",
      "Episode length: 2.80 +/- 0.87\n",
      "Eval num_timesteps=267000, episode_reward=49.71 +/- 90.09\n",
      "Episode length: 2.40 +/- 1.02\n",
      "Eval num_timesteps=268000, episode_reward=94.32 +/- 86.39\n",
      "Episode length: 2.65 +/- 1.11\n",
      "Eval num_timesteps=269000, episode_reward=121.29 +/- 105.38\n",
      "Episode length: 2.55 +/- 0.74\n",
      "Eval num_timesteps=270000, episode_reward=58.86 +/- 94.18\n",
      "Episode length: 2.35 +/- 0.96\n",
      "Eval num_timesteps=271000, episode_reward=122.87 +/- 117.00\n",
      "Episode length: 2.45 +/- 0.74\n",
      "Eval num_timesteps=272000, episode_reward=66.89 +/- 160.91\n",
      "Episode length: 2.70 +/- 1.23\n",
      "Eval num_timesteps=273000, episode_reward=122.06 +/- 111.18\n",
      "Episode length: 2.15 +/- 0.79\n",
      "Eval num_timesteps=274000, episode_reward=-324.55 +/- 1780.89\n",
      "Episode length: 2.50 +/- 1.02\n",
      "Eval num_timesteps=275000, episode_reward=71.15 +/- 78.14\n",
      "Episode length: 2.15 +/- 1.01\n",
      "Eval num_timesteps=276000, episode_reward=136.47 +/- 115.39\n",
      "Episode length: 2.10 +/- 0.83\n",
      "Eval num_timesteps=277000, episode_reward=72.06 +/- 99.69\n",
      "Episode length: 2.90 +/- 0.89\n",
      "Eval num_timesteps=278000, episode_reward=113.78 +/- 89.38\n",
      "Episode length: 2.35 +/- 0.73\n",
      "Eval num_timesteps=279000, episode_reward=89.35 +/- 98.72\n",
      "Episode length: 2.65 +/- 0.96\n",
      "Eval num_timesteps=280000, episode_reward=109.38 +/- 93.25\n",
      "Episode length: 2.30 +/- 0.78\n",
      "Eval num_timesteps=281000, episode_reward=-306.23 +/- 1761.64\n",
      "Episode length: 2.35 +/- 0.85\n",
      "Eval num_timesteps=282000, episode_reward=110.03 +/- 82.99\n",
      "Episode length: 2.50 +/- 0.87\n",
      "Eval num_timesteps=283000, episode_reward=96.20 +/- 64.67\n",
      "Episode length: 2.70 +/- 0.95\n",
      "Eval num_timesteps=284000, episode_reward=89.16 +/- 104.66\n",
      "Episode length: 2.05 +/- 0.67\n",
      "Eval num_timesteps=285000, episode_reward=104.19 +/- 117.86\n",
      "Episode length: 2.30 +/- 0.78\n",
      "Eval num_timesteps=286000, episode_reward=-283.82 +/- 1776.39\n",
      "Episode length: 2.25 +/- 0.94\n",
      "Eval num_timesteps=287000, episode_reward=68.66 +/- 82.25\n",
      "Episode length: 2.80 +/- 1.17\n",
      "Eval num_timesteps=288000, episode_reward=88.02 +/- 109.03\n",
      "Episode length: 2.65 +/- 1.15\n",
      "Eval num_timesteps=289000, episode_reward=95.60 +/- 89.51\n",
      "Episode length: 2.60 +/- 0.92\n",
      "Eval num_timesteps=290000, episode_reward=130.85 +/- 88.78\n",
      "Episode length: 2.25 +/- 0.83\n",
      "Eval num_timesteps=291000, episode_reward=85.59 +/- 110.41\n",
      "Episode length: 2.45 +/- 0.80\n",
      "Eval num_timesteps=292000, episode_reward=107.76 +/- 82.91\n",
      "Episode length: 3.15 +/- 0.96\n",
      "Eval num_timesteps=293000, episode_reward=67.57 +/- 94.56\n",
      "Episode length: 2.35 +/- 1.01\n",
      "Eval num_timesteps=294000, episode_reward=110.46 +/- 91.81\n",
      "Episode length: 2.40 +/- 0.86\n",
      "Eval num_timesteps=295000, episode_reward=96.98 +/- 76.90\n",
      "Episode length: 2.70 +/- 0.95\n",
      "Eval num_timesteps=296000, episode_reward=-363.57 +/- 1831.54\n",
      "Episode length: 2.35 +/- 1.01\n",
      "Eval num_timesteps=297000, episode_reward=57.65 +/- 76.29\n",
      "Episode length: 2.30 +/- 1.00\n",
      "Eval num_timesteps=298000, episode_reward=113.47 +/- 78.12\n",
      "Episode length: 2.70 +/- 0.84\n",
      "Eval num_timesteps=299000, episode_reward=92.58 +/- 98.47\n",
      "Episode length: 2.65 +/- 0.79\n",
      "Eval num_timesteps=300000, episode_reward=111.01 +/- 88.65\n",
      "Episode length: 2.70 +/- 0.64\n",
      "Eval num_timesteps=301000, episode_reward=104.83 +/- 84.54\n",
      "Episode length: 3.00 +/- 0.84\n",
      "Eval num_timesteps=302000, episode_reward=92.12 +/- 93.69\n",
      "Episode length: 2.55 +/- 0.86\n",
      "Eval num_timesteps=303000, episode_reward=95.52 +/- 91.08\n",
      "Episode length: 2.35 +/- 0.79\n",
      "Eval num_timesteps=304000, episode_reward=91.36 +/- 71.27\n",
      "Episode length: 2.50 +/- 0.97\n",
      "Eval num_timesteps=305000, episode_reward=134.11 +/- 72.53\n",
      "Episode length: 2.80 +/- 0.87\n",
      "Eval num_timesteps=306000, episode_reward=92.25 +/- 108.52\n",
      "Episode length: 2.35 +/- 0.57\n",
      "Eval num_timesteps=307000, episode_reward=78.58 +/- 161.27\n",
      "Episode length: 3.05 +/- 1.02\n",
      "Eval num_timesteps=308000, episode_reward=98.11 +/- 114.67\n",
      "Episode length: 2.70 +/- 0.95\n",
      "Eval num_timesteps=309000, episode_reward=-340.56 +/- 1785.94\n",
      "Episode length: 2.40 +/- 0.86\n",
      "Eval num_timesteps=310000, episode_reward=69.26 +/- 85.31\n",
      "Episode length: 2.85 +/- 1.19\n",
      "Eval num_timesteps=311000, episode_reward=142.09 +/- 68.47\n",
      "Episode length: 2.50 +/- 0.87\n",
      "Eval num_timesteps=312000, episode_reward=56.14 +/- 91.18\n",
      "Episode length: 3.00 +/- 0.84\n",
      "Eval num_timesteps=313000, episode_reward=76.83 +/- 103.47\n",
      "Episode length: 2.50 +/- 1.07\n",
      "Eval num_timesteps=314000, episode_reward=80.00 +/- 95.58\n",
      "Episode length: 2.55 +/- 0.80\n",
      "Eval num_timesteps=315000, episode_reward=65.08 +/- 103.77\n",
      "Episode length: 2.45 +/- 0.97\n",
      "Eval num_timesteps=316000, episode_reward=56.74 +/- 109.13\n",
      "Episode length: 2.35 +/- 0.91\n",
      "Eval num_timesteps=317000, episode_reward=121.66 +/- 108.53\n",
      "Episode length: 2.70 +/- 0.95\n",
      "Eval num_timesteps=318000, episode_reward=42.46 +/- 154.63\n",
      "Episode length: 3.30 +/- 1.10\n",
      "Eval num_timesteps=319000, episode_reward=78.39 +/- 77.45\n",
      "Episode length: 2.55 +/- 0.80\n",
      "Eval num_timesteps=320000, episode_reward=97.42 +/- 93.30\n",
      "Episode length: 2.80 +/- 0.87\n",
      "Eval num_timesteps=321000, episode_reward=68.42 +/- 113.99\n",
      "Episode length: 2.70 +/- 1.14\n",
      "Eval num_timesteps=322000, episode_reward=78.96 +/- 81.06\n",
      "Episode length: 2.85 +/- 0.79\n",
      "Eval num_timesteps=323000, episode_reward=111.24 +/- 99.32\n",
      "Episode length: 2.50 +/- 1.02\n",
      "Eval num_timesteps=324000, episode_reward=73.93 +/- 100.03\n",
      "Episode length: 2.45 +/- 0.80\n",
      "Eval num_timesteps=325000, episode_reward=72.30 +/- 113.95\n",
      "Episode length: 3.25 +/- 1.04\n",
      "Eval num_timesteps=326000, episode_reward=-310.07 +/- 1768.09\n",
      "Episode length: 2.50 +/- 0.97\n",
      "Eval num_timesteps=327000, episode_reward=33.36 +/- 150.20\n",
      "Episode length: 3.35 +/- 1.65\n",
      "Eval num_timesteps=328000, episode_reward=97.80 +/- 92.69\n",
      "Episode length: 2.60 +/- 0.97\n",
      "Eval num_timesteps=329000, episode_reward=-1099.31 +/- 2909.30\n",
      "Episode length: 2.95 +/- 0.80\n",
      "Eval num_timesteps=330000, episode_reward=71.42 +/- 101.33\n",
      "Episode length: 2.85 +/- 1.11\n",
      "Eval num_timesteps=331000, episode_reward=94.57 +/- 106.45\n",
      "Episode length: 2.55 +/- 0.67\n",
      "Eval num_timesteps=332000, episode_reward=49.28 +/- 87.51\n",
      "Episode length: 2.40 +/- 1.11\n",
      "Eval num_timesteps=333000, episode_reward=81.04 +/- 105.75\n",
      "Episode length: 2.70 +/- 0.71\n",
      "Eval num_timesteps=334000, episode_reward=87.75 +/- 68.69\n",
      "Episode length: 2.60 +/- 0.92\n",
      "Eval num_timesteps=335000, episode_reward=81.86 +/- 82.89\n",
      "Episode length: 2.60 +/- 0.92\n",
      "Eval num_timesteps=336000, episode_reward=-1121.14 +/- 2896.31\n",
      "Episode length: 2.70 +/- 0.64\n",
      "Eval num_timesteps=337000, episode_reward=108.98 +/- 152.33\n",
      "Episode length: 2.65 +/- 1.53\n",
      "Eval num_timesteps=338000, episode_reward=63.85 +/- 104.48\n",
      "Episode length: 2.60 +/- 1.11\n",
      "Eval num_timesteps=339000, episode_reward=108.43 +/- 110.25\n",
      "Episode length: 2.65 +/- 1.01\n",
      "Eval num_timesteps=340000, episode_reward=91.70 +/- 88.86\n",
      "Episode length: 2.65 +/- 1.01\n",
      "Eval num_timesteps=341000, episode_reward=98.69 +/- 93.52\n",
      "Episode length: 2.30 +/- 0.84\n",
      "Eval num_timesteps=342000, episode_reward=134.32 +/- 74.97\n",
      "Episode length: 2.65 +/- 0.65\n",
      "Eval num_timesteps=343000, episode_reward=61.01 +/- 79.47\n",
      "Episode length: 2.25 +/- 1.13\n",
      "Eval num_timesteps=344000, episode_reward=121.67 +/- 76.44\n",
      "Episode length: 2.70 +/- 0.64\n",
      "Eval num_timesteps=345000, episode_reward=-312.77 +/- 1790.38\n",
      "Episode length: 2.85 +/- 0.91\n",
      "Eval num_timesteps=346000, episode_reward=-1156.85 +/- 2927.37\n",
      "Episode length: 2.20 +/- 1.03\n",
      "Eval num_timesteps=347000, episode_reward=103.17 +/- 100.99\n",
      "Episode length: 2.35 +/- 0.85\n",
      "Eval num_timesteps=348000, episode_reward=125.01 +/- 88.08\n",
      "Episode length: 2.70 +/- 0.71\n",
      "Eval num_timesteps=349000, episode_reward=66.59 +/- 177.80\n",
      "Episode length: 2.55 +/- 1.20\n",
      "Eval num_timesteps=350000, episode_reward=82.10 +/- 100.95\n",
      "Episode length: 2.20 +/- 0.68\n",
      "Eval num_timesteps=351000, episode_reward=137.76 +/- 90.99\n",
      "Episode length: 2.70 +/- 0.78\n",
      "Eval num_timesteps=352000, episode_reward=93.57 +/- 76.35\n",
      "Episode length: 2.60 +/- 0.86\n",
      "Eval num_timesteps=353000, episode_reward=63.56 +/- 93.83\n",
      "Episode length: 2.50 +/- 1.07\n",
      "Eval num_timesteps=354000, episode_reward=96.08 +/- 113.31\n",
      "Episode length: 2.65 +/- 0.91\n",
      "Eval num_timesteps=355000, episode_reward=91.32 +/- 66.49\n",
      "Episode length: 2.85 +/- 0.91\n",
      "Eval num_timesteps=356000, episode_reward=64.54 +/- 116.13\n",
      "Episode length: 2.65 +/- 1.01\n",
      "Eval num_timesteps=357000, episode_reward=100.67 +/- 100.77\n",
      "Episode length: 2.45 +/- 0.97\n",
      "Eval num_timesteps=358000, episode_reward=83.76 +/- 99.35\n",
      "Episode length: 3.05 +/- 0.86\n",
      "Eval num_timesteps=359000, episode_reward=99.64 +/- 51.32\n",
      "Episode length: 2.70 +/- 0.71\n",
      "Eval num_timesteps=360000, episode_reward=62.85 +/- 131.99\n",
      "Episode length: 2.75 +/- 0.70\n",
      "Eval num_timesteps=361000, episode_reward=57.72 +/- 171.23\n",
      "Episode length: 2.90 +/- 1.09\n",
      "Eval num_timesteps=362000, episode_reward=90.14 +/- 97.45\n",
      "Episode length: 2.40 +/- 1.11\n",
      "Eval num_timesteps=363000, episode_reward=78.53 +/- 100.80\n",
      "Episode length: 2.10 +/- 0.70\n",
      "Eval num_timesteps=364000, episode_reward=-331.19 +/- 1807.65\n",
      "Episode length: 2.45 +/- 0.80\n",
      "Eval num_timesteps=365000, episode_reward=97.56 +/- 63.99\n",
      "Episode length: 2.75 +/- 0.83\n",
      "Eval num_timesteps=366000, episode_reward=59.64 +/- 79.42\n",
      "Episode length: 2.45 +/- 0.92\n",
      "Eval num_timesteps=367000, episode_reward=110.43 +/- 110.88\n",
      "Episode length: 2.65 +/- 0.79\n",
      "Eval num_timesteps=368000, episode_reward=120.08 +/- 75.09\n",
      "Episode length: 2.35 +/- 0.65\n",
      "Eval num_timesteps=369000, episode_reward=89.73 +/- 93.29\n",
      "Episode length: 2.65 +/- 0.79\n",
      "Eval num_timesteps=370000, episode_reward=89.69 +/- 99.44\n",
      "Episode length: 2.35 +/- 1.11\n",
      "Eval num_timesteps=371000, episode_reward=93.14 +/- 81.83\n",
      "Episode length: 2.25 +/- 0.83\n",
      "Eval num_timesteps=372000, episode_reward=107.14 +/- 66.80\n",
      "Episode length: 2.60 +/- 0.92\n",
      "Eval num_timesteps=373000, episode_reward=73.08 +/- 92.27\n",
      "Episode length: 2.70 +/- 1.00\n",
      "Eval num_timesteps=374000, episode_reward=-381.32 +/- 1996.07\n",
      "Episode length: 3.05 +/- 2.01\n",
      "Eval num_timesteps=375000, episode_reward=107.03 +/- 93.72\n",
      "Episode length: 2.35 +/- 0.91\n",
      "Eval num_timesteps=376000, episode_reward=70.06 +/- 115.74\n",
      "Episode length: 3.00 +/- 0.95\n",
      "Eval num_timesteps=377000, episode_reward=94.94 +/- 70.19\n",
      "Episode length: 2.45 +/- 0.80\n",
      "Eval num_timesteps=378000, episode_reward=-267.97 +/- 1794.33\n",
      "Episode length: 2.60 +/- 0.86\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m400000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# 继续训练200,000步\u001B[39;49;00m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meval_callback\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# 保存重新训练的模型\u001B[39;00m\n\u001B[0;32m      7\u001B[0m model\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel/dqn_retrained_electric_vehicle3\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:267\u001B[0m, in \u001B[0;36mDQN.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlearn\u001B[39m(\n\u001B[0;32m    259\u001B[0m     \u001B[38;5;28mself\u001B[39m: SelfDQN,\n\u001B[0;32m    260\u001B[0m     total_timesteps: \u001B[38;5;28mint\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    265\u001B[0m     progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    266\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m SelfDQN:\n\u001B[1;32m--> 267\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    268\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    269\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    270\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_interval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    271\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_log_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    272\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    273\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    274\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:328\u001B[0m, in \u001B[0;36mOffPolicyAlgorithm.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_freq, TrainFreq)  \u001B[38;5;66;03m# check done in _setup_learn()\u001B[39;00m\n\u001B[0;32m    327\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps \u001B[38;5;241m<\u001B[39m total_timesteps:\n\u001B[1;32m--> 328\u001B[0m     rollout \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect_rollouts\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    329\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    330\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain_freq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    331\u001B[0m \u001B[43m        \u001B[49m\u001B[43maction_noise\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maction_noise\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    332\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    333\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlearning_starts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearning_starts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    334\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreplay_buffer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreplay_buffer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    335\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_interval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    336\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    338\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m rollout\u001B[38;5;241m.\u001B[39mcontinue_training:\n\u001B[0;32m    339\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:560\u001B[0m, in \u001B[0;36mOffPolicyAlgorithm.collect_rollouts\u001B[1;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001B[0m\n\u001B[0;32m    557\u001B[0m actions, buffer_actions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sample_action(learning_starts, action_noise, env\u001B[38;5;241m.\u001B[39mnum_envs)\n\u001B[0;32m    559\u001B[0m \u001B[38;5;66;03m# Rescale and perform action\u001B[39;00m\n\u001B[1;32m--> 560\u001B[0m new_obs, rewards, dones, infos \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mactions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    562\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mnum_envs\n\u001B[0;32m    563\u001B[0m num_collected_steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:206\u001B[0m, in \u001B[0;36mVecEnv.step\u001B[1;34m(self, actions)\u001B[0m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    200\u001B[0m \u001B[38;5;124;03mStep the environments with the given action\u001B[39;00m\n\u001B[0;32m    201\u001B[0m \n\u001B[0;32m    202\u001B[0m \u001B[38;5;124;03m:param actions: the action\u001B[39;00m\n\u001B[0;32m    203\u001B[0m \u001B[38;5;124;03m:return: observation, reward, done, information\u001B[39;00m\n\u001B[0;32m    204\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    205\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep_async(actions)\n\u001B[1;32m--> 206\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep_wait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001B[0m, in \u001B[0;36mDummyVecEnv.step_wait\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep_wait\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m VecEnvStepReturn:\n\u001B[0;32m     56\u001B[0m     \u001B[38;5;66;03m# Avoid circular imports\u001B[39;00m\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m env_idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_envs):\n\u001B[1;32m---> 58\u001B[0m         obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_rews[env_idx], terminated, truncated, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_infos[env_idx] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menvs\u001B[49m\u001B[43m[\u001B[49m\u001B[43menv_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     59\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mactions\u001B[49m\u001B[43m[\u001B[49m\u001B[43menv_idx\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     60\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m         \u001B[38;5;66;03m# convert to SB3 VecEnv api\u001B[39;00m\n\u001B[0;32m     62\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_dones[env_idx] \u001B[38;5;241m=\u001B[39m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated\n",
      "File \u001B[1;32m~\\PycharmProjects\\chong_new\\env.py:175\u001B[0m, in \u001B[0;36mElectricVehicleEnv.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    162\u001B[0m     \u001B[38;5;66;03m# 额外信息，用于调试\u001B[39;00m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    164\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfrom_node\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_node,\n\u001B[0;32m    165\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mto_node\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_node,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    173\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime_end\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mremaining_time\n\u001B[0;32m    174\u001B[0m     }\n\u001B[1;32m--> 175\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_state\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreward, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtruncated, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\n",
      "File \u001B[1;32m~\\PycharmProjects\\chong_new\\env.py:213\u001B[0m, in \u001B[0;36mElectricVehicleEnv._get_state\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    210\u001B[0m energy_consumed \u001B[38;5;241m=\u001B[39m distance_to_next_node \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevs_50\u001B[38;5;241m.\u001B[39miloc[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_vehicle][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m行驶能耗\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m100\u001B[39m\n\u001B[0;32m    212\u001B[0m \u001B[38;5;66;03m# 计算时间消耗\u001B[39;00m\n\u001B[1;32m--> 213\u001B[0m travel_time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtime_50\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcurrent_node\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnext_node\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;66;03m# 判断该路段是否为充电路段\u001B[39;00m\n\u001B[0;32m    216\u001B[0m charging \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\pandas\\core\\indexing.py:1096\u001B[0m, in \u001B[0;36m_LocationIndexer.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   1094\u001B[0m     key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(com\u001B[38;5;241m.\u001B[39mapply_if_callable(x, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m key)\n\u001B[0;32m   1095\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_scalar_access(key):\n\u001B[1;32m-> 1096\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_value\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtakeable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_takeable\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1097\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_tuple(key)\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1099\u001B[0m     \u001B[38;5;66;03m# we by definition only have the 0th axis\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\pandas\\core\\frame.py:3867\u001B[0m, in \u001B[0;36mDataFrame._get_value\u001B[1;34m(self, index, col, takeable)\u001B[0m\n\u001B[0;32m   3848\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   3849\u001B[0m \u001B[38;5;124;03mQuickly retrieve single value at passed column and index.\u001B[39;00m\n\u001B[0;32m   3850\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3864\u001B[0m \u001B[38;5;124;03m`self.columns._index_as_unique`; Caller is responsible for checking.\u001B[39;00m\n\u001B[0;32m   3865\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   3866\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m takeable:\n\u001B[1;32m-> 3867\u001B[0m     series \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ixs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3868\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m series\u001B[38;5;241m.\u001B[39m_values[index]\n\u001B[0;32m   3870\u001B[0m series \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_item_cache(col)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\pandas\\core\\frame.py:3666\u001B[0m, in \u001B[0;36mDataFrame._ixs\u001B[1;34m(self, i, axis)\u001B[0m\n\u001B[0;32m   3662\u001B[0m \u001B[38;5;66;03m# icol\u001B[39;00m\n\u001B[0;32m   3663\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   3664\u001B[0m     label \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns[i]\n\u001B[1;32m-> 3666\u001B[0m     col_mgr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_mgr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3667\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_box_col_values(col_mgr, i)\n\u001B[0;32m   3669\u001B[0m     \u001B[38;5;66;03m# this is a cached value, mark it so\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1093\u001B[0m, in \u001B[0;36mBlockManager.iget\u001B[1;34m(self, i, track_ref)\u001B[0m\n\u001B[0;32m   1091\u001B[0m \u001B[38;5;66;03m# shortcut for select a single-dim from a 2-dim BM\u001B[39;00m\n\u001B[0;32m   1092\u001B[0m bp \u001B[38;5;241m=\u001B[39m BlockPlacement(\u001B[38;5;28mslice\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(values)))\n\u001B[1;32m-> 1093\u001B[0m nb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mblock\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1094\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mplacement\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mndim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrefs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mblock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrefs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrack_ref\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[0;32m   1095\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1096\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m SingleBlockManager(nb, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxes[\u001B[38;5;241m1\u001B[39m])\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\pandas\\_libs\\internals.pyx:616\u001B[0m, in \u001B[0;36mpandas._libs.internals.SharedBlock.__cinit__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\UAV\\lib\\site-packages\\pandas\\_libs\\internals.pyx:894\u001B[0m, in \u001B[0;36mpandas._libs.internals.BlockValuesRefs.add_reference\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mMemoryError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(\n",
    "    total_timesteps=400000,  # 继续训练200,000步\n",
    "    tb_log_name=log_name,\n",
    "    callback=eval_callback\n",
    ")\n",
    "# 保存重新训练的模型\n",
    "model.save(\"model/dqn_retrained_electric_vehicle3\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-12T11:03:20.248919700Z",
     "start_time": "2024-09-12T09:35:11.091129900Z"
    }
   },
   "id": "4647e47600616bc1"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model.save(\"model/dqn_retrained_electric_vehicle3\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-12T11:04:53.374259500Z",
     "start_time": "2024-09-12T11:04:53.316699800Z"
    }
   },
   "id": "b2aaf05ad14dd4a1"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1b231aa4230c4934"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
